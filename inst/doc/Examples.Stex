\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Examples}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Byron Hall\\STATISTICAT, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon} Examples}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Byron Hall} %% comma-separated
\Plaintitle{LaplacesDemon Examples} %% without formatting
\Shorttitle{Examples} %% a short title (if necessary)

\Abstract{The \pkg{LaplacesDemon} package in \proglang{R} enables Bayesian inference with any Bayesian model, provided the user specifies the likelihood.  This vignette is a compendium of examples of how to specify different model forms.
}
\Keywords{Bayesian, Bayesian Inference, Laplace's Demon, LaplacesDemon, R, 
STATISTICAT}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
  Byron Hall\\
  STATISTICAT, LLC\\
  Farmington, CT\\
  E-mail: \email{statisticat@gmail.com}\\
  URL: \url{http://www.statisticat.com/laplacesdemon.html}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
\pkg{LaplacesDemon} \citep{r:laplacesdemon}, usually referred to as Laplace's Demon, is an \proglang{R} package that is available on CRAN \citep{rdct:r}. A formal introduction to Laplace's Demon is provided in an accompanying vignette entitled ``\pkg{LaplacesDemon} Tutorial'', and an introduction to Bayesian inference is provided in the ``Bayesian Inference'' vignette.

The purpose of this document is to provide users of the \pkg{LaplacesDemon} package with examples of a variety of Bayesian methods. It is also a testament to the diverse applicability of \pkg{LaplacesDemon} to Bayesian inference.

To conserve space, the examples are not worked out in detail, and only the minimum of necessary materials is provided for using the various methodologies. Necessary materials include the form expressed in notation, data (which is often simulated), initial values, and the \code{Model} function. The provided data, initial values, and model specification may be copy/pasted into an \proglang{R} file and updated with the \code{LaplacesDemon} or (usually) \code{LaplaceApproximation} functions. Although many of these examples update quickly, some examples are computationally intensive.

Notation in this vignette follows these standards: Greek letters represent parameters, lower case letters represent indices, lower case bold face letters represent scalars or vectors, probability distributions are represented with calligraphic font, upper case letters represent index limits, and upper case bold face letters represent matrices.

This vignette will grow over time as examples of more methods become included. Contributed examples are welcome. Please send contributed examples or discovered errors in a similar format in an email to \email{statisticat@gmail.com} for review and testing. All accepted contributions are, of course, credited.

\begin{center} \Large{\textbf{Contents}} \end{center}
\begin{itemize}
\item ANCOVA \ref{ancova}
\item ANOVA, One-Way \ref{anova.one.way}
\item ANOVA, Two-Way \ref{anova.two.way}
\item Approximate Bayesian Computation (ABC) \ref{abc}
\item ARCH-M(1,1) \ref{archm}
\item Autoregression, AR(1) \ref{ar1}
\item Autoregressive Conditional Heteroskedasticity, ARCH(1,1) \ref{arch11}
\item Autoregressive Moving Average, ARMA(1,1) \ref{arma11}
\item Beta Regression \ref{beta.reg}
\item Beta-Binomial \ref{beta.binomial}
\item Binary Logit \ref{binary.logit}
\item Binary Log-Log Link Mixture \ref{binary.loglog.mixture}
\item Binary Probit \ref{binary.probit}
\item Binomial Logit \ref{binomial.logit}
\item Binomial Probit \ref{binomial.probit}
\item Cluster Analysis, Confirmatory (CCA) \ref{cca}
\item Cluster Analysis, Exploratory (ECA) \ref{eca}
\item Conditional Autoregression (CAR), Poisson \ref{car.poisson}
\item Conditional Predictive Ordinate (CPO) \ref{cpo}
\item Contingency Table \ref{contingency.table}
\item Covariance Separation Strategy \ref{cov.sep.strat}
\item Discrete Choice, Conditional Logit \ref{conditional.logit}
\item Discrete Choice, Mixed Logit \ref{mixed.logit}
\item Discrete Choice, Multinomial Probit \ref{dc.mnp}
\item Distributed Lag, Koyck \ref{dl.koyck}
\item Dynamic Linear Model (DLM) \ref{dfa} \ref{ssm.lin.reg} \ref{ssm.ll} \ref{ssm.llt}
\item Exponential Smoothing \ref{exp.smo}
\item Factor Analysis, Approximate Dynamic (ADFA) \ref{adfa}
\item Factor Analysis, Confirmatory (CFA) \ref{cfa}
\item Factor Analysis, Dynamic (DFA) \ref{dfa}
\item Factor Analysis, Exploratory (EFA) \ref{efa}
\item Factor Regression \ref{factor.reg}
\item Gamma Regression \ref{gamma.reg}
\item GARCH(1,1) \ref{garch}
\item GARCH-M(1,1) \ref{garchm}
\item Geographically Weighted Regression \ref{gwr}
\item Hierarchical Bayes \ref{linear.reg.hb}
\item Inverse Gaussian Regression \ref{ig.reg}
\item Kriging \ref{kriging}
\item Kriging, Predictive Process \ref{kriging.pp}
\item Laplace Regression \ref{laplace.reg}
\item Linear Regression \ref{linear.reg}
\item Linear Regression, Frequentist \ref{linear.reg.freq}
\item Linear Regression, Hierarchical Bayesian \ref{linear.reg.hb}
\item Linear Regression, Multilevel \ref{linear.reg.ml}
\item Linear Regression with Full Missingness \ref{linear.reg.full.miss}
\item Linear Regression with Missing Response \ref{linear.reg.miss.resp}
\item LSTAR \ref{lstar}
\item MANCOVA \ref{mancova}
\item MANOVA \ref{manova}
\item Mixture Model, Dirichlet Process \ref{eca}
\item Mixture Model, Finite \ref{cca} \ref{fmm}
\item Mixture Model, Infinite \ref{eca} \ref{imm}
\item Mixture Model, Poisson-Gamma \ref{poisson.gamma}
\item Model Averaging \ref{ssvs}
\item Multilevel Model \ref{linear.reg.ml}
\item Multinomial Logit \ref{mnl}
\item Multinomial Logit, Nested \ref{nmnl}
\item Multinomial Probit \ref{mnp}
\item Multivariate Binary Probit \ref{multiv.bin.probit}
\item Multivariate Laplace Regression \ref{multivariate.lap.reg}
\item Multivariate Regression \ref{multivariate.reg}
\item Negative Binomial Regression \ref{negbin.reg}
\item Normal, Multilevel \ref{norm.ml}
\item Ordinal Logit \ref{ordinal.logit}
\item Ordinal Probit \ref{ordinal.probit}
\item Panel, Autoregressive Poisson \ref{panel.ap}
\item Penalized Spline Regression \ref{penalized.spline}
\item Poisson Regression \ref{poisson.reg}
\item Poisson Regression, Overdispersed \ref{poisson.gamma} \ref{negbin.reg}
\item Poisson-Gamma Regression \ref{poisson.gamma}
\item Polynomial Regression \ref{polynomial.reg}
\item Proportional Hazards Regression, Weibull \ref{prop.haz.weib}
\item Revision, Normal \ref{revision.normal}
\item Robust Regression \ref{robust.reg}
\item Seemingly Unrelated Regression (SUR) \ref{sur}
\item Simultaneous Equations \ref{simultaneous}
\item Space-Time, Dynamic \ref{spacetime.dynamic}
\item Space-Time, Nonseparable \ref{spacetime.nonsep}
\item Space-Time, Separable \ref{spacetime.sep}
\item Spatial Autoregression (SAR) \ref{sar}
\item STARMA(1,1) \ref{starma}
\item State Space Model (SSM), Dynamic Factor Analysis (DFA) \ref{dfa}
\item State Space Model (SSM), Linear Regression \ref{ssm.lin.reg}
\item State Space Model (SSM), Local Level \ref{ssm.ll}
\item State Space Model (SSM), Local Linear Trend \ref{ssm.llt}
\item State Space Model (SSM), Stochastic Volatility (SV) \ref{sv}
\item Stochastic Volatility (SV) \ref{sv}
\item T-test \ref{anova.one.way}
\item Threshold Autoregression (TAR) \ref{tar}
\item TARCH(1) \ref{tarch}
\item Variable Selection, BAL \ref{bal}
\item Variable Selection, SSVS \ref{ssvs}
\item Vector Autoregression, VAR(1) \ref{var1}
\item Weighted Regression \ref{weighted.reg}
\item Zero-Inflated Poisson (ZIP) \ref{zip}
\end{itemize}

\section{ANCOVA} \label{ancova}
This example is essentially the same as the two-way ANOVA (see section \ref{anova.two.way}), except that a covariate $\textbf{X}_{,3}$ has been added, and its parameter is $\delta$.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}] + \delta \textbf{X}_{i,2}, \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,(K-1)$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- matrix(cbind(round(runif(N,0.5,J+0.49)),round(runif(N,0.5,K+0.49)), \\
\hspace*{0.27 in} runif(N,-2,2)), N, 3) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
gamma <- runif(K,-2,2) \\
gamma[J] <- -sum(gamma[1:(K-1)]) \\
delta <- runif(1,-2,2) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + delta*X[,3] + rnorm(N,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","sigma[1]","sigma[2]","sigma[3]", \\
\hspace*{0.27 in} "s.beta","s.gamma","s.epsilon") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} delta=0, log.sigma=rep(0,3))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), 0, rep(log(1),3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- rep(NA,Data$K) \\
\hspace*{0.27 in} gamma[1:(Data$K-1)] <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma[K] <- -sum(gamma[1:(Data$K-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} delta <- parm[grep("delta", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] + \\
\hspace*{0.62 in} delta*Data$X[,3] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], sigma, s.beta, s.gamma, s.epsilon), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ANOVA, One-Way} \label{anova.one.way}
When $J=2$, this is a Bayesian form of a t-test.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{x}_i], \quad i=1,\dots,N$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \displaystyle\sum^{J-1}_{j=1} \beta_j$$
$$\sigma_{1:2} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
x <- rcat(N, rep(1,J)) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
y <- rep(NA, N) \\
for (i in 1:N) \{y[i] <- alpha + beta[x[i]] + rnorm(1,0,0.2)\} \\
mon.names <- c("LP","beta[1]","sigma[1]","sigma[2]") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
MyData <- list(J=J, N=N, mon.names=mon.names, parm.names=parm.names, x=x, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(log(1),2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$x] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,beta[Data$J], \\
\hspace*{0.62 in} sigma), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ANOVA, Two-Way} \label{anova.two.way}
In this representation, $\sigma^m$ are the superpopulation variance components, \code{s.beta} and \code{s.gamma} are the finite-population within-variance components of the factors or treatments, and \code{s.epsilon} is the finite-population between-variance component.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}], \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,(K-1)$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- matrix(cbind(round(runif(N, 0.5, J+0.49)),round(runif(N,0.5,K+0.49))), \\
\hspace*{0.27 in} N, 2) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
gamma <- runif(K,-2,2) \\
gamma[J] <- -sum(gamma[1:(K-1)]) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + rnorm(1,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","sigma[1]","sigma[2]","sigma[3]", \\
\hspace*{0.27 in} "s.beta","s.gamma","s.epsilon") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} log.sigma=rep(0,3))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), rep(log(1),3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- rep(NA,Data$K) \\
\hspace*{0.27 in} gamma[1:(Data$K-1)] <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma[K] <- -sum(gamma[1:(Data$K-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], sigma, s.beta, s.gamma, s.epsilon), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Approximate Bayesian Computation (ABC)} \label{abc}
Approximate Bayesian Computation (ABC), also called likelihood-free estimation, is not a statistical method, but a family of numerical approximation techniques in Bayesian inference. ABC is especially useful when evaluation of the likelihood, $p(\textbf{y} | \Theta)$ is computationally prohibitive, or when suitable likelihoods are unavailable. The current example is the application of ABC in the context of linear regression. The log-likelihood is replaced with the negative sum of the distance between $\textbf{y}$ and $\textbf{y}^{rep}$ as the approximation of the log-likelihood. Distance reduces to the absolute difference. Although linear regression has an easily calculated likelihood, it is used as an example due to its generality. This example demonstrates how ABC may be estimated either with MCMC via the \code{LaplacesDemon} function or with Laplace Approximation via the \code{LaplaceApproximation} function. In this method, a tolerance (which is found often in ABC) does not need to be specified, and the logarithm of the unnormalized joint posterior density is maximized, as usual. The negative and summed distance, above, may be replaced with the negative and summed distance between summaries of the data, rather than the data itself, but this has not been desirable in testing.
\subsection{Form}
$$\textbf{y} = \mu + \epsilon$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood Approximation \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma <- sd(epsilon) \\
\hspace*{0.27 in} LL <- -sum(abs(epsilon)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior Approximation \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ARCH-M(1,1)} \label{archm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta \epsilon^2_T$$
$$\sigma^2_t = \omega + \theta \epsilon^2_{t-1}$$
$$\omega <- \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(0, 1)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","log.omega","theta") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0.5,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; delta <- parm[3] \\
\hspace*{0.27 in} omega <- exp(parm[4]) \\
\hspace*{0.27 in} parm[5] <- theta <- interval(parm[5], 1e-10, 1-1e-5) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 0, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T] \\
\hspace*{0.27 in} sigma2.new <- omega + theta*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregression, AR(1)} \label{ar1}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \mu_{T+1}$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew") \\
parm.names <- c("alpha","phi","log.sigma") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; sigma <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregressive Conditional Heteroskedasticity, ARCH(1,1)} \label{arch11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta \epsilon^2_T$$
$$\sigma^2_t = \omega + \theta \epsilon^2_{t-1},$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(0, 1)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","log.omega","theta") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0.5,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; omega <- exp(parm[3]) \\
\hspace*{0.27 in} parm[4] <- theta <- interval(parm[4], 1e-10, 1-1e-5)
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 0, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2.new <- omega + theta*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, \\
\hspace*{0.62 in} sigma2.new), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregressive Moving Average, ARMA(1,1)} \label{arma11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \phi \textbf{y}_T + \theta \epsilon_T$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \theta \epsilon_{t-1}$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew") \\
parm.names <- c("alpha","phi","sigma","theta") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; theta <- parm[3] \\
\hspace*{0.27 in} sigma <- exp(parm[4]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnormv(theta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} mu <- c(mu[1], mu[-1] + theta * epsilon[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + theta*epsilon[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma, ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Beta Regression} \label{beta.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BETA}(a,b)$$
$$a = \mu \phi$$
$$b = (1 - \mu) \phi$$
$$\mu = \Phi(\beta_1 + \beta_2 \textbf{x})$$
$$\beta_j \sim \mathcal{N}(0, 10), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{G}(1,1)$$
where $\Phi$ is the normal CDF.
\subsection{Data}
\code{N <- 10 \\
x <- runif(N) \\
y <- qbeta(0.5, pnorm(2-3*x)*4, (1-pnorm(2-3*x))*4) \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]","log.phi") \\
MyData <- list(x=x, y=y, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(0.01))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:2]; phi <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 10, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dgamma(phi, 1, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- pnorm(beta[1] + beta[2]*Data$x) \\
\hspace*{0.27 in} a <- mu * phi \\
\hspace*{0.27 in} b <- (1-mu) * phi \\
\hspace*{0.27 in} LL <- sum(dbeta(Data$y, a, b, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Beta-Binomial} \label{beta.binomial}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{BIN}(\textbf{n}_i, \pi_i), \quad i=1,\dots,N$$
$$\pi_i \sim \mathcal{BETA}(\alpha, \beta) \in [0.001,0.999]$$
\subsection{Data}
\code{N <- 20 \\
n <- round(runif(N, 50, 100)) \\
y <- round(runif(N, 1, 10)) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(pi=rep(0,N))) \\
MyData <- list(N=N, mon.names=mon.names, n=n, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0.5,N))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} pi <- interval(parm[1:Data$N], 0.001, 0.999) \\
\hspace*{0.27 in} parm[1:Data$N] <- pi \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} pi.prior <- sum(dbeta(pi, 1, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, pi, log=TRUE)) \\
\hspace*{0.27 in} yrep <- pi * Data$n \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + pi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Logit} \label{binary.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\eta)$$
$$\eta = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} eta <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(eta >= (sum(Data$y)/length(Data$y)),1,0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Log-Log Link Mixture} \label{binary.loglog.mixture}
A weighted mixture of the log-log and complementary log-log link functions is used, where $\alpha$ is the weight. Since the log-log and complementary log-log link functions are asymmetric (as opposed to the symmetric logit and probit link functions), it may be unknown \textit{a priori} whether the log-log or complementary log-log will perform better. 
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\eta)$$
$$\eta = \alpha \exp(-\exp(\mu)) + (1 - \alpha) (1 - \exp(-\exp(\mu)))$$
$$\mu = \textbf{X} \beta$$
$$\alpha \sim \mathcal{U}(0, 1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 30, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","alpha") \\
parm.names <- as.parm.names(list(beta=rep(0,J), logit.alpha=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- invlogit(parm[Data$J+1]) \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 0, 1, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} eta <- alpha*invloglog(mu) + (1-alpha)*invcloglog(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(eta >= (sum(Data$y)/length(Data$y)),1,0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,alpha), \\
\hspace*{0.62 in} yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Probit} \label{binary.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\textbf{p})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \textbf{X} \beta \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where $\phi$ is the CDF of the standard normal distribution, and $J$=3.
\subsection{Data}
\code{data(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(p >= (sum(Data$y)/length(Data$y)),1,0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Binomial Logit} \label{binomial.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \beta_1 + \beta_2 \textbf{x}$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} p <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- p * Data$n \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Binomial Probit} \label{binomial.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \beta_1 + \beta_2 \textbf{x} \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the CDF of the standard normal distribution, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- p * Data$n \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Cluster Analysis, Confirmatory (CCA)} \label{cca}
This is a parametric, model-based, cluster analysis, also called a finite mixture model or latent class cluster analysis, where the number of clusters $C$ is fixed. When the number of clusters is unknown, exploratory cluster analysis should be used (see section \ref{eca}).
\subsection{Form}
$$\textbf{Y}_{i,j} \sim \mathcal{N}(\mu_{\theta[i],j}, \sigma^2_{\theta[i]}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\theta_i = \mathrm{Max}(\textbf{p}_{i,1:C})$$
$$\textbf{p}_{i,c} = \frac{\delta_{i,c}}{\sum^C_{c=1} \delta_{i,c}}$$
$$\pi_{1:C} \sim \mathcal{D}(\alpha_{1:C})$$
$$\pi_c = \frac{\sum^N_{i=1} \delta_{i,c}}{\sum \delta}$$
$$\alpha_c = 1$$
$$\delta_{i,C} = 1$$
$$\delta_{i,c} \sim \mathcal{N}(\log(\frac{1}{C}), 1000) \in [\exp(-10),\exp(10)], \quad c=1,\dots,(C-1)$$
$$\mu_{c,j} \sim \mathcal{N}(0, \nu^2_j)$$
$$\sigma_c \sim \mathcal{HC}(25)$$
$$\nu_j \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 3 \#Number of clusters \\
alpha <- rep(1,C) \#Prior probability of cluster proportion \\
\# Create a Y matrix \\
n <- 100; N <- 15 \#Full sample; model sample \\
J <- 5 \#Number of predictor variables \\
cluster <- rcat(n, rep(1,C)) \\
centers <- matrix(runif(C*J, 0, 10), C, J) \\
Y.Full <- matrix(0, n, J) \\
for (i in 1:n) \{for (j in 1:J) \\
\hspace*{0.27 in} \{Y.Full[i,j] <- rnorm(1,centers[cluster[i],j],1)\}\} \\
mean.temp <- colMeans(Y.Full) \\
sigma.temp <- apply(Y.Full,2,sd) \\
centers.cs <- (centers - matrix(rep(mean.temp,C), C, J, byrow=TRUE)) / \\
\hspace*{0.27 in} (2 * matrix(rep(sigma.temp,C), C, J, byrow=TRUE)) \\
for (j in 1:J) \{Y.Full[,j] <- scale(Y.Full[,j],2)\} \\
\#summary(Y.Full) \\
MySample <- sample(1:n, N) \\
Y <- Y.Full[MySample,] \\
mon.names <- c("LP", as.parm.names(list(nu=rep(0,J), pi=rep(0,C), \\
\hspace*{0.27 in} sigma=rep(0,C), theta=rep(0,N)))) \\
parm.names <- as.parm.names(list(log.delta=matrix(0,N,C-1), mu=matrix(0,C,J), \\
\hspace*{0.27 in} log.nu=rep(0,J), log.sigma=rep(0,C))) \\
MyData <- list(C=C, J=J, N=N, Y=Y, alpha=alpha, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(N*(C-1),-1,1), rep(0,C*J), rep(0,J), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$C) \\
\hspace*{0.27 in} mu <- matrix(parm[grep("mu", Data$parm.names)], Data$C, Data$J) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu",Data$parm.names)]) \\
\hspace*{0.27 in} pi <- colSums(delta) / sum(delta) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$C), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnorm(mu, 0, matrix(rep(nu,Data$C), Data$C, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} theta <- max.col(p) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu[theta,], sigma[theta], log=TRUE)) \\
\hspace*{0.27 in} Yrep <- mu[theta,] \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + delta.prior + mu.prior + nu.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,nu,pi,sigma,theta), \\
\hspace*{0.62 in} yhat=Yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Cluster Analysis, Exploratory (ECA)} \label{eca}
In ``exploratory cluster analysis'', the optimal number of clusters $C$ is unknown before the model update. This is a nonparametric, model-based, infinite mixture model that uses truncated stick-breaking within a truncated Dirichlet process. The user must specify the maximum number of clusters (mixture components), $C$ to explore, where $C$ is discrete, greater than one, and less than the number of records, $N$. The records in the $N \times J$ matrix \textbf{Y} are clustered, where $J$ is the number of predictors.
\subsection{Form}
$$\textbf{Y}_{i,j} \sim \mathcal{N}(\mu_{\theta[i],j}, \sigma^2_{\theta[i]}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\theta_i \sim \mathcal{CAT}(\pi_{i,1:C})$$
$$\textbf{p}_{i,c} = \frac{\delta_{i,c}}{\sum^C_{c=1} \delta_{i,c}}$$
$$\pi \sim \mathrm{Stick}(\gamma)$$
$$\delta_{i,C} = 1$$
$$\delta_{i,c} \sim \mathcal{N}(\log(\frac{1}{C}), 1000) \in [\exp(-10),\exp(10)], \quad c=1,\dots,(C-1)$$
$$\mu_{c,j} \sim \mathcal{N}(0, \nu^2_j)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\beta \sim \mathcal{HC}(25)$$
$$\gamma \sim \mathcal{G}(\alpha, \beta)$$
$$\sigma_c \sim \mathcal{HC}(25)$$
$$\nu_j \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 3 \#Number of clusters for simulated DGP \\
\# Create a Y matrix \\
n <- 100; N <- 15 \#Full sample; model sample \\
J <- 5 \#Number of predictor variables \\
cluster <- round(runif(n,0.5,C+0.49)) \\
centers <- matrix(runif(C*J, 0, 10), C, J) \\
Y.Full <- matrix(0, n, J) \\
for (i in 1:n) \{for (j in 1:J) \\
\hspace*{0.27 in} \{Y.Full[i,j] <- rnorm(1,centers[cluster[i],j],1)\}\} \\
mean.temp <- colMeans(Y.Full) \\
sigma.temp <- apply(Y.Full,2,sd) \\
centers.cs <- (centers - matrix(rep(mean.temp,C), C, J, byrow=TRUE)) / \\
\hspace*{0.27 in} (2 * matrix(rep(sigma.temp,C), C, J, byrow=TRUE)) \\
for (j in 1:J) \{Y.Full[,j] <- scale(Y.Full[,j],2)\} \\
MySample <- sample(1:n, N) \\
Y <- Y.Full[MySample,] \\
C <- 10 \#Number of clusters to explore \\
mon.names <- c("LP", as.parm.names(list(nu=rep(0,J), pi=rep(0,C), \\
\hspace*{0.27 in} sigma=rep(0,C), theta=rep(0,N)))) \\
parm.names <- as.parm.names(list(log.delta=matrix(0,N,C-1), \\
\hspace*{0.27 in} mu=matrix(0,C,J), log.nu=rep(0,J), log.sigma=rep(0,C), \\
\hspace*{0.27 in} lambda=rep(0,C-1), log.alpha=0, log.beta=0, log.gamma=0)) \\
MyData <- list(C=C, J=J, N=N, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(N*(C-1),-1,1), rep(0,C*J), rep(0,J), rep(0,C), \\
\hspace*{0.27 in} rbeta(C-1,1,2), rep(1,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} alpha <- exp(parm[grep("log.alpha", Data$parm.names)]) \\
\hspace*{0.27 in} beta <- exp(parm[grep("log.beta", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- exp(parm[grep("log.gamma", Data$parm.names)]) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu",Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$C) \\
\hspace*{0.27 in} lambda <- interval(parm[grep("lambda", Data$parm.names)], 1e-5, 1-1e-5) \\
\hspace*{0.27 in} mu <- matrix(parm[grep("mu", Data$parm.names)], Data$C, Data$J) \\
\hspace*{0.27 in} pi <- as.vector(Stick(lambda)) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} theta <- max.col(p) \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dhalfcauchy(beta, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, alpha, beta, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$C), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnorm(mu, 0, matrix(rep(nu,Data$C), Data$C, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- dStick(pi, gamma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, pi, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu[theta,], sigma[theta], log=TRUE)) \\
\hspace*{0.27 in} Yrep <- mu[theta,] \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + delta.prior + mu.prior + nu.prior + pi.prior + \\
\hspace*{0.62 in} alpha.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,nu,pi,sigma,theta), \\
\hspace*{0.62 in} yhat=Yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Conditional Autoregression (CAR), Poisson} \label{car.poisson}
This CAR example is a slightly modified form of example 7.3 (Model A) in \citet{congdon03}. The Scottish lip cancer data also appears in the WinBUGS \citep{spiegelhalter03} examples and is a widely analyzed example. The data $\textbf{y}$ consists of counts for $i=1,\dots,56$ counties in Scotland. A single predictor $\textbf{x}$ is provided. The errors, $\epsilon$, are allowed to include spatial effects as smoothing by spatial effects from areal neighbors. The vector $\epsilon_\mu$ is the mean of each area's error, and is a weighted average of errors in contiguous areas. Areal neighbors are indicated in adjacency matrix $\textbf{A}$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\log(\textbf{E}) + \beta_1 + \beta_2 \textbf{x} + \epsilon)$$
$$\epsilon \sim \mathcal{N}(\epsilon_\mu, \sigma^2)$$
$$\epsilon_{\mu[i]} = \rho \sum^J_{j=1} \textbf{A}_{i,j} \epsilon_j, \quad i=1,\dots,N$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\rho \sim \mathcal{U}(-1,1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 56 \#Number of areas \\
NN <- 264 \#Number of adjacent areas \\
y <- c(9,39,11,9,15,8,26,7,6,20,13,5,3,8,17,9,2,7,9,7,16,31,11,7,19,15,7, \\
\hspace*{0.27 in} 10,16,11,5,3,7,8,11,9,11,8,6,4,10,8,2,6,19,3,2,3,28,6,1,1,1,1,0,0) \\
E <- c( 1.4,8.7,3.0,2.5,4.3,2.4,8.1,2.3,2.0,6.6,4.4,1.8,1.1,3.3,7.8,4.6, \\
\hspace*{0.27 in} 1.1,4.2,5.5,4.4,10.5,22.7,8.8,5.6,15.5,12.5,6.0,9.0,14.4,10.2,4.8, \\
\hspace*{0.27 in} 2.9,7.0,8.5,12.3,10.1,12.7,9.4,7.2,5.3,18.8,15.8,4.3,14.6,50.7,8.2, \\
\hspace*{0.27 in} 5.6,9.3,88.7,19.6,3.4,3.6,5.7,7.0,4.2,1.8) \#Expected \\
x <- c(16,16,10,24,10,24,10,7,7,16,7,16,10,24,7,16,10,7,7,10,7,16,10,7,1,1, \\
\hspace*{0.27 in} 7,7,10,10,7,24,10,7,7,0,10,1,16,0,1,16,16,0,1,7,1,1,0,1,1,0,1,1,16,10) \\
A <- matrix(0, N, N) \\
A[1,c(5,9,11,19)] <- 1 \#Area 1 is adjacent to areas 5, 9, 11, and 19 \\
A[2,c(7,10)] <- 1 \#Area 2 is adjacent to areas 7 and 10 \\
A[3,c(6,12)] <- 1; A[4,c(18,20,28)] <- 1; A[5,c(1,11,12,13,19)] <- 1 \\
A[6,c(3,8)] <- 1; A[7,c(2,10,13,16,17)] <- 1; A[8,6] <- 1 \\
A[9,c(1,11,17,19,23,29)] <- 1; A[10,c(2,7,16,22)] <- 1 \\
A[11,c(1,5,9,12)] <- 1; A[12,c(3,5,11)] <- 1; A[13,c(5,7,17,19)] <- 1 \\
A[14,c(31,32,35)] <- 1; A[15,c(25,29,50)] <- 1 \\
A[16,c(7,10,17,21,22,29)] <- 1; A[17,c(7,9,13,16,19,29)] <- 1 \\
A[18,c(4,20,28,33,55,56)] <- 1; A[19,c(1,5,9,13,17)] <- 1 \\
A[20,c(4,18,55)] <- 1; A[21,c(16,29,50)] <- 1; A[22,c(10,16)] <- 1 \\
A[23,c(9,29,34,36,37,39)] <- 1; A[24,c(27,30,31,44,47,48,55,56)] <- 1 \\
A[25,c(15,26,29)] <- 1; A[26,c(25,29,42,43)] <- 1 \\
A[27,c(24,31,32,55)] <- 1; A[28,c(4,18,33,45)] <- 1 \\
A[29,c(9,15,16,17,21,23,25,26,34,43,50)] <- 1 \\
A[30,c(24,38,42,44,45,56)] <- 1; A[31,c(14,24,27,32,35,46,47)] <- 1 \\
A[32,c(14,27,31,35)] <- 1; A[33,c(18,28,45,56)] <- 1 \\
A[34,c(23,29,39,40,42,43,51,52,54)] <- 1; A[35,c(14,31,32,37,46)] <- 1 \\
A[36,c(23,37,39,41)] <- 1; A[37,c(23,35,36,41,46)] <- 1 \\
A[38,c(30,42,44,49,51,54)] <- 1; A[39,c(23,34,36,40,41)] <- 1 \\
A[40,c(34,39,41,49,52)] <- 1; A[41,c(36,37,39,40,46,49,53)] <- 1 \\
A[42,c(26,30,34,38,43,51)] <- 1; A[43,c(26,29,34,42)] <- 1 \\
A[44,c(24,30,38,48,49)] <- 1; A[45,c(28,30,33,56)] <- 1 \\
A[46,c(31,35,37,41,47,53)] <- 1; A[47,c(24,31,46,48,49,53)] <- 1 \\
A[48,c(24,44,47,49)] <- 1; A[49,c(38,40,41,44,47,48,52,53,54)] <- 1 \\
A[50,c(15,21,29)] <- 1; A[51,c(34,38,42,54)] <- 1 \\
A[52,c(34,40,49,54)] <- 1; A[53,c(41,46,47,49)] <- 1 \\
A[54,c(34,38,49,51,52)] <- 1; A[55,c(18,20,24,27,56)] <- 1 \\
A[56,c(18,24,30,33,45,55)] <- 1 \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,2), epsilon=rep(0,N), rho=0, \\
\hspace*{0.27 in} log.sigma=0)) \\
MyData <- list(A=A, E=E, N=N, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=x, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0,N), 0, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:2] \\
\hspace*{0.27 in} epsilon <- parm[grep("epsilon", Data$parm.names)] \\
\hspace*{0.27 in} rho <- interval(parm[grep("rho", Data$parm.names)], -1, 1) \\
\hspace*{0.27 in} parm[grep("rho", Data$parm.names)] <- rho \\
\hspace*{0.27 in} epsilon.mu <- rho * rowSums(epsilon * Data$A) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} epsilon.prior <- sum(dnorm(epsilon, epsilon.mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} rho.prior <- dunif(rho, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(log(Data$E) + beta[1] + beta[2]*Data$x/10 + epsilon) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + epsilon.prior + rho.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=lambda, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Conditional Predictive Ordinate} \label{cpo}
For a more complete introduction to the conditional predictive ordinate (CPO), see the vignette entitled ``Bayesian Inference''. Following is a brief guide to the applied use of CPO.

To include CPO in any model that is to be updated with MCMC, calculate and monitor the record-level inverse of the likelihood, $\mathrm{InvL}_i$ for records $i=1,\dots,N$. $\mathrm{CPO}_i$ is the inverse of the posterior mean of $\mathrm{InvL}_i$. The inverse $\mathrm{CPO}_i$, or $\mathrm{ICPO}_i$, is the posterior mean of $\mathrm{InvL}_i$. ICPOs larger than 40 can be considered as possible outliers, and higher than 70 as extreme values.

Here, CPO is added to the linear regression example in section \ref{linear.reg}. In this data, record 6 is a possible outlier, and record 8 is an extreme value.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma", as.parm.names(list(InvL=rep(0,N)))) \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- dnorm(Data$y, mu, sigma, log=TRUE) \\
\hspace*{0.27 in} InvL <- 1 / exp(LL) \\
\hspace*{0.27 in} LL <- sum(LL) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,InvL), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Contingency Table} \label{contingency.table}
The two-way contingency table, matrix $\textbf{Y}$, can easily be extended to more dimensions. For this example, it is vectorized as $y$, and used like an ANOVA data set. Contingency table $\textbf{Y}$ has J rows and K columns. The cell counts are fit with Poisson regression, according to intercept $\alpha$, main effects $\beta_j$ for each row, main effects $\gamma_k$ for each column, and interaction effects $\delta_{j,k}$ for dependence effects. An omnibus (all cells) test of independence is done by estimating two models (one with $\delta$, and one without), and a large enough Bayes Factor indicates a violation of independence when the model with $\delta$ fits better than the model without $\delta$. In an ANOVA-like style, main effects contrasts can be used to distinguish rows or groups of rows from each other, as well as with columns.  Likewise, interaction effects contrasts can be used to test independence in groups of $\delta_{j,k}$ elements. Finally, single-cell interactions can be used to indicate violations of independence for a given cell, such as when zero is not within its 95\% probability interval. Although a little different, this example is similar to a method presented by \citet{albert97}. 
\subsection{Form}
$$\textbf{Y}_{j,k} \sim \mathcal{P}(\lambda_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\lambda_{j,k} = \exp(\alpha + \beta_j + \gamma_k + \delta_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \beta^2_\sigma), \quad j=1,\dots,J$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_k \sim \mathcal{N}(0, \gamma^2_\sigma), \quad k=1,\dots,K$$
$$\gamma_\sigma \sim \mathcal{HC}(25)$$
$$\delta_{j,k} \sim \mathcal{N}(0, \delta^2_\sigma)$$
$$\delta_\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 4 \#Rows \\
K <- 4 \#Columns \\
Y <- matrix(c(10,20,60,20, 40,30,10,40, 10,10,40,10, 40,50,1,40), J, K, \\
\hspace*{0.27 in} dimnames=list(c("Chrysler","Ford","Foreign","GM"), \\
\hspace*{0.27 in} c("I-4","I-6","V-6","V-8"))) \\
y <- as.vector(Y) \\
N <- length(y) \#Cells \\
r <- rep(1:J, N/J) \\
c <- rep(1,K) \\
for (k in 2:K) \{c <- c(c, rep(k, K))\} \\
mon.names <- c("LP","beta.sigma","gamma.sigma","delta.sigma") \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,J), gamma=rep(0,J), \\
\hspace*{0.27 in} log.b.sigma=0, log.g.sigma=0, log.d.sigma=0, \\
\hspace*{0.27 in} delta=matrix(0,J,K))) \\
MyData <- list(J=J, K=K, N=N, c=c, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, r=r, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,J), rep(0,K), rep(0,3), rep(0,J*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- exp(parm[grep("log.b.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} gamma.sigma <- exp(parm[grep("log.g.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} delta.sigma <- exp(parm[grep("log.d.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} delta <- matrix(parm[grep("delta", Data$parm.names)], \\
\hspace*{0.62 in} Data$J, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} beta.sigma.prior <- dhalfcauchy(beta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} gamma.sigma.prior <- dhalfcauchy(gamma.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} delta.sigma.prior <- dhalfcauchy(delta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, gamma.sigma, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnorm(delta, 0, delta.sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(alpha + beta[Data$r] + gamma[Data$c] + \\
\hspace*{0.62 in} diag(delta[Data$r,Data$c])) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + beta.sigma.prior + \\
\hspace*{0.62 in} gamma.prior + gamma.sigma.prior + delta.prior + \\
\hspace*{0.62 in} delta.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta.sigma, \\
\hspace*{0.62 in} gamma.sigma, delta.sigma), yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Covariance Separation Strategy} \label{cov.sep.strat}
A Seemingly Unrelated Regression (SUR) model is used to provide an example of a flexible way to estimate covariance or precision matrices with the ``separation strategy'' decomposition of \citet{barnard00}. For more information on SUR models, see section \ref{sur}.

The most common way of specifying a covariance matrix, such as for the multivariate normal distribution, may be with the conjugate inverse Wishart distribution. Alternatively, the conjugate Wishart distribution is often used for a precision matrix. The Wishart and inverse Wishart distributions, however, do not always perform well, due to only one parameter for variability, and usually in the case of small sample sizes or when its dimension approaches the sample size. There are several alternatives. This example decomposes a covariance matrix into a standard deviation vector and a correlation matrix, each of which are easy to understand (as opposed to setting priors on eigenvalues). A precision matrix may be decomposed similarly, though the separated components are interpreted differently.

\citet{barnard00} prefer to update the covariance separation strategy with Gibbs sampling rather than Metropolis-Hastings, though the form presented here works well in testing with Adaptive MCMC.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}_J(\mu_{t,j}, \Sigma), \quad t=1,\dots,T; \quad j=1,\dots,J$$
$$\mu_{t,1} = \alpha_1 + \alpha_2 \textbf{X}_{t-1,1} +  \alpha_3 \textbf{X}_{t-1,2}, \quad t=2,\dots,T$$
$$\mu_{t,2} = \beta_1 + \beta_2 \textbf{X}_{t-1,3} +  \beta_3 \textbf{X}_{t-1,4}, \quad t=2,\dots,T$$
$$\Sigma = \textbf{S} \textbf{R} \textbf{S}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\textbf{R}_{i,j} \sim \mathcal{N}(\rho_\mu, \rho^2_\sigma), \quad \textbf{R}_{i,j} \in [-1,1], \quad i=1,\dots,J$$
$$\textbf{S} = \sigma \textbf{I}_J$$
$$\rho_\mu \sim \mathcal{N}(0, 2), \quad \in [-1, 1]$$
$$\rho_\sigma \sim \mathcal{HC}(25), \quad \in (0, 1000]$$
$$\sigma_j \sim \mathcal{N}(\sigma_\mu, \sigma_\sigma)$$
$$\sigma_\mu \sim \mathcal{HN}(1000), \quad \in (0, 1000]$$
$$\sigma_\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 20 \#Time-periods \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
J <- 2 \#Number of dependent variables \\
Y <- matrix(c(IG,IW), T, J) \\
R <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} R=diag(J), rho.mu=0, rho.sigma=0, log.sigma=rep(0,J), sigma.mu=0, \\
\hspace*{0.27 in} log.sig.sigma=0), uppertri=c(0,0,1,0,0,0,0,0)) \\
MyData <- list(J=J, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, \\
\hspace*{0.27 in} VW=VW, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), upper.triangle(R, diag=TRUE),
\hspace*{0.27 in} rep(0,2), rep(0,J), rep(1,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} rho.mu <- interval(parm[grep("rho.mu", Data$parm.names)], -1, 1) \\
\hspace*{0.27 in} parm[grep("rho.mu", Data$parm.names)] <- rho.mu \\
\hspace*{0.27 in} rho.sigma <- interval(parm[grep("rho.sigma", Data$parm.names)], \\
\hspace*{0.62 in} .Machine$double.eps, 1000) \\
\hspace*{0.27 in} parm[grep("rho.sigma", Data$parm.names)] <- rho.sigma \\
\hspace*{0.27 in} sigma.mu <- interval(parm[grep("sigma.mu", Data$parm.names)], \\
\hspace*{0.62 in} .Machine$double.eps, 1000) \\
\hspace*{0.27 in} parm[grep("sigma.mu", Data$parm.names)] <- sigma.mu \\
\hspace*{0.27 in} sigma.sigma <- exp(parm[grep("log.sig.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:3] \\
\hspace*{0.27 in} beta <- parm[4:6] \\
\hspace*{0.27 in} R <- as.parm.matrix(R, Data$J, parm, Data, a=-1, b=1) \\
\hspace*{0.27 in} parm[grep("R", Data$parm.names)] <- upper.triangle(R, diag=TRUE) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} S <- diag(Data$J); diag(S) <- sigma \\
\hspace*{0.27 in} Sigma <- S \%*\% R \%*\% S \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} rho.mu.prior <- dtrunc(rho.mu, "norm", a=-1, b=1, mean=0, sd=2, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} rho.sigma.prior <- dhalfcauchy(rho.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} sigma.mu.prior <- dhalfnorm(sigma.mu, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.sigma.prior <- dhalfcauchy(sigma.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} R.prior <- sum(dtrunc(upper.triangle(R, diag=TRUE), "norm", \\
\hspace*{0.62 in} a=-1, b=1, mean=rho.mu, sd=rho.sigma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dnorm(sigma, sigma.mu, sigma.sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} mu[-1,1] <- alpha[1] + alpha[2]*Data$CG[-Data$T] + \\
\hspace*{0.62 in} alpha[3]*Data$VG[-Data$T] \\
\hspace*{0.27 in} mu[-1,2] <- beta[1] + beta[2]*Data$CW[-Data$T] + \\
\hspace*{0.62 in} beta[3]*Data$VW[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dmvn(Data$Y[-1,], mu[-1,], Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + R.prior + rho.mu.prior + \\
\hspace*{0.62 in} rho.sigma.prior + sigma.prior + sigma.mu.prior + \\
\hspace*{0.62 in} sigma.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Discrete Choice, Conditional Logit} \label{conditional.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C))) \\
MyData <- list(C=C, J=J, K=K, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(tcrossprod(gamma, Data$Z), Data$J), Data$N, Data$J) \\
\hspace*{0.27 in} mu[,-Data$J] <- mu[,-Data$J] + tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(p) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Discrete Choice, Mixed Logit} \label{mixed.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(\zeta_{\mu[c]}, \zeta^2_{\sigma[c]})$$
$$\zeta_{\mu[c]} \sim \mathcal{N}(0, 1000)$$
$$\zeta_{\sigma[c]} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- c("LP", as.parm.names(list(zeta.sigma=rep(0,C)))) \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C), \\
\hspace*{0.27 in} zeta.mu=rep(0,C), log.zeta.sigma=rep(0,C))) \\
MyData <- list(C=C, J=J, K=K, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,N*C), rep(0,C), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} zeta.mu <- parm[grep("zeta.mu", Data$parm.names)] \\
\hspace*{0.27 in} zeta.sigma <- exp(parm[grep("log.zeta.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, matrix(zeta.mu, Data$N, Data$C, \\
\hspace*{0.62 in} byrow=TRUE), matrix(zeta.sigma, Data$N, Data$C, byrow=TRUE), \\
\hspace*{0.27 in} log=TRUE)) \\
\hspace*{0.27 in} zeta.mu.prior <- sum(dnormv(zeta.mu, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.sigma.prior <- sum(dhalfcauchy(zeta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(rowSums(gamma * Data$Z),Data$J), Data$N, Data$J) \\
\hspace*{0.27 in} mu[,-Data$J] <- tcrossprod(Data$X, beta) + gamma * Data$Z \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(p) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + zeta.mu.prior + zeta.sigma.prior\\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,zeta.sigma), \\
\hspace*{0.62 in} yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Discrete Choice, Multinomial Probit} \label{dc.mnp}
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
 \[\textbf{Z}_{i,j} \in \left\{
 \begin{array}{l l}
  $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
  $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K} + \textbf{W} \gamma[a,1:C]$$
 \[\textbf{a} = \left\{
 \begin{array}{l l}
  $1$ & \quad \mbox{if $\textbf{y}_i < J$}\\
  $2$ \\ \end{array} \right. \]
$$\Sigma \sim \mathcal{IW}_{J+1}(\textbf{S}^{-1}), \quad \textbf{S} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\gamma_{1,1:C} \sim \mathcal{N}(0, 1000)$$
$$\gamma_{2,c} = - \gamma_{1,c}, \quad c=1,\dots,C$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{y <- x1 <- x2 <- w1 <- w2 <- c(1:30) \\
y[1:10] <- 1 \\
y[11:20] <- 2 \\
y[21:30] <- 3 \\
x1[1:10] <- rnorm(10, 25, 2.5) \\
x1[11:20] <- rnorm(10, 40, 4.0) \\
x1[21:30] <- rnorm(10, 35, 3.5) \\
x2[1:10] <- rnorm(10, 2.51, 0.25) \\
x2[11:20] <- rnorm(10, 2.01, 0.20) \\ 
x2[21:30] <- rnorm(10, 2.70, 0.27) \\
w1[1:10] <- 10 \\
w1[11:20] <- 4 \\
w1[21:30] <- 1 \\
w2[1:10] <- 40 \\
w2[11:20] <- 50 \\
w2[21:30] <- 100 \\
N <- length(y) \\
J <- length(unique(y)) \#Number of categories in y \\
K <- 3 \#Number of columns to be in design matrix X \\
S <- diag(J) \\
X <- matrix(c(rep(1,N),x1,x2),N,K) \\
C <- 2 \#Number of choice-based attributes \\
W <- matrix(c(w1,w2),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
sigma.temp <- as.parm.names(list(Sigma=diag(J)), uppertri=1) \\
parm.names <- c(sigma.temp[2:length(sigma.temp)], \\
\hspace*{0.27 in} as.parm.names(list(beta=matrix(0,(J-1),K), gamma=rep(0,C), \\
\hspace*{0.27 in} Z=matrix(0,N,J)))) \\
MyData <- list(J=J, K=K, N=N, S=S, W=W, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, length(upper.triangle(S, diag=TRUE)) - 1), \\
\hspace*{0.27 in} rep(0,(J-1)*K), rep(0,C), rep(0,N*J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} beta <- rbind(beta, colSums(beta)*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma <- rbind(gamma, gamma*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} Sigma <- as.parm.matrix(Sigma, Data$J, parm, Data, restrict=TRUE) \\
\hspace*{0.27 in} parm[grep("Sigma", Data$parm.names)] <- upper.triangle(Sigma, \\
\hspace*{0.62 in} diag=TRUE)[-1] \\
\hspace*{0.27 in} Z <- matrix(parm[grep("Z", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Sigma.prior <- dinvwishart(Sigma, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} Z.prior <- sum(dnormv(Z, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(c(rep(tcrossprod(gamma[1,], Data$W),J), \\
\hspace*{0.62 in} tcrossprod(gamma[2,], Data$W)),Data$N,Data$J) \\
\hspace*{0.27 in} mu <- mu + tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} Z <- ifelse(Z > 10, 10, Z); Z <- ifelse(\{Y == 0\} \& \{Z > 0\}, 0, Z) \\
\hspace*{0.27 in} Z <- ifelse(Z < -10, -10, Z); Z <- ifelse(\{Y == 1\} \& \{Z < 0\}, 0, Z) \\
\hspace*{0.27 in} parm[grep("Z", Data$parm.names)] <- as.vector(Z) \\
\hspace*{0.27 in} LL <- sum(dmvn(Z, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(Z) \\
\hspace*{0.27 in} \#eta <- exp(mu) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + Sigma.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Distributed Lag, Koyck} \label{dl.koyck}
This example applies Koyck or geometric distributed lags to $k=1,\dots,K$ discrete events in covariate $\textbf{x}$, transforming the covariate into a $N$ x $K$ matrix $\textbf{X}$ and creates a $N$ x $K$ lag matrix $\textbf{L}$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \sum^K_{k=1} \textbf{X}_{t,k} \beta \lambda^{\textbf{L}[t,k]}, \quad k=1,\dots,K, \quad t=2,\dots,T$$
$$\mu_1 = \alpha + \sum^K_{k=1} \textbf{X}_{1,k} \beta \lambda^{\textbf{L}[1,k]}, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\lambda \sim \mathcal{U}(0, 1)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in}  2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
x <- c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\
\hspace*{0.27 in} 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \\
T <- length(y) \\
K <- length(which(x != 0)) \\
L <- X <- matrix(0, T, K) \\
for (i in 1:K) \{ \\
\hspace*{0.27 in} X[which(x != 0)[i]:T,i] <- x[which(x != 0)[i]] \\
\hspace*{0.27 in} L[(which(x != 0)[i]):T,i] <- 0:(T - which(x != 0)[i])\} \\
mon.names <- "LP" \\
parm.names <- c("alpha","beta","lambda","phi","log.sigma") \\
MyData <- list(L=L, T=T, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0.5, 0, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; beta <- parm[2] \\
\hspace*{0.27 in} lambda <- interval(parm[3],0,1); parm[3] <- lambda \\
\hspace*{0.27 in} phi <- parm[4]; sigma <- exp(parm[5]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnormv(beta, 0, 1000, log=TRUE) \\ 
\hspace*{0.27 in} lambda.prior <- dunif(lambda, 0, 1, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) + \\
\hspace*{0.62 in} rowSums(Data$X * beta * lambda\textasciicircum Data$L) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior + phi.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Exponential Smoothing} \label{exp.smo}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_t = \alpha \textbf{y}_{t-1} + (1 - \alpha) \mu_{t-1}, \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{U}(0,1)$$
$$\sigma \sim \mathcal{HC}$$
\subsection{Data}
\code{T <- 10 \\
y <- rep(0,T) \\
y[1] <- 0 \\
for (t in 2:T) \{y[t] <- y[t-1] + rnorm(1,0,0.1)\} \\
mon.names <- c("LP", "sigma") \\
parm.names <- c("alpha","log.sigma") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1], 0, 1); parm[1] <- alpha \\
\hspace*{0.27 in} sigma <- exp(parm[2]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 0, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- y \\
\hspace*{0.27 in} mu[-1] <- alpha*Data$y[-1] \\
\hspace*{0.27 in} mu[-1] <- mu[-1] + (1 - alpha) * mu[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[-1], mu[-Data$T], sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Factor Analysis, Approximate Dynamic} \label{adfa}
The Approximate Dynamic Factor Analysis (ADFA) model has many names, including the approximate factor model and approximate dynamic factor model. An ADFA is a Dynamic Factor Analysis (DFA) in which the factor scores of the dynamic factors are approximated with principal components. This is a combination of principal components and common factor analysis, in which the factor loadings of common factors are estimated from the data and factor scores are estimated from principal components. This is a two-stage model: principal components are estimated in the first stage and a decision is made regarding how many principal components to retain, and ADFA is estimated in the second stage. For more information on DFA, see section \ref{dfa}.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=2,\dots,T, \quad j=1,\dots,J$$
$$\mu_{t,} = \textbf{F}_{t-1,} \Lambda$$
$$\Lambda_{p,j} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad j=1,\dots,J$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{T <- 10 \#Number of time-periods \\
J <- 20 \#Number of variables \\
P <- 5 \#Number of approximate dynamic factors \\
Lambda <- matrix(runif(J*P,-1,1), P, J) \\
Sigma <- matrix(runif(P*P), P, P); diag(Sigma) <- runif(P)*5 \\
Sigma <- as.symmetric.matrix(Sigma); Sigma <- as.positive.definite(Sigma) \\
F <- rmvn(T, rep(0,P), Sigma) \\
Y <- tcrossprod(F, t(Lambda)) \\
PCA <- prcomp(Y, scale=TRUE) \\
F <- PCA$x[,1:P] \\
mon.names <- c("LP", paste("ynew[", 1:J, "]", sep="")) \\
parm.names <- as.parm.names(list(Lambda=matrix(0,P,J), log.sigma=rep(0,J))) \\
MyData <- list(F=F, J=J, P=P, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,P*J), rep(1,J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} Lambda <- matrix(parm[1:(Data$P*Data$J)], Data$P, Data$J) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rbind(rep(0, Data$J), tcrossprod(F[-Data$T,], t(Lambda))) \\
\hspace*{0.27 in} ynew <- tcrossprod(F[Data$T,], t(Lambda)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, matrix(sigma, Data$T, Data$J, byrow=TRUE), \\
\hspace*{0.27 in}      log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + Lambda.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), yhat=mu, \\
\hspace*{0.27 in}      parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Factor Analysis, Confirmatory} \label{cfa}
Factor scores are in matrix \textbf{F}, factor loadings for each variable are in vector $\lambda$, and $\textbf{f}$ is a vector that indicates which variable loads on which factor.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu = \alpha^T + \textbf{F}_{1:N,\textbf{f}} \lambda^T$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\lambda_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$ 
\subsection{Data}
\code{data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
\hspace*{0.27 in} swiss$Catholic, swiss$Infant.Mortality) \\
M <- ncol(Y) \#Number of variables \\
N <- nrow(Y) \#Number of records \\
P <- 3 \#Number of factors \\
f <- c(1,3,2,2,1) \#Indicator f for the factor for each variable m \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(F=matrix(0,N,P), lambda=rep(0,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.sigma=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, f=f, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,M), upper.triangle(S, diag=TRUE), \\
\hspace*{0.27 in} rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} lambda <- parm[grep("lambda", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} F <- matrix(parm[grep("F", Data$parm.names)], Data$N, Data$P) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$P, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dnormv(lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- sum(dmvnp(F, Data$gamma, Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$M, byrow=TRUE) + F[,Data$f] * \\
\hspace*{0.62 in} matrix(lambda, Data$N, Data$M, byrow=TRUE) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + lambda.prior + sigma.prior + F.prior + \\
\hspace*{0.62 in} Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Analysis, Dynamic} \label{dfa}
The factor scores in \textbf{F} are dynamic with respect to time, and are estimated as in a state space model (SSM) or dynamic linear model (DLM) with constant variance in the state vector. For more information on SSMs, see section \ref{ssm.lin.reg}. For more information on exploratory factor analysis, see section \ref{efa}.
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=2,\dots,T, \quad j=1,\dots,J$$
$$\mu_{2:T,} = \textbf{F}_{1:(T-1),} \Lambda$$
$$\textbf{F}_{1,1:P} \sim \mathcal{N}_P(0, \Omega^{-1})$$
$$\textbf{F}_{t,1:P} \sim \mathcal{N}_P(\textbf{F}_{t-1,1:P}, \Omega^{-1}), \quad t=2,\dots,T$$
$$\Lambda_{p,j} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad j=1,\dots,J$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
\subsection{Data}
\code{T <- 10 \#Number of time-periods \\
J <- 20 \#Number of time-series \\
P <- 3 \#Number of dynamic factors \\
Lambda <- matrix(runif(J*P,-1,1), P, J) \\
Sigma <- matrix(runif(P*P), P, P); diag(Sigma) <- runif(P)*5 \\
Sigma <- as.symmetric.matrix(Sigma); Sigma <- as.positive.definite(Sigma) \\
F <- rmvn(T, rep(0,P), Sigma) \\
Y <- tcrossprod(F, t(Lambda)) \\
S <- diag(P) \\
mon.names <- c("LP", paste("ynew[", 1:J, "]", sep="")) \\
parm.names <- as.parm.names(list(F=matrix(0,T,P), Omega=diag(P), \\
\hspace*{0.27 in} Lambda=matrix(0,P,J), log.sigma=rep(0,J)), uppertri=c(0,1,0,0)) \\
MyData <- list(J=J, P=P, S=S, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
Dyn <- matrix(".", T, P) \\
for (t in 1:T) \{for (p in 1:P) \{ \\
\hspace*{0.27 in} Dyn[t,p] <- paste("F[",t,",",p,"]", sep="")\}\} \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,T*P), S[upper.tri(S, diag=TRUE)], rep(0,P*J), \\
\hspace*{0.27 in} rep(0,J)) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} F <- matrix(parm[1:(Data$T*Data$P)], Data$T, Data$P) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$P, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- Omega[upper.tri(Omega, \\
\hspace*{0.62 in} diag=TRUE)] \\
\hspace*{0.27 in} Lambda <- matrix(parm[grep("Lambda", Data$parm.names)], Data$P, Data$J) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} F.prior <- sum(dmvnp(F, rbind(rep(0, Data$P), F[-Data$T,]), Omega, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$P+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rbind(rep(0, Data$J), tcrossprod(F[-Data$T,], t(Lambda))) \\
\hspace*{0.27 in} ynew <- tcrossprod(F[Data$T,], t(Lambda)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, matrix(sigma, Data$T, Data$J, byrow=TRUE), \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + F.prior + Omega.prior + Lambda.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Analysis, Exploratory} \label{efa}
Factor scores are in matrix \textbf{F} and factor loadings are in matrix $\Lambda$.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu = \alpha^T + \textbf{F} \Lambda$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\gamma_p = 0, \quad p=1,\dots,P$$
$$\Lambda_{p,m} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
\subsection{Data}
\code{M <- 10 \#Number of variables \\
N <- 20 \#Number of records \\
P <- 3 \#Number of factors \\
alpha <- runif(M)*10 \\
Lambda <- matrix(runif(M*P,-1,1), P, M) \\
Sigma <- matrix(runif(P*P), P, P); diag(Sigma) <- 1 + runif(P)*5 \\
Sigma <- as.symmetric.matrix(Sigma); Sigma <- as.positive.definite(Sigma) \\
F <- rmvn(N, rep(0,P), Sigma) \\
Y <- matrix(alpha, N, M, byrow=TRUE) + tcrossprod(F, t(Lambda)) \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(F=matrix(0,N,P), Lambda=matrix(0,P,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.sigma=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, gamma=gamma, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,P*M), upper.triangle(S, diag=TRUE), \\
\hspace*{0.27 in} rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} F <- matrix(parm[grep("F", Data$parm.names)], Data$N, Data$P) \\
\hspace*{0.27 in} Lambda <- matrix(parm[grep("Lambda", Data$parm.names)], \\
\hspace*{0.62 in} Data$P, Data$M) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$P, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} F.prior <- sum(dmvnp(F, Data$gamma, Omega, log=TRUE)) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnormv(Lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$M, byrow=TRUE) + \\
\hspace*{0.62 in} tcrossprod(F, t(Lambda)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, matrix(sigma, Data$N, Data$M, \\
\hspace*{0.62 in} byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + F.prior + Lambda.prior + Omega.prior + alpha.prior +
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Regression} \label{factor.reg}
This example of factor regression is constrained to the case where the number of factors is equal to the number of independent variables (IVs) less the intercept. The purpose of this form of factor regression is to orthogonalize the IVs with respect to $\textbf{y}$, rather than variable reduction. This method is the combination of confirmatory factor analysis in section \ref{cfa} and linear regression in section \ref{linear.reg}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\nu, \sigma^2_{J+1})$$
$$\nu = \mu \beta$$
$$\mu_{i,1} = 1$$
$$\mu_{i,j+1} = \mu_{i,j}, \quad j=1,\dots,J$$
$$\textbf{X}_{i,j} \sim \mathcal{N}(\mu_{i,j}, \sigma^2_j), \quad i=1,\dots,N, \quad j=2,\dots,J$$
$$\mu_{i,j} = \alpha_j + \lambda_j \textbf{F}_{i,j}, \quad i=1,\dots,N, \quad j=2,\dots,J$$
$$\textbf{F}_{i,1:J} \sim \mathcal{N}_{J-1}(0, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\lambda_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,(J+1)$$
$$\Omega \sim \mathcal{W}_N(\textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- 3 \\
y <- log(demonsnacks$Calories) \\
X <- as.matrix(demonsnacks[,c(7,8,10)]) \\
for (j in 1:J) \{X[,j] <- CenterScale(X[,j])\} \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,J), beta=rep(0,J+1), \\
\hspace*{0.27 in} lambda=rep(0,J), log.sigma=rep(0,J+1), F=matrix(0,N,J), \\
\hspace*{0.27 in} Omega=diag(J)), uppertri=c(0,0,0,0,0,1)) \\
MyData <- list(J=J, N=N, S=S, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J+1), rep(0,J), rep(0,J+1), \\
\hspace*{0.27 in} rep(0,N*J), upper.triangle(S, diag=TRUE))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} lambda <- parm[grep("lambda", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} F <- matrix(parm[grep("F", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$J, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dnormv(lambda, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- sum(dmvnp(F, rep(0,Data$J), Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha, Data$N, Data$J, byrow=TRUE) + F * \\
\hspace*{0.62 in} matrix(lambda, Data$N, Data$J, byrow=TRUE) \\
\hspace*{0.27 in} nu <- tcrossprod(beta, cbind(rep(1,Data$N),mu)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$X, mu, matrix(sigma[1:Data$J], Data$N, Data$J, \\
\hspace*{0.62 in} byrow=TRUE)), dnorm(Data$y, nu, sigma[Data$J+1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior + sigma.prior + \\
\hspace*{0.62 in} F.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=nu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Gamma Regression} \label{gamma.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{G}(\lambda \tau, \tau)$$
$$\lambda = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 20 \\
J <- 3 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- round(exp(tcrossprod(X, t(beta)))) + 0.1 \#Must be > 0 \\
mon.names <- c("LP","sigma2") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.tau=0)) \\
MyData <- list(J=J, N=N, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} tau <- exp(parm[grep("log.tau", Data$parm.names)]) \\
\hspace*{0.27 in} sigma2 <- 1/tau \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dgamma(Data$y, tau*lambda, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma2), yhat=lambda, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{GARCH(1,1)} \label{garch}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T + \theta_3 \sigma^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1} + \theta_3 \sigma^2_{t-1}$$
$$\theta_k = \frac{1}{1 + \exp(-\theta_k)}, \quad k=1,\dots,3$$
$$\theta_k \sim \mathcal{N}(0, 1000) \in [-10,10], \quad k=1,\dots,3$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","logit.theta[1]","logit.theta[2]", \\
\hspace*{0.27 in} "logit.theta[3]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2] \\
\hspace*{0.27 in} theta <- invlogit(interval(parm[grep("logit.theta", \\
\hspace*{0.62 in} Data$parm.names)], -10, 10)) \\
\hspace*{0.27 in} parm[grep("logit.theta", Data$parm.names)] <- logit(theta) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnormv(theta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(theta[1], theta[1] + theta[2]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[3]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- theta[1] + theta[2]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[3]*sigma2[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{GARCH-M(1,1)} \label{garchm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \omega + \theta_1 \epsilon^2_T + \theta_2 \sigma^2_T$$
$$\sigma^2_t = \omega + \theta_1 \epsilon^2_{t-1} + \theta_2 \sigma^2_{t-1}$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta_k \sim \mathcal{U}(0, 1), \quad k=1,\dots,2$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","log.omega","theta[1]", "theta[2]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0.5,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; delta <- parm[3] \\
\hspace*{0.27 in} omega <- exp(parm[4]) \\
\hspace*{0.27 in} parm[5:6] <- theta <- interval(parm[5:6], 1e-10, 1-1e-5) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnormv(phi, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnormv(delta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dunif(theta, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(omega, omega + theta[1]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[2]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- omega + theta[1]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[2]*sigma2[Data$T] \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, mu, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + omega.prior + \\
\hspace*{0.62 in} theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Geographically Weighted Regression} \label{gwr}
\subsection{Form}
$$\textbf{y}_{i,k} \sim \mathcal{N}(\mu_{i,k}, \tau^{-1}_{i,k}), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\mu_{i,1:N} = \textbf{X} \beta_{i,1:J}$$
$$\tau = \frac{1}{\sigma^2} \textbf{w} \nu$$
$$\textbf{w} = \frac{\exp(-0.5 \textbf{Z}^2)}{\textbf{h}}$$
$$\alpha \sim \mathcal{U}(1.5, 100)$$
$$\beta_{i,j} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{h} \sim \mathcal{N}(0.1, 1000) \in [0.1, \infty]$$
$$\nu_{i,k} \sim \mathcal{G}(\alpha, 2), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\sigma_i \sim \mathcal{HC}(25), \quad i=1,\dots,N$$
\subsection{Data}
\code{crime <-   c(18.802, 32.388, 38.426,  0.178, 15.726, 30.627, 50.732, \\
\hspace*{0.27 in} 26.067, 48.585, 34.001, 36.869, 20.049, 19.146, 18.905, 27.823, \\
\hspace*{0.27 in} 16.241,  0.224, 30.516, 33.705, 40.970, 52.794, 41.968, 39.175, \\
\hspace*{0.27 in} 53.711, 25.962, 22.541, 26.645, 29.028, 36.664, 42.445, 56.920, \\
\hspace*{0.27 in} 61.299, 60.750, 68.892, 38.298, 54.839, 56.706, 62.275, 46.716, \\
\hspace*{0.27 in} 57.066, 54.522, 43.962, 40.074, 23.974, 17.677, 14.306, 19.101, \\
\hspace*{0.27 in} 16.531, 16.492) \\
income <-  c(21.232,  4.477, 11.337,  8.438, 19.531, 15.956, 11.252, \\
\hspace*{0.27 in} 16.029,  9.873, 13.598,  9.798, 21.155, 18.942, 22.207, 18.950, \\
\hspace*{0.27 in} 29.833, 31.070, 17.586, 11.709,  8.085, 10.822,  9.918, 12.814, \\
\hspace*{0.27 in} 11.107, 16.961, 18.796, 11.813, 14.135, 13.380, 17.017,  7.856, \\
\hspace*{0.27 in}  8.461,  8.681, 13.906, 14.236,  7.625, 10.048,  7.467,  9.549, \\
\hspace*{0.27 in}  9.963, 11.618, 13.185, 10.655, 14.948, 16.940, 18.739, 18.477, \\
\hspace*{0.27 in} 18.324, 25.873) \\
housing <- c(44.567, 33.200, 37.125, 75.000, 80.467, 26.350, 23.225, \\
\hspace*{0.27 in} 28.750, 18.000, 96.400, 41.750, 47.733, 40.300, 42.100, 42.500, \\
\hspace*{0.27 in} 61.950, 81.267, 52.600, 30.450, 20.300, 34.100, 23.600, 27.000, \\
\hspace*{0.27 in} 22.700, 33.500, 35.800, 26.800, 27.733, 25.700, 43.300, 22.850, \\
\hspace*{0.27 in} 17.900, 32.500, 22.500, 53.200, 18.800, 19.900, 19.700, 41.700, \\
\hspace*{0.27 in} 42.900, 30.600, 60.000, 19.975, 28.450, 31.800, 36.300, 39.600, \\
\hspace*{0.27 in} 76.100, 44.333) \\
easting <- c(35.62, 36.50, 36.71, 33.36, 38.80, 39.82, 40.01, 43.75, \\
\hspace*{0.27 in} 39.61, 47.61, 48.58, 49.61, 50.11, 51.24, 50.89, 48.44, 46.73, \\
\hspace*{0.27 in} 43.44, 43.37, 41.13, 43.95, 44.10, 43.70, 41.04, 43.23, 42.67, \\
\hspace*{0.27 in} 41.21, 39.32, 41.09, 38.3,  41.31, 39.36, 39.72, 38.29, 36.60, \\
\hspace*{0.27 in} 37.60, 37.13, 37.85, 35.95, 35.72, 35.76, 36.15, 34.08, 30.32, \\
\hspace*{0.27 in} 27.94, 27.27, 24.25, 25.47, 29.02) \\
northing <- c(42.38, 40.52, 38.71, 38.41, 44.07, 41.18, 38.00, 39.28, \\
\hspace*{0.27 in} 34.91, 36.42, 34.46, 32.65, 29.91, 27.80, 25.24, 27.93, 31.91, \\
\hspace*{0.27 in} 35.92, 33.46, 33.14, 31.61, 30.40, 29.18, 28.78, 27.31, 24.96, \\
\hspace*{0.27 in} 25.90, 25.85, 27.49, 28.82, 30.90, 32.88, 30.64, 30.35, 32.09, \\
\hspace*{0.27 in} 34.08, 36.12, 36.30, 36.40, 35.60, 34.66, 33.92, 30.42, 28.26, \\
\hspace*{0.27 in} 29.85, 28.21, 26.69, 25.71, 26.58) \\
N <- length(crime) \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(c(rep(1,N), income, housing),N,J) \\
D <- as.matrix(dist(cbind(northing,easting), diag=TRUE, upper=TRUE)) \\
Z <- D / sd(as.vector(D)) \\
y <- matrix(0,N,N); for (i in 1:N) \{for (k in 1:N) \{y[i,k] <- crime[k]\}\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=0, beta=matrix(0,N,J), log.h=0, \\
\hspace*{0.27 in} log.nu=matrix(0,N,N), log.sigma=rep(0,N))) \\
MyData <- list(J=J, N=N, X=X, Z=Z, latitude=northing, longitude=easting, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(1,1.5,100), rep(0,N*J), log(1), rep(0,N*N), \\
\hspace*{0.27 in} log(rep(100,N)))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1], 1.5, 100); parm[1] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} h <- exp(parm[2+(N*J)]) + 0.1 \\
\hspace*{0.27 in} nu <- exp(matrix(parm[grep("log.nu", Data$parm.names)], \\
\hspace*{0.62 in} Data$N, Data$N)) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 1.5, 100, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} h.prior <- dtrunc(h, "normv", a=0.1, b=Inf, mean=0.1, var=1000, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dgamma(nu, alpha, 2, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} w <- exp(-0.5 * Data$Z\textasciicircum 2) / h \\
\hspace*{0.27 in} tau <- (1/sigma\textasciicircum 2) * w * nu \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dnormp(Data$y, mu, tau, log=TRUE)) \\
\hspace*{0.27 in} \#WSE <- w * nu * (Data$y - mu)\textasciicircum 2; w.y <- w * nu * Data$y \\
\hspace*{0.27 in} \#WMSE <- rowMeans(WSE); y.w <- rowSums(w.y) / rowSums(w) \\
\hspace*{0.27 in} \#LAR2 <- 1 - WMSE / sd(y.w)\textasciicircum 2 \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + h.prior + nu.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Inverse Gaussian Regression} \label{ig.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}^{-1}(\mu, \lambda)$$
$$\mu = \exp(\textbf{X}\beta) + C$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\lambda \sim \mathcal{HC}(25)$$
where $C$ is a small constant, such as 1.0E-10. 
\subsection{Data}
\code{N <- 100 \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta.orig <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- exp(tcrossprod(X, t(beta.orig)) + e) \\
mon.names <- c("LP","lambda") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.lambda=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} lambda <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- dhalfcauchy(lambda, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) + 1.0E-10 \\
\hspace*{0.27 in} LL <- sum(dinvgaussian(Data$y, mu, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + lambda.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,lambda), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Kriging} \label{kriging}
This is an example of universal kriging of $\textbf{y}$ given $\textbf{X}$, regression effects $\beta$, and spatial effects $\zeta$. Euclidean distance between spatial coordinates (longitude and latitude) is used for each of $i=1,\dots,N$ records of $\textbf{y}$. An additional record is created from the same data-generating process to compare the accuracy of interpolation. For the spatial component, $\phi$ is the rate of spatial decay and $\kappa$ is the scale. $\kappa$ is often difficult to identify, so it is set to 1 (Gaussian), but may be allowed to vary up to 2 (Exponential). In practice, $\phi$ is also often difficult to identify. While $\Sigma$ is spatial covariance, spatial correlation is $\rho = \exp(-\phi \textbf{D})$. To extend this to a large data set, consider the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$ \mu = \textbf{X} \beta + \zeta$$
$$ \textbf{y}^{new} = \textbf{X} \beta + \sum^N_{i=1} \left ( \frac{\rho_i}{\sum \rho} \zeta_i \right )$$
$$ \rho = \exp(-\phi \textbf{D}^{new})^\kappa$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp(-\phi \textbf{D})^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_j \sim \mathcal{HC}(25) \in [1,\infty], \quad j=1,\dots,2$$
$$ \phi \sim \mathcal{U}(1, 5)$$
$$ \zeta_\mu = 0$$
$$ \kappa = 1$$
\subsection{Data}
\code{N <- 20 \\
longitude <- runif(N+1,0,100) \\
latitude <- runif(N+1,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma <- 10000 * exp(-1.5 * D) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,N+1), Sigma), 2, mean)) \\
beta <- c(50,2) \\
X <- matrix(runif((N+1)*2,-2,2),(N+1),2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
y <- mu + zeta \\
longitude.new <- longitude[N+1]; latitude.new <- latitude[N+1] \\
Xnew <- X[N+1,]; ynew <- y[N+1] \\
longitude <- longitude[1:N]; latitude <- latitude[1:N] \\
X <- X[1:N,]; y <- y[1:N] \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
D.new <- sqrt((longitude - longitude.new)\textasciicircum 2 + (latitude - latitude.new)\textasciicircum 2) \\
mon.names <- c("LP","ynew") \\
parm.names <- as.parm.names(list(zeta=rep(0,N), beta=rep(0,2), \\
\hspace*{0.27 in} sigma=rep(0,2), phi=0)) \\
MyData <- list(D=D, D.new=D.new, N=N, X=X, Xnew=Xnew, latitude=latitude, \\
\hspace*{0.27 in} longitude=longitude, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), rep(0,2), rep(1,2), 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[grep("sigma", Data$parm.names)], 1, Inf) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-phi * Data$D)\textasciicircum kappa \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0, Data$N), Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma - 1, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, 1, 5, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Interpolation \\
\hspace*{0.27 in} rho <- exp(-phi * Data$D.new)\textasciicircum kappa \\
\hspace*{0.27 in} ynew <- sum(beta * Data$Xnew) + sum(rho / sum(rho) * zeta) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) + zeta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sigma.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Kriging, Predictive Process} \label{kriging.pp}
The first $K$ of $N$ records in $\textbf{y}$ are used as knots for the parent process, and the predictive process involves records $(K+1),\dots,N$. For more information on kriging, see section \ref{kriging}.
\subsection{Form}
$$ \textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$ \mu_{1:K} = \textbf{X}_{1:K,1:J} \beta + \zeta$$
$$ \mu_{(K+1):N} = \textbf{X}_{(K+1):N,1:J} \beta + \sum^{N-K}_{p=1} \frac{\lambda_{p,1:K}}{\sum^{N-K}_{q=1} \lambda_{q,1:K}} \zeta^T$$
$$ \lambda = \exp(-\phi \textbf{D}_P)^\kappa$$
$$ \textbf{y}^{new} = \textbf{X} \beta + \sum^K_{k=1} (\frac{\rho_k}{\sum \rho} \zeta_k)$$
$$ \rho = \exp(-\phi \textbf{D}^{new})^\kappa$$
$$ \zeta \sim \mathcal{N}_K(0, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp(-\phi \textbf{D})^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
$$ \phi \sim \mathrm{N}(0, 1000) \in [0, \infty]$$
$$ \kappa = 1$$
\subsection{Data}
\code{N <- 100 \\
K <- 30 \#Number of knots \\
longitude <- runif(N+1,0,100) \\
latitude <- runif(N+1,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma <- 10000 * exp(-1.5 * D) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,N+1), Sigma), 2, mean)) \\
beta <- c(50,2) \\
X <- matrix(runif((N+1)*2,-2,2),(N+1),2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
y <- mu + zeta \\
longitude.new <- longitude[N+1]; latitude.new <- latitude[N+1] \\
Xnew <- X[N+1,]; ynew <- y[N+1] \\
longitude <- longitude[1:N]; latitude <- latitude[1:N] \\
X <- X[1:N,]; y <- y[1:N] \\
D <- as.matrix(dist(cbind(longitude[1:K],latitude[1:K]), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
D.P <- matrix(0, N-K, K) \\
for (i in (K+1):N) \{ \\
\hspace*{0.27 in} D.P[K+1-i,] <- sqrt((longitude[1:K] - longitude[i])\textasciicircum 2 + \\
\hspace*{0.62 in} (latitude[1:K] - latitude[i])\textasciicircum 2)\} \\
D.new <- sqrt((longitude[1:K] - longitude.new)\textasciicircum 2 + \\
\hspace*{0.27 in} (latitude[1:K] - latitude.new)\textasciicircum 2) \\
mon.names <- c("LP","sigma[1]","sigma[2]","ynew") \\
parm.names <- as.parm.names(list(zeta=rep(0,K), beta=rep(0,2), \\
\hspace*{0.27 in} sigma=rep(0,2), log.phi=0)) \\
MyData <- list(D=D, D.new=D.new, D.P=D.P, K=K, N=N, X=X, Xnew=Xnew, \\
\hspace*{0.27 in} latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), c(mean(y), 0), rep(0,2), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1 \\
\hspace*{0.27 in} sigma <- interval(parm[grep("sigma", Data$parm.names)], 0, Inf) \\
\hspace*{0.27 in} parm[grep("sigma", Data$parm.names)] <- sigma \\
\hspace*{0.27 in} phi <- exp(parm[grep("log.phi", Data$parm.names)]) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-phi * Data$D)\textasciicircum kappa \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0, Data$K), Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, 1, 5, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Interpolation \\
\hspace*{0.27 in} rho <- exp(-phi * Data$D.new)\textasciicircum kappa \\
\hspace*{0.27 in} ynew <- sum(beta * Data$Xnew) + sum(rho / sum(rho) * zeta) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} mu[1:Data$K] <- mu[1:Data$K] + zeta \\
\hspace*{0.27 in} lambda <- exp(-phi * Data$D.P)\textasciicircum kappa \\
\hspace*{0.27 in} mu[(Data$K+1):Data$N] <- mu[(Data$K+1):Data$N] + \\
\hspace*{0.62 in} rowSums(lambda / rowSums(lambda) * \\
\hspace*{0.62 in} matrix(zeta, Data$N - Data$K, Data$K, byrow=TRUE)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sigma.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Laplace Regression} \label{laplace.reg}
This linear regression specifies that $\textbf{y}$ is Laplace-distributed, where it is usually Gaussian or normally-distributed.  It has been claimed that it should be surprising that the normal distribution became the standard, when the Laplace distribution usually fits better and has wider tails \citep{kotz01}. Another popular alternative is to use the t-distribution (see Robust Regression in section \ref{robust.reg}), though it is more computationally expensive to estimate, because it has three parameters.  The Laplace distribution has only two parameters, location and scale like the normal distribution, and is computationally easier to fit.  This example could be taken one step further, and the parameter vector $\beta$ could be Laplace-distributed.  Laplace's Demon recommends that users experiment with replacing the normal distribution with the Laplace distribution.
\subsection{Form}
$$\textbf{y} \sim \mathcal{L}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rlaplace(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- c("LP", "sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dlaplace(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression} \label{linear.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- c("LP", "sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression, Frequentist} \label{linear.reg.freq}
By eliminating prior probabilities, a frequentist linear regression example is presented. Although frequentism is not endorsed here, the purpose of this example is to illustrate how the \pkg{LaplacesDemon} package can be used for Bayesian or frequentist inference.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- c("LL", "sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} Modelout <- list(LP=LL, Dev=-2*LL, Monitor=c(LL, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression, Hierarchical Bayesian} \label{linear.reg.hb}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(\gamma, \delta), \quad j=1,\dots,J$$
$$\gamma \sim \mathcal{N}(0, 1000)$$
$$\delta \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(\tau)$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","delta","sigma","tau") \\
parm.names <- as.parm.names(list(beta=rep(0,J), gamma=0, log.delta=0, \\
     log.sigma=0, log.tau=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0, rep(1,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- parm[Data$J+1] \\
\hspace*{0.27 in} delta <- exp(parm[Data$J+2]) \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+4]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+3]) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dnorm(gamma, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dhalfcauchy(delta, 25, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, gamma, delta, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, tau, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + delta.prior + sigma.prior + \\
\hspace*{0.62 in} tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,delta,sigma,tau), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Linear Regression, Multilevel} \label{linear.reg.ml}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_i = \textbf{X} \beta_{\textbf{m}[i],1:J}$$
$$\beta_{g,1:J} \sim \mathcal{N}_J(\gamma, \Omega^{-1}), \quad g=1,\dots,G$$
$$\Omega \sim \mathcal{W}_{J+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
where $\textbf{m}$ is a vector of length $N$, and each element indicates the multilevel group ($g=1,\dots,G$) for the associated record.
\subsection{Data}
\code{N <- 30 \\
J <- 2 \#\#\# Number of predictors (including intercept) \\
G <- 2 \#\#\# Number of Multilevel Groups \\
X <- matrix(rnorm(N,0,1),N,J); X[,1] <- 1 \\
Sigma <- matrix(runif(J*J,-1,1),J,J) \\
diag(Sigma) <- runif(J,1,5) \\
Sigma <- as.positive.definite(Sigma) \\
gamma <- runif(J,-1,1) \\
beta <- matrix(NA,G,J) \\
for (g in 1:G) \{beta[g,] <- rmvn(1, gamma, Sigma)\} \\
m <- rcat(N, rep(1,G)) \#\#\# Multilevel group indicator \\
y <- rowSums(beta[m,] * X) + rnorm(N,0,0.1) \\
S <- diag(J) \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=matrix(0,G,J), log.sigma=0, \\
\hspace*{0.27 in} gamma=rep(0,J), Omega=S), uppertri=c(0,0,0,1)) \\
MyData <- list(G=G, J=J, N=N, S=S, X=X, m=m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial.Values}
\code{Initial.Values <- c(rep(0,G*J), log(1), rep(0,J), \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[1:(Data$G * Data$J)], Data$G, Data$J) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$J, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dmvnp(beta, gamma, Omega, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 100, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rowSums(beta[Data$m,] * Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + Omega.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Linear Regression with Full Missingness} \label{linear.reg.full.miss}
With `full missingness', there are missing values for both the dependent variable (DV) and at least one independent variable (IV). Regarding initial values, only missing values for the IVs need to be specified. The `full likelihood` approach to full missingness is excellent as long as the model is identifiable. When it is not identifiable, imputation may be done in a previous stage.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\nu, \sigma^2_J)$$
$$\nu = \textbf{X} \beta$$
$$\mu_{,j} = (\textbf{X} | \textbf{A}) \Gamma_{,j-1}, \quad j=2,\dots,J$$
\[\textbf{A}_{i,j} = \left\{
\begin{array}{l l}
 $$\alpha_k$$ & \quad \mbox{if $\textbf{X}_{i,j} = \mathrm{NA}$}\\
 \textbf{X}_{i,j} \\ \end{array} \right. \]
$$\quad i=1,\dots,N, \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\Gamma_{j,l} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad l=1,\dots,(J-1)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,J$$
Missing values for IVs are in vector $\alpha$. These parameter values are inserted into design matrix \textbf{X} via matrix \textbf{A}, which indicates the position of missing values. Matrix $\Gamma$ contains parameters to predict the IVs as additive, linear functions of other IVs. Vector $\beta$ contains parameters to predict \textbf{y}.
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \#Design matrix X \\
M <- matrix(round(runif(N*J)-0.45),N,J); M[,1] <- 0 \#Missing indicators \\
X <- ifelse(M == 1, NA, X) \#Simulated X gets missings according to M \\
beta.orig <- runif(J,-2,2) \\
y <- as.vector(tcrossprod(X, t(beta.orig)) + rnorm(N,0,0.1)) \\
y[sample(1:N, round(N*.05))] <- NA \\
m <- ifelse(is.na(y), 1, 0) \#Missing indicator for vector y \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,length(which(is.na(X)))), \\
\hspace*{0.27 in} beta=rep(0,J), Gamma=matrix(0,J-1,J-1), log.sigma=rep(0,J))) \\
MyData <- list(J=J, M=M, N=N, X=X, m=m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,length(which(is.na(X)))), rep(0,J), \\
\hspace*{0.27 in} rep(0,(J-1)\textasciicircum 2), rep(1,J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} Gamma <- matrix(parm[grep("Gamma", Data$parm.names)], Data$J-1, \\
\hspace*{0.62 in} Data$J-1) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Gamma.prior <- sum(dnormv(Gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} A <- rep(0,Data$N*Data$J) \\
\hspace*{0.27 in} A[which(is.na(Data$X))] <- alpha \\
\hspace*{0.27 in} mu <- X.imputed <- ifelse(is.na(Data$X), 0, Data$X) + \\
\hspace*{0.62 in} matrix(A, Data$N, Data$J) \\
\hspace*{0.27 in} for (j in 2:Data$J) \{mu[,j] <- tcrossprod(X.imputed[,-j], \\
\hspace*{0.62 in} t(Gamma[,(j-1)]))\} \\
\hspace*{0.27 in} nu <- tcrossprod(X.imputed, t(beta)) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), nu, Data$y) \\
\hspace*{0.27 in} LL <- sum((1-Data$M[,-1]) * dnorm(X.imputed[,-1], mu[,-1], \\
\hspace*{0.62 in} matrix(sigma[1:(Data$J-1)], Data$N, Data$J-1), log=TRUE), \\
\hspace*{0.62 in} (1-Data$m) * dnorm(y.imputed, nu, sigma[Data$J], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + Gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=nu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Linear Regression with Missing Response} \label{linear.reg.miss.resp}
Initial values do not need to be specified for missing values in this response, $\textbf{y}$. Instead, at each iteration, missing values in $\textbf{y}$ are replaced with their estimate in $\mu$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
y[sample(1:N, round(N*0.05))] <- NA \\
M <- ifelse(is.na(y), 1, 0) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, M=M, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), mu, Data$y) \\
\hspace*{0.27 in} LL <- sum((1-Data$M) * dnorm(y.imputed, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{LSTAR} \label{lstar}
This is a Logistic Smooth-Threshold Autoregression (LSTAR), and is specified with a transition function that includes $\gamma$ as the shape parameter, $\textbf{y}$ as the transition variable, $\theta$ as the location parameter, and $d$ as the delay parameter.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\mu_t = \pi_t (\alpha_1 + \phi_1 \textbf{y}_{t-1}) + (1 - \pi_t) (\alpha_2 + \phi_2 \textbf{y}_{t-1}), \quad t=2,\dots,T$$
$$\pi_t = \frac{1}{1 + \exp(-(\gamma (\textbf{y}_{t-d} - \theta)))}$$
$$\alpha_j \sim \mathcal{N}(0, 1000) \in [\textbf{y}_{min}, \textbf{y}_{max}], \quad j=1,\dots,2$$
$$\phi_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$\gamma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{U}(\textbf{y}_{min}, \textbf{y}_{max})$$
$$\pi_1 \sim \mathcal{U}(0.001, 0.999)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(26.73, 26.75, 26.24, 25.94, 27.40, 26.14, 23.99, 23.08, 22.55, \\
\hspace*{0.27 in} 20.64, 23.28, 24.92, 25.07, 26.53, 28.14, 30.10, 27.43, 27.24, \\
\hspace*{0.27 in} 23.96, 25.85, 26.76, 26.05, 26.79, 26.69, 29.89, 29.09, 23.84, \\
\hspace*{0.27 in} 24.87, 24.47, 22.85, 22.05, 22.82, 22.99, 21.60, 20.32, 20.80, \\
\hspace*{0.27 in} 19.78, 19.87, 18.78, 19.64, 20.00, 21.51, 21.49, 21.96, 22.58, \\
\hspace*{0.27 in} 21.22, 22.34, 22.76, 18.37, 17.50, 17.55, 12.14,  4.76,  3.75, \\ 
\hspace*{0.27 in}  2.05,  2.69,  3.85,  4.72,  5.00,  3.31,  3.02,  3.15,  2.50, \\ 
\hspace*{0.27 in}  3.33,  3.95,  4.00,  3.86,  3.87,  3.51,  3.19,  2.39,  2.33, \\ 
\hspace*{0.27 in}  2.57,  2.80,  2.43,  2.43,  2.10,  2.31,  2.21,  2.11,  2.10, \\ 
\hspace*{0.27 in}  1.70,  1.35,  1.83,  1.55,  1.63,  1.91,  2.14,  2.41,  2.06, \\ 
\hspace*{0.27 in}  1.87,  2.11,  2.28,  2.26,  2.03,  2.06,  2.08,  1.91,  1.95, \\ 
\hspace*{0.27 in}  1.56,  1.44,  1.60,  1.77,  1.77,  1.95,  2.01,  1.65,  1.87, \\ 
\hspace*{0.27 in}  2.01,  1.84,  1.94,  1.93,  1.93,  1.75,  1.73,  1.80,  1.74, \\ 
\hspace*{0.27 in}  1.80,  1.75,  1.67,  1.60,  1.61,  1.55,  1.56,  1.57,  1.55, \\ 
\hspace*{0.27 in}  1.56,  1.57,  1.69,  1.66,  1.74,  1.64,  1.65,  1.62,  1.54, \\ 
\hspace*{0.27 in}  1.58,  1.49,  1.41,  1.42,  1.37,  1.45,  1.31,  1.37,  1.26, \\ 
\hspace*{0.27 in}  1.35,  1.41,  1.29,  1.28,  1.23,  1.08,  1.03,  1.00,  1.04, \\ 
\hspace*{0.27 in}  1.04,  0.92,  0.96,  0.90,  0.85,  0.78,  0.73,  0.59,  0.54, \\ 
\hspace*{0.27 in}  0.53,  0.41,  0.46,  0.52,  0.42,  0.42,  0.43,  0.43,  0.35, \\ 
\hspace*{0.27 in}  0.35,  0.35,  0.42,  0.41,  0.41,  0.50,  0.83,  0.96,  1.38, \\ 
\hspace*{0.27 in}  1.62,  1.26,  1.48,  1.39,  1.20,  1.10,  1.02,  0.95,  1.00, \\ 
\hspace*{0.27 in}  1.07,  1.14,  1.14,  1.10,  1.05,  1.08,  1.16,  1.42,  1.52, \\ 
\hspace*{0.27 in}  1.60,  1.69,  1.62,  1.29,  1.46,  1.43,  1.50,  1.46,  1.40, \\ 
\hspace*{0.27 in}  1.34,  1.41,  1.38,  1.38,  1.46,  1.73,  1.84,  1.95,  2.01, \\ 
\hspace*{0.27 in}  1.90,  1.81,  1.60,  1.84,  1.72,  1.83,  1.81,  1.78,  1.80, \\ 
\hspace*{0.27 in}  1.70,  1.70,  1.66,  1.67,  1.69,  1.66,  1.56,  1.47,  1.64, \\ 
\hspace*{0.27 in}  1.71,  1.66,  1.65,  1.60,  1.61,  1.61,  1.53,  1.48,  1.40, \\ 
\hspace*{0.27 in}  1.47,  1.53,  1.39,  1.41,  1.42,  1.46,  1.46,  1.33,  1.16) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew", paste("pi[", 1:T, "]", sep=""), \\
\hspace*{0.27 in} "pi.new") \\
parm.names <- as.parm.names(list(alpha=rep(0,2), phi=rep(0,2), log.gamma=0, \\
\hspace*{0.27 in} theta=0, pi=0, log.sigma=0)) \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), log(1), mean(y), 0.5, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[1:2] <- alpha <- interval(parm[1:2], min(Data$y), max(Data$y)) \\
\hspace*{0.27 in} parm[3:4] <- phi <- interval(parm[3:4], -1, 1) \\
\hspace*{0.27 in} gamma <- exp(parm[5]) \\
\hspace*{0.27 in} parm[6] <- theta <- interval(parm[6], min(Data$y), max(Data$y)) \\
\hspace*{0.27 in} pi <- interval(parm[7], 0.001, 0.999); parm[7] <- pi \\
\hspace*{0.27 in} sigma <- exp(parm[8]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dnorm(phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- dhalfcauchy(gamma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, min(Data$y), max(Data$y), log=TRUE) \\
\hspace*{0.27 in} pi.prior <- dunif(pi, 0.001, 0.999, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} pi <- c(pi, 1 / (1 + exp(-(gamma*(Data$y[-Data$T]-theta))))) \\
\hspace*{0.27 in} pi.new <- 1 / (1 + exp(-(gamma*(Data$y[Data$T]-theta)))) \\
\hspace*{0.27 in} mu <- pi * c(alpha[1], alpha[1] + phi[1]*Data$y[-Data$T]) + \\
\hspace*{0.62 in} (1-pi) * c(alpha[2], alpha[2] + phi[2]*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- pi.new * (alpha[1] + phi[1]*Data$y[Data$T]) + \\
\hspace*{0.62 in} (1-pi.new) * (alpha[2] + phi[2]*Data$y[Data$T]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + gamma.prior + theta.prior + \\
\hspace*{0.62 in} pi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew,pi,pi.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{MANCOVA} \label{mancova}
Since this is a multivariate extension of ANCOVA, please see the ANCOVA example in section \ref{ancova} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}_K(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]} + \textbf{X}_{1:N,3:(C+J)} \delta_{k,1:C}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\delta_{k,c} \sim \mathcal{N}(0, 1000)$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\Sigma = \Omega^{-1}$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 2 \#Number of covariates \\
J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- matrix(cbind(round(runif(N, 0.5, L+0.49)),round(runif(N,0.5,M+0.49)), \\
\hspace*{0.27 in} runif(C*N,0,1)), N, J + C) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
delta <- matrix(runif(K*C), K, C) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + \\
\hspace*{0.27 in} tcrossprod(delta[k,], X[,-c(1,2)]) + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} as.parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), delta=matrix(0,K,C), Omega=diag(K), \\
\hspace*{0.27 in} log.sigma=rep(0,2)), uppertri=c(0,0,0,0,1,0)) \\
MyData <- list(C=C, J=J, K=K, L=L, M=M, N=N, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} rep(0,C*K), upper.triangle(S, diag=TRUE), rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- matrix(c(parm[grep("beta", Data$parm.names)], rep(0,K)), \\
\hspace*{0.27 in} Data$K, Data$L) \\
\hspace*{0.27 in} beta[,L] <- -rowSums(beta[,-L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[grep("gamma", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,M] <- -rowSums(gamma[,-M]) \\
\hspace*{0.27 in} delta <- matrix(parm[grep("delta", Data$parm.names)], Data$K, Data$C) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$K, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]] + \\
\hspace*{0.62 in} tcrossprod(Data$X[,-c(1,2)], t(delta[k,]))\} \\
\hspace*{0.27 in} LL <- sum(dmvnp(Data$Y, mu, Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- apply(beta,1,sd) \\
\hspace*{0.27 in} s.gamma <- apply(gamma,1,sd) \\
\hspace*{0.27 in} s.epsilon <- apply(Data$Y - mu,2,sd) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} Omega.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{MANOVA} \label{manova}
Since this is a multivariate extension of ANOVA, please see the two-way ANOVA example in section \ref{anova.two.way} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}_K(\mu_{i,1:J}, \Omega^{-1}), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- matrix(cbind(round(runif(N, 0.5, L+0.49)),round(runif(N,0.5,M+0.49))), \\
\hspace*{0.27 in} N, J) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} as.parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), Omega=diag(K), log.sigma=rep(0,2)), \\
\hspace*{0.27 in} uppertri=c(0,0,0,1,0)) \\
MyData <- list(J=J, K=K, L=L, M=M, N=N, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE), rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- matrix(c(parm[grep("beta", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$L) \\
\hspace*{0.27 in} beta[,L] <- -rowSums(beta[,-L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[grep("gamma", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,M] <- -rowSums(gamma[,-M]) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$K, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]]\} \\
\hspace*{0.27 in} LL <- sum(dmvnp(Data$Y, mu, Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- apply(beta,1,sd) \\
\hspace*{0.27 in} s.gamma <- apply(gamma,1,sd) \\
\hspace*{0.27 in} s.epsilon <- apply(Data$Y - mu,2,sd) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + Omega.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Mixture Model, Finite} \label{fmm}
This finite mixture model (FMM) imposes a multilevel structure on each of the $J$ regression effects in $\beta$, so that mixture components share a common residual standard deviation, $\nu_j$. Identifiability is gained at the expense of some shrinkage.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_{1:N,m}, \sigma^2)$$
$$\mu_{1:N,m} = \textbf{X}\beta_{m,1:J}, \quad m=1,\dots,M$$
$$\beta_{m,j} \sim \mathcal{N}(0, \nu^2_j), \quad j=1,\dots,J$$
$$\nu_j \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\pi_{1:M} \sim \mathcal{D}(\alpha_{1:M})$$
$$\pi_m = \frac{\sum^N_{i=1} \delta_{i,m}}{\sum \delta}$$
$$\textbf{p}_{i,m} = \frac{\delta_{i,m}}{\sum^M_{m=1} \delta_{i,m}}$$
$$\delta_{i,m} = \exp(\textbf{X}\delta_{i,m}), \quad m=1,\dots,(M-1)$$
$$\delta_{1:N,M} = 1$$
$$\delta_{i,m} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad m=1,\dots,(M-1)$$
$$\alpha_m = 1$$
\subsection{Data}
\code{M <- 2 \#Number of mixtures \\
alpha <- rep(1,M) \#Prior probability of mixing probabilities \\
data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", as.parm.names(list(pi=rep(0,M), sigma=0))) \\
parm.names <- as.parm.names(list(beta=matrix(0,M,J), log.nu=rep(0,J), \\
\hspace*{0.27 in} log.delta=matrix(0,N,M-1), log.sigma=0)) \\
MyData <- list(J=J, M=M, N=N, X=X, alpha=alpha, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(M*J), rep(0,J), runif(N*(M-1),-1,1), 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$M, Data$J) \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$M) \\
\hspace*{0.27 in} pi <- colSums(delta) / sum(delta) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu", Data$parm.names)]) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, matrix(rep(nu, Data$M), Data$M, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$M), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} p <- max.col(p) \\
\hspace*{0.27 in} mu <- diag(mu[,p]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior + pi.prior + nu.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Mixture Model, Infinite} \label{imm}
This infinite mixture model (IMM) imposes a multilevel structure on each of the $J$ regression effects in $\beta$, so that mixture components share a common residual standard deviation, $\nu_j$. The infinite number of mixture components is truncated to a finite number, and the user specifies the maximum number to explore, $M$, where $M$ is discrete, greater than one, and less than the number of records, $N$. A truncated stick-breaking process within a truncated Dirichlet process defines the nonparametric mixture component selection.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_{1:N,m}, \sigma^2)$$
$$\mu_{1:N,m} = \textbf{X}\beta_{m,1:J}, \quad m=1,\dots,M$$
$$\beta_{m,j} \sim \mathcal{N}(0, \nu^2_j), \quad j=1,\dots,J$$
$$\nu_j \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta_i \sim \mathcal{CAT}(\pi_{i,1:C})$$
$$\pi \sim \mathrm{Stick}(\gamma)$$
$$\textbf{p}_{i,m} = \frac{\delta_{i,m}}{\sum^M_{m=1} \delta_{i,m}}$$
$$\delta_{i,m} = \exp(\textbf{X}\delta_{i,m}), \quad m=1,\dots,(M-1)$$
$$\delta_{1:N,M} = 1$$
$$\delta_{i,m} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad m=1,\dots,(M-1)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\iota \sim \mathcal{HC}(25)$$
$$\gamma \sim \mathcal{G}(\alpha, \iota)$$
\subsection{Data}
\code{M <- 3 \#Maximum number of mixtures to explore \\
data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", as.parm.names(list(pi=rep(0,M), sigma=0))) \\
parm.names <- as.parm.names(list(beta=matrix(0,M,J), log.nu=rep(0,J), \\
\hspace*{0.27 in} log.delta=matrix(0,N,M-1), log.sigma=0, lambda=rep(0,M-1), \\
\hspace*{0.27 in} log.alpha=0, log.iota=0, log.gamma=0)) \\
MyData <- list(J=J, M=M, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(M*J), rep(0,J), runif(N*(M-1),-1,1), 0, \\
\hspace*{0.27 in} rbeta(M-1,1,2), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} alpha <- exp(parm[grep("log.alpha", Data$parm.names)]) \\
\hspace*{0.27 in} iota <- exp(parm[grep("log.iota", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- exp(parm[grep("log.gamma", Data$parm.names)]) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu",Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$M, Data$J) \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$M) \\
\hspace*{0.27 in} lambda <- interval(parm[grep("lambda", Data$parm.names)], 1e-5, 1-1e-5) \\
\hspace*{0.27 in} pi <- as.vector(Stick(lambda)) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} theta <- max.col(p) \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} iota.prior <- dhalfcauchy(iota, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, alpha, iota, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, matrix(rep(nu, Data$M), Data$M, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$M), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- dStick(pi, gamma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dcat(theta, pi, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- diag(mu[,theta]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior + pi.prior + nu.prior + \\
\hspace*{0.62 in} sigma.prior + alpha.prior + iota.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Mixture Model, Poisson-Gamma} \label{poisson.gamma}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda \sim \mathcal{G}(\alpha \mu, \alpha)$$
$$\mu = \exp(\textbf{X}\beta)$$
$$\alpha \sim \mathcal{HC}(25)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 20 \\
J <- 3 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- round(exp(tcrossprod(X, t(beta)))) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(log.alpha=0, beta=rep(0,J), \\
\hspace*{0.27 in} log.lambda=rep(0,N))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,J), rep(0,N))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha <- exp(parm[grep("log.alpha", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} lambda <- exp(parm[grep("log.lambda", Data$parm.names)]) \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} alpha.prior <- dhalfcauchy(alpha, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dgamma(lambda, alpha*mu, alpha, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + lambda.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Multinomial Logit} \label{mnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}, \quad \sum^J_{j=1} \textbf{p}_{i,j} = 1$$
$$\phi = \exp(\mu)$$
$$\mu_{i,J} = 0, \quad i=1,\dots,N$$
$$\mu_{i,j} = \textbf{X}_{i,1:K} \beta_{j,1:K} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,J-1,K))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm, Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} mu[,-Data$K] <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(p) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Logit, Nested} \label{nmnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{P}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{P}_{1:N,1} = \frac{\textbf{R}}{\textbf{R} + \exp(\alpha \textbf{I})}$$
$$\textbf{P}_{1:N,2} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,1}}{\textbf{V}}$$
$$\textbf{P}_{1:N,3} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,2}}{\textbf{V}}$$
$$\textbf{R}_{1:N} = \exp(\mu_{1:N,1})$$
$$\textbf{S}_{1:N,1:2} = \exp(\mu_{1:N,2:3})$$
$$\textbf{I} = \log(\textbf{V})$$
$$\textbf{V}_i = \displaystyle\sum^K_{k=1} \textbf{S}_{i,k}, \quad i=1,\dots,N$$
$$\mu_{1:N,1} = \textbf{X} \iota \in [-700,700]$$
$$\mu_{1:N,2} = \textbf{X} \beta_{2,1:K} \in [-700,700]$$
$$\iota = \alpha \beta_{1,1:K}$$
$$\alpha \sim \mathcal{EXP}(1) \in [0,2]$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1) \quad k=1,\dots,K$$
where there are $J=3$ categories of $\textbf{y}$, $K=3$ predictors, $\textbf{R}$ is the non-nested alternative, $\textbf{S}$ is the nested alternative, $\textbf{V}$ is the observed utility in the nest, $\alpha$ is effectively 1 - correlation and has a truncated exponential distribution, and $\iota$ is a vector of regression effects for the isolated alternative after $\alpha$ is taken into account. The third alternative is the reference category.
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- c("LP", as.parm.names(list(iota=rep(0,K)))) \\
parm.names <- as.parm.names(list(alpha=0, beta=matrix(0,J-1,K))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, rep(0.1,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.rate <- 1 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1],0,2); parm[1] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dtrunc(alpha, "exp", a=0, b=2, rate=alpha.rate, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- P <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} iota <- alpha * beta[1,] \\
\hspace*{0.27 in} mu[,1] <- tcrossprod(Data$X, t(iota)) \\
\hspace*{0.27 in} mu[,2] <- tcrossprod(Data$X, t(beta[2,])) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} R <- exp(mu[,1]) \\
\hspace*{0.27 in} S <- exp(mu[,-1]) \\
\hspace*{0.27 in} V <- rowSums(S) \\
\hspace*{0.27 in} I <- log(V) \\
\hspace*{0.27 in} P[,1] <- R / (R + exp(alpha*I)) \\
\hspace*{0.27 in} P[,2] <- (1 - P[,1]) * S[,1] / V \\
\hspace*{0.27 in} P[,3] <- (1 - P[,1]) * S[,2] / V \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(P) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,iota), yhat=yrep, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Probit} \label{mnp}
In this form of MNP, the $\beta$ parameters are sum-to-zero constraints in the reference category, and covariance matrix $\Sigma$ includes all $J$ categories of $\textbf{y}$.
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{Z}_{i,j} \in \left\{
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K}$$
$$\Sigma \sim \mathcal{IW}_{J+1}(\textbf{S}^{-1}), \quad \textbf{S} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{y <- x1 <- x2 <- c(1:30) \\
y[1:10] <- 1 \\
y[11:20] <- 2 \\
y[21:30] <- 3 \\
x1[1:10] <- rnorm(10, 25, 2.5) \\
x1[11:20] <- rnorm(10, 40, 4.0) \\
x1[21:30] <- rnorm(10, 35, 3.5) \\
x2[1:10] <- rnorm(10, 2.51, 0.25) \\
x2[11:20] <- rnorm(10, 2.01, 0.20) \\
x2[21:30] <- rnorm(10, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of columns to be in design matrix X \\
S <- diag(J) \\
X <- matrix(c(rep(1,N),x1,x2),N,K) \\
mon.names <- "LP" \\
sigma.temp <- as.parm.names(list(Sigma=diag(J)), uppertri=1) \\
parm.names <- c(sigma.temp[2:length(sigma.temp)], \\
\hspace*{0.27 in} as.parm.names(list(beta=matrix(0,(J-1),K), Z=matrix(0,N,J)))) \\
MyData <- list(J=J, K=K, N=N, S=S, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, length(upper.triangle(S, diag=TRUE)) - 1), \\
\hspace*{0.27 in} rep(0,(J-1)*K), rep(0,N*J)) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} beta <- rbind(beta, colSums(beta)*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} Sigma <- as.parm.matrix(Sigma, Data$J, parm, Data, restrict=TRUE) \\
\hspace*{0.27 in} parm[grep("Sigma", Data$parm.names)] <- upper.triangle(Sigma, \\
\hspace*{0.62 in} diag=TRUE)[-1] \\
\hspace*{0.27 in} Z <- matrix(parm[grep("Z", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Sigma.prior <- dinvwishart(Sigma, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} Z.prior <- sum(dnormv(Z, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} Y <- as.indicator.matrix(Data$y) \\
\hspace*{0.27 in} Z <- ifelse(Z > 10, 10, Z); Z <- ifelse(\{Y == 0\} \& \{Z > 0\}, 0, Z) \\
\hspace*{0.27 in} Z <- ifelse(Z < -10, -10, Z); Z <- ifelse(\{Y == 1\} \& \{Z < 0\}, 0, Z) \\
\hspace*{0.27 in} parm[grep("Z", Data$parm.names)] <- as.vector(Z) \\
\hspace*{0.27 in} LL <- sum(dmvn(Z, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(Z) \\
\hspace*{0.27 in} \#eta <- exp(mu) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Sigma.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Multivariate Binary Probit} \label{multiv.bin.probit}
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{Z}_{i,j} \in \left\{ 
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\ 
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K}$$
$$\Sigma \sim \mathcal{IW}_{J+1}(\textbf{S}^{-1}), \quad \textbf{S} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{N <- 30 \\
J <- 3 \#Number of binary dependent variables \\
K <- 3 \#Number of columns to be in design matrix X \\
Y <- matrix(round(runif(N*J)),N,J) \\
X <- matrix(1,N, K) \\
for (k in 2:K) \{X[,k] <- rnorm(N, runif(1,-3,3), runif(1,0.1,3))\} \\
S <- diag(J) \\
mon.names <- "LP" \\
sigma.temp <- as.parm.names(list(Sigma=diag(J)), uppertri=1) \\
parm.names <- c(sigma.temp[2:length(sigma.temp)], \\
\hspace*{0.27 in} as.parm.names(list(beta=matrix(0,(J-1),K), Z=matrix(0,N,J)))) \\
MyData <- list(J=J, K=K, N=N, S=S, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, length(upper.triangle(S, diag=TRUE)) - 1), \\
\hspace*{0.27 in} rep(0,(J-1)*K), rep(0,N*J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} beta <- rbind(beta, colSums(beta)*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} Sigma <- as.parm.matrix(Sigma, Data$J, parm, Data, restrict=TRUE) \\
\hspace*{0.27 in} parm[grep("Sigma", Data$parm.names)] <- upper.triangle(Sigma, \\
\hspace*{0.62 in} diag=TRUE)[-1] \\
\hspace*{0.27 in} Z <- matrix(parm[grep("Z", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Sigma.prior <- dinvwishart(Sigma, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} Z.prior <- sum(dnormv(Z, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} Z <- ifelse(Z > 10, 10, Z) \\
\hspace*{0.27 in} Z <- ifelse(\{Data$Y == 0\} \& \{Z > 0\}, 0, Z) \\
\hspace*{0.27 in} Z <- ifelse(Z < -10, -10, Z) \\
\hspace*{0.27 in} Z <- ifelse(\{Data$Y == 1\} \& \{Z < 0\}, 0, Z) \\
\hspace*{0.27 in} parm[grep("Z", Data$parm.names)] <- as.vector(Z) \\
\hspace*{0.27 in} LL <- sum(dmvn(Z, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(Z >= 0, 1, 0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Sigma.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Multivariate Laplace Regression} \label{multivariate.lap.reg}
\subsection{Form}
$$\textbf{Y}_{i,k} \sim \mathcal{L}_K(\mu_{i,k}, \Sigma), \quad i=1,\dots,N; \quad k=1,\dots,K$$
$$\mu_{i,k} = \textbf{X}_{1:N,k} \beta_{k,1:J}$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\beta_{k,j} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 100 \\
J <- 6 \#Number of columns in design matrix \\
K <- 3 \#Number of DVs \\
X <- matrix(runif(N*J),N,J); X[,1] <- 1 \\
Y <- mu <- tcrossprod(X, matrix(rnorm(J*K),K,J)) \\
Sigma <- matrix(runif(K*K),K,K); diag(Sigma) <- runif(K,1,K) \\
Sigma <- as.symmetric.matrix(Sigma) \\
for (i in 1:N) \{Y[i,] <- colMeans(rmvn(1000, mu[i,], Sigma))\} \\
S <- diag(K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,K,J), Omega=diag(K)), \\
\hspace*{0.27 in} uppertri=c(0,1)) \\
MyData <- list(J=J, K=K, N=N, S=S, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J*K), upper.triangle(S, diag=TRUE))}
\subsection{Data}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$K, Data$J) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$K, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} Sigma <- as.inverse(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dmvl(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Multivariate Regression} \label{multivariate.reg}
\subsection{Form}
$$\textbf{Y}_{i,k} \sim \mathcal{N}_K(\mu_{i,k}, \Omega^{-1}), \quad i=1,\dots,N; \quad k=1,\dots,K$$
$$\mu_{i,k} = \textbf{X}_{1:N,k} \beta_{k,1:J}$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\beta_{k,j} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 100 \\
J <- 6 \#Number of columns in design matrix \\
K <- 3 \#Number of DVs \\
X <- matrix(runif(N*J),N,J); X[,1] <- 1 \\
Y <- mu <- tcrossprod(X, matrix(rnorm(J*K),K,J)) \\
Sigma <- matrix(runif(K*K),K,K); diag(Sigma) <- runif(K,1,K) \\
Sigma <- as.symmetric.matrix(Sigma) \\
for (i in 1:N) \{Y[i,] <- colMeans(rmvn(1000, mu[i,], Sigma))\} \\
S <- diag(K) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=matrix(0,K,J), Omega=diag(K)), \\
\hspace*{0.27 in} uppertri=c(0,1)) \\
MyData <- list(J=J, K=K, N=N, S=S, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J*K), upper.triangle(S, diag=TRUE))}
\subsection{Data}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$K, Data$J) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$K, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$K+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, beta) \\
\hspace*{0.27 in} LL <- sum(dmvnp(Data$Y, mu, Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Negative Binomial Regression} \label{negbin.reg}
This example was contributed by Jim Robison-Cox.
\subsection{Form}
$$\textbf{y} \sim \mathcal{NB}(\mu, \kappa)$$
$$p = \frac{\kappa}{\kappa + \mu}$$
$$\mu = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\kappa \sim \mathcal{HC}(25) \in (0,\infty]$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of predictors, including the intercept \\
kappa.orig <- 2 \\
beta.orig <- runif(J,-2,2) \\
X <- matrix(runif(J*N,-2, 2), N, J); X[,1] <- 1 \\
mu <- exp(tcrossprod(X, t(beta.orig)) + rnorm(N)) \\
p <- kappa.orig / (kappa.orig + mu) \\
y <- rnbinom(N, size=kappa.orig, mu=mu) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J), kappa=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} parm[Data$J + 1] <- kappa <- interval(parm[Data$J + 1], \\
\hspace*{0.62 in} .Machine$double.xmin, Inf) \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} \#p <- kappa / (kappa + mu) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} kappa.prior <- dhalfcauchy(kappa, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnbinom(Data$y, mu=mu, size=kappa, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + kappa.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Normal, Multilevel} \label{norm.ml}
This is Gelman's school example \citep{gelman04}. Note that \pkg{LaplacesDemon} is slower to converge than \proglang{WinBUGS} through the \pkg{R2WinBUGS} package \citep{r:r2winbugs}, an \proglang{R} package on CRAN. This example is very sensitive to the prior distributions. The recommended, default, half-Cauchy priors with scale 25 on scale parameters overwhelms the likelihood, so uniform priors are used.
\subsection{Form}
$$\textbf{y}_j \sim \mathcal{N}(\theta_j, \sigma^2_j), \quad j=1,\dots,J$$
$$\theta_j \sim \mathcal{N}(\theta_{\mu}, \theta_\sigma^2)$$
$$\theta_{\mu} \sim \mathcal{N}(0, 1000000)$$
$$\theta_{\sigma[j]} \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{U}(0, 1000)$$
\subsection{Data}
\code{J <- 8 \\
y <- c(28.4, 7.9, -2.8, 6.8, -0.6, 0.6, 18.0, 12.2) \\
sd <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(theta=rep(0,J), theta.mu=0, \\
\hspace*{0.27 in} theta.sigma=0)) \\
MyData <- list(J=J, mon.names=mon.names, parm.names=parm.names, sd=sd, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(mean(y),J), mean(y), 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} theta.mu <- parm[Data$J+1] \\
\hspace*{0.27 in} theta.sigma <- interval(parm[Data$J+2], .Machine\$double.eps, Inf) \\
\hspace*{0.27 in} parm[Data$J+2] <- theta.sigma \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} theta.mu.prior <- dnormp(theta.mu, 0, 1.0E-6, log=TRUE) \\
\hspace*{0.27 in} theta.sigma.prior <- dunif(theta.sigma, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, theta.mu, theta.sigma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dunif(Data$sd, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, theta, Data$sd, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.prior + theta.mu.prior + theta.sigma.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=theta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Ordinal Logit} \label{ordinal.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(P_{i,1:J})$$
$$P_{,J} = 1 - Q_{,(J-1)}$$
$$P_{,j} = Q_{,j} - Q_{,(j-1)}, \quad j=2,\dots,(J-1)$$
$$P_{,1} = Q_{,1}$$
$$Q = \frac{1}{1 + \exp(\mu)}$$
$$\mu_{,j} = \delta_j + \textbf{X} \beta, \quad \in [-5,5]$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\delta_j \sim \mathcal{N}(0, 1000) \in [(j-1),j], \quad j=1,\dots,(J-1)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- 3 \#Number of categories in y \\
K <- ncol(demonsnacks) \#Number of columns in design matrix X \\
y <- log(demonsnacks$Calories) \\
y <- ifelse(y < 4.5669, 1, ifelse(y > 5.5268, 3, 2)) \#Discretize \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (k in 2:K) \{X[,k] <- CenterScale(X[,k])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,K), delta=rep(0,J-1))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
     parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), seq(from=-1, to=1, len=(J-1)))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$K] \\
\hspace*{0.27 in} delta <- parm[-(1:Data$K)] \\
\hspace*{0.27 in} delta[-1] <- ifelse(delta[-1] < delta[-length(delta)], \\
\hspace*{0.62 in} delta[-length(delta)], delta[-1]) \\
\hspace*{0.27 in} parm[-(1:Data$K)] <- delta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(delta, Data$N, Data$J-1, byrow=TRUE) + \\
\hspace*{0.62 in} matrix(tcrossprod(Data$X, t(beta)), Data$N, Data$J-1) \\
\hspace*{0.27 in} mu <- interval(mu, -5, 5) \\
\hspace*{0.27 in} P <- Q <- invlogit(mu) \\
\hspace*{0.27 in} P[,-1] <- Q[,-1] - Q[,-(Data$J-1)] \\
\hspace*{0.27 in} P <- cbind(P, 1 - Q[,(Data$J-1)]) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(P) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Ordinal Probit} \label{ordinal.probit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(P_{i,1:J})$$
$$P_{,J} = 1 - Q_{,(J-1)}$$
$$P_{,j} = Q_{,j} - Q_{,(j-1)}, \quad j=2,\dots,(J-1)$$
$$P_{,1} = Q_{,1}$$
$$Q = \phi(\mu)$$
$$\mu_{,j} = \delta_j + \textbf{X} \beta, \quad \in [-5,5]$$
$$\beta_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\delta_j \sim \mathcal{N}(0, 1000) \in [(j-1),j], \quad j=1,\dots,(J-1)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- 3 \#Number of categories in y \\
K <- ncol(demonsnacks) \#Number of columns in design matrix X \\
y <- log(demonsnacks$Calories) \\
y <- ifelse(y < 4.5669, 1, ifelse(y > 5.5268, 3, 2)) \#Discretize \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (k in 2:K) \{X[,k] <- CenterScale(X[,k])\} \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,K), delta=rep(0,J-1))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
     parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), seq(from=-1, to=1, len=(J-1)))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$K] \\
\hspace*{0.27 in} delta <- parm[-(1:Data$K)] \\
\hspace*{0.27 in} delta[-1] <- ifelse(delta[-1] < delta[-length(delta)], \\
\hspace*{0.62 in} delta[-length(delta)], delta[-1]) \\
\hspace*{0.27 in} parm[-(1:Data$K)] <- delta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(delta, Data$N, Data$J-1, byrow=TRUE) + \\
\hspace*{0.62 in} matrix(tcrossprod(Data$X, t(beta)), Data$N, Data$J-1) \\
\hspace*{0.27 in} mu <- interval(mu, -5, 5) \\
\hspace*{0.27 in} P <- Q <- pnorm(mu) \\
\hspace*{0.27 in} P[,-1] <- Q[,-1] - Q[,-(Data$J-1)] \\
\hspace*{0.27 in} P <- cbind(P, 1 - Q[,(Data$J-1)]) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} yrep <- max.col(P) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Panel, Autoregressive Poisson} \label{panel.ap}
\subsection{Form}
$$\textbf{Y} \sim \mathcal{P}(\Lambda)$$
$$\Lambda_{1:N,1} = \exp(\alpha + \beta \textbf{x})$$
$$\Lambda_{1:N,t} = \exp(\alpha + \beta \textbf{x} + \rho \log(\textbf{Y}_{1:N,t-1})), \quad t=2,\dots,T$$
$$\alpha_i \sim \mathcal{N}(\alpha_\mu, \alpha^2_\sigma), \quad i=1,\dots,N$$
$$\alpha_\mu \sim \mathcal{N}(0, 1000)$$
$$\alpha_\sigma \sim \mathcal{HC}(25)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\rho \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{N <- 10 \\
T <- 10 \\
alpha <- rnorm(N,2,0.5) \\
rho <- 0.5 \\
beta <- 0.5 \\
x <- runif(N,0,1) \\
Y <- matrix(NA,N,T) \\
Y[,1] <- exp(alpha + beta*x) \\
for (t in 2:T) \{Y[,t] <- exp(alpha + beta*x + rho*log(Y[,t-1]))\} \\
Y <- round(Y) \\
mon.names <- c("LP","alpha.sigma") \\
parm.names <- as.parm.names(list(alpha=rep(0,N), alpha.mu=0, \\
\hspace*{0.27 in} log.alpha.sigma=0, beta=0, rho=0)) \\
MyData <- list(N=N, T=T, Y=Y, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=x) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), 0, log(1), 0, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- parm[Data$N+1] \\
\hspace*{0.27 in} alpha.sigma <- exp(parm[Data$N+2]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$N] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} rho <- parm[grep("rho", Data$parm.names)] \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} alpha.mu.prior <- dnormv(alpha.mu, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} alpha.sigma.prior <- dhalfcauchy(alpha.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- dnormv(beta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} rho.prior <- dnormv(rho, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- Data$Y \\
\hspace*{0.27 in} Lambda[,1] <- exp(alpha + beta*x) \\
\hspace*{0.27 in} Lambda[,2:Data$T] <- exp(alpha + beta*Data$x + \\
\hspace*{0.62 in} rho*log(Data$Y[,1:(Data$T-1)])) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$Y, Lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + alpha.mu.prior + alpha.sigma.prior + \\
\hspace*{0.62 in} beta.prior + rho.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,alpha.sigma), \\
\hspace*{0.62 in} yhat=Lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Penalized Spline Regression} \label{penalized.spline}
This example is adapted from \citet{crainiceanu05}. The user specifies the degree $D$ of polynomials and the number $K$ of knots. Regression effects $\beta$ regard the polynomial in design matrix $\textbf{X}$, and $\gamma$ regard the splines in design matrix $\textbf{S}$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu = \textbf{X} \beta + \textbf{S} \gamma$$
\[\textbf{S}_{i,k} = \left\{ 
\begin{array}{l l}
  (\textbf{x}_i - k)^D & \quad \mbox{if $\textbf{S}_{i,k} > 0$}\\
  0 \\ \end{array} \right. \]
$$\textbf{X}_{i,d} = \textbf{x}^{d-1}_i, \quad d=2,\dots,(D+1)$$
$$\textbf{X}_{i,1} = 1$$
$$\beta_d \sim \mathcal{N}(0, 1000), \quad d=1,\dots,(D+1)$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_2), \quad k=1,\dots,K$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{N <- 100 \\
x <- 1:N \\
y <- sin(2*pi*x/N) + runif(N,-1,1) \\
K <- 10 \#Number of knots \\
D <- 2 \#Degree of polynomial \\
x <- CenterScale(x) \\
k <- as.vector(quantile(x, probs=(1:K / (K+1)))) \\
X <- cbind(rep(1,N), matrix(x, N, D)\textasciicircum matrix(1:D, N, D, byrow=TRUE)) \\
S <- matrix(x, N, K) - matrix(k, N, K, byrow=TRUE) \\
S <- ifelse(S > 0, S, 0); S <- S\textasciicircum D \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,D+1), gamma=rep(0,K), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
MyData <- list(N=N, S=S, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,D+1), rep(0,K), c(1,1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) + tcrossprod(Data$S, t(gamma)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Poisson Regression} \label{poisson.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\textbf{X}\beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- round(exp(tcrossprod(X, t(beta)))) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Polynomial Regression} \label{polynomial.reg}
In this univariate example, the degree of the polynomial is specified as $D$. For a more robust extension to estimating nonlinear relationships between $\textbf{y}$ and $\textbf{x}$, see penalized spline regression in section \ref{penalized.spline}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X} \beta$$
$$\textbf{X}_{i,d} = \textbf{x}^{d-1}_i, \quad d=1,\dots,(D+1)$$
$$\textbf{X}_{i,1} = 1$$
$$\beta_d \sim \mathcal{N}(0, 1000), \quad d=1,\dots,(D+1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
D <- 2 \#Degree of polynomial \\
y <- log(demonsnacks$Calories) \\
x <- demonsnacks[,7] \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,D+1), log.sigma=0)) \\
MyData <- list(D=D, N=N, mon.names=mon.names, parm.names=parm.names, x=x, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,D+1), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} X <- matrix(Data$x, Data$N, Data$D) \\
\hspace*{0.27 in} for (d in 2:Data$D) \{X[,d] <- X[,d]\textasciicircum d\} \\
\hspace*{0.27 in} X <- cbind(1,X) \\
\hspace*{0.27 in} mu <- tcrossprod(X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Proportional Hazards Regression, Weibull} \label{prop.haz.weib}
Although the dependent variable is usually denoted as $\textbf{t}$ in survival analysis, it is denoted here as $\textbf{y}$ so Laplace's Demon recognizes it as a dependent variable for posterior predictive checks. This example does not support censoring, but it will be included soon.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{WEIB}(\gamma, \mu_i), \quad i=1,\dots,N$$
$$\mu = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\gamma \sim \mathcal{G}(1, 0.001)$$
\subsection{Data}
\code{N <- 50 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- c(1,runif(J-1,-1,1)) \\
y <- round(exp(tcrossprod(X, t(beta)))) + 1 \# Undefined at zero \\
mon.names <- c("LP","gamma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.gamma=0)) \\
MyData <- list(J=J, N=N, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} gamma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, 1, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- exp(tcrossprod(Data$X, t(beta))) \\
\hspace*{0.27 in} LL <- sum(dweibull(Data$y, gamma, mu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, gamma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Revision, Normal} \label{revision.normal}
This example provides both an analytic solution and numerical approximation of the revision of a normal distribution. Given a normal prior distribution ($\alpha$) and data distribution ($\beta$), the posterior ($\gamma$) is the revised normal distribution. This is an introductory example of Bayesian inference, and allows the user to experiment numerical approximation, such as with MCMC in \code{LaplacesDemon}. Note that, regardless of the data sample size $N$ in this example, Laplace Approximation is inappropriate due to asymptotics since the data ($\beta$) is perceived by the algorithm as a single datum rather than a collection of data. MCMC, on the other hand, is biased only by the effective number of samples taken of the posterior. \\
\code{\#\#\# Analytic Solution \\
prior.mu <- 0 \\
prior.sigma <- 10 \\
N <- 10 \\
data.mu <- 1 \\
data.sigma <- 2 \\
posterior.mu <- (prior.sigma\textasciicircum -2 * prior.mu + N * data.sigma\textasciicircum -2 * data.mu) / \\
\hspace*{0.27 in} (prior.sigma\textasciicircum -2 + N * data.sigma\textasciicircum -2) \\
posterior.sigma <- sqrt(1/(prior.sigma\textasciicircum -2 + data.sigma\textasciicircum -2)) \\
posterior.mu \\
posterior.sigma \\
}
\subsection{Form}
$$\alpha \sim \mathcal{N}(0,10)$$
$$\beta \sim \mathcal{N}(1,2)$$
$$\gamma = \frac{\alpha^{-2}_\sigma \alpha + N \beta^{-2}_\sigma \beta}{\alpha^{-2}_\sigma + N \beta^{-2}_\sigma}$$
\subsection{Data}
\code{N <- 10 \\
mon.names <- c("LP","gamma") \\
parm.names <- c("alpha","beta") \\
MyData <- list(N=N, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0,0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- 0 \\
\hspace*{0.27 in} alpha.sigma <- 10 \\
\hspace*{0.27 in} beta.mu <- 1 \\
\hspace*{0.27 in} beta.sigma <- 2 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood Density \\
\hspace*{0.27 in} LL <- dnorm(beta, beta.mu, beta.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Posterior \\
\hspace*{0.27 in} gamma <- (alpha.sigma\textasciicircum -2 * alpha + N * beta.sigma\textasciicircum -2 * beta) / \\
\hspace*{0.62 in} (alpha.sigma\textasciicircum -2 + N * beta.sigma\textasciicircum -2) \\
\hspace*{0.27 in} \#\#\# Log(Posterior Density) \\
\hspace*{0.27 in} LP <- LL + alpha.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,gamma), yhat=LL, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Robust Regression} \label{robust.reg}
By replacing the normal distribution with the Student t distribution, linear regression is often called robust regression. As an alternative approach to robust regression, consider Laplace regression (see section \ref{laplace.reg}).
\subsection{Form}
$$\textbf{y} \sim \mathrm{t}(\mu, \sigma^2, \nu)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\nu \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- tcrossprod(X, t(beta)) + e \\
mon.names <- c("LP", "sigma", "nu") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0, log.nu=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1), log(2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} nu <- exp(parm[Data$J+2]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- dhalfcauchy(nu, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dst(Data$y, mu, sigma, nu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior + nu.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,nu), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Seemingly Unrelated Regression (SUR)} \label{sur}
The following data was used by \citet{zellner62} when introducing the Seemingly Unrelated Regression methodology. This model uses the conjugate Wishart distribution for precision in a multivariate normal distribution. See section \ref{cov.sep.strat} for a non-Wishart alternative that is more flexible and converges much faster.
\subsection{Form}
$$\textbf{Y}_{t,k} \sim \mathcal{N}_K(\mu_{t,k}, \Omega^{-1}), \quad t=1,\dots,T; \quad k=1,\dots,K$$
$$\mu_{1,t} = \alpha_1 + \alpha_2 \textbf{X}_{t-1,1} + \alpha_3 \textbf{X}_{t-1,2}, \quad t=2,\dots,T$$
$$\mu_{2,t} = \beta_1 + \beta_2 \textbf{X}_{t-1,3} + \beta_3 \textbf{X}_{t-1,4}, \quad t=2,\dots,T$$
$$\Omega \sim \mathcal{W}_{K+1}(\textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where J=3, K=2, and T=20.
\subsection{Data}
\code{T <- 20 \#Time-periods \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
J <- 2 \#Number of dependent variables \\
Y <- matrix(c(IG,IW), T, J) \\
S <- diag(J) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} Omega=diag(2)), uppertri=c(0,0,1)) \\
MyData <- list(J=J, S=S, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, \\
\hspace*{0.27 in} VW=VW, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), upper.triangle(S, diag=TRUE))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:3] \\
\hspace*{0.27 in} beta <- parm[4:6] \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, Data$J, parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$J+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} mu[-1,1] <- alpha[1] + alpha[2]*Data$CG[-Data$T] + \\
\hspace*{0.62 in} alpha[3]*Data$VG[-Data$T] \\
\hspace*{0.27 in} mu[-1,2] <- beta[1] + beta[2]*Data$CW[-Data$T] + \\
\hspace*{0.62 in} beta[3]*Data$VW[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dmvnp(Data$Y[-1,], mu[-1,], Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Simultaneous Equations} \label{simultaneous}
This example of simultaneous equations uses Klein's Model I \citep{kleine50} regarding economic fluctations in the United States in 1920-1941 (\textbf{N}=22). Usually, this example is modeled with 3-stage least sqaures (3SLS), excluding the uncertainty from multiple stages. By constraining each element in the instrumental variables matrix $\nu \in [-10,10]$, this example estimates the model without resorting to stages. The dependent variable is matrix \textbf{Y}, in which $\textbf{Y}_{1,1:N}$ is \textbf{C} or Consumption, $\textbf{Y}_{2,1:N}$ is \textbf{I} or Investment, and $\textbf{Y}_{3,1:N}$ is \textbf{Wp} or Private Wages. Here is a data dictionary: \\
\code{\hspace*{0.27 in} A = Time Trend measured as years from 1931 \\
\hspace*{0.27 in} \textbf{C} = Consumption \\
\hspace*{0.27 in} \textbf{G} = Government Nonwage Spending \\
\hspace*{0.27 in} \textbf{I} = Investment \\
\hspace*{0.27 in} \textbf{K} = Capital Stock \\
\hspace*{0.27 in} \textbf{P} = Private (Corporate) Profits \\
\hspace*{0.27 in} \textbf{T} = Indirect Business Taxes Plus Neg Exports \\
\hspace*{0.27 in} \textbf{Wg} = Government Wage Bill \\
\hspace*{0.27 in} \textbf{Wp} = Private Wages \\
\hspace*{0.27 in} \textbf{X} = Equilibrium Demand (GNP) \\
}
See \citet{kleine50} for more information.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{N}_3(\mu, \Omega^{-1})$$
$$ \mu_{1,1} = \alpha_1 + \alpha_2 \nu_{1,1} + \alpha_4 \nu_{2,1}$$
$$ \mu_{1,i} = \alpha_1 + \alpha_2 \nu_{1,i} + \alpha_3 \textbf{P}_{i-1} + \alpha_4 \nu_{2,i}, \quad i=2,\dots,N$$
$$ \mu_{2,1} = \beta_1 + \beta_2 \nu_{1,1} + \beta_4 \textbf{K}_1$$
$$ \mu_{2,i} = \beta_1 + \beta_2 \nu_{1,i} + \beta_3 \textbf{P}_{i-1} + \beta_4 \textbf{K}_i, \quad i=2,\dots,N$$
$$\mu_{3,1} = \gamma_1 + \gamma_2 \nu_{3,1} + \gamma_4 \textbf{A}_1$$
$$\mu_{3,i} = \gamma_1 + \gamma_2 \nu_{3,i} + \gamma_3 \textbf{X}_{i-1} + \gamma_4 \textbf{A}_i, \quad i=2,\dots,N$$
$$\textbf{Z}_{j,i} \sim \mathcal{N}(\nu_{j,i}, \sigma^2_j), \quad j=1,\dots,3$$
$$\nu_{j,1} = \pi_{j,1} + \pi_{j,3} \textbf{K}_1 + \pi_{j,5} \textbf{A}_1 + \pi_{j,6} \textbf{T}_1 + \pi_{j,7} \textbf{G}_1, \quad j=1,\dots,3$$
$$\nu_{j,i} = \pi_{j,1} + \pi_{j,2} \textbf{P}_{i-1} + \pi_{j,3} \textbf{K}_i + \pi_{j,4} \textbf{X}_{i-1} + \pi_{j,5} \textbf{A}_i + \pi_{j,6} \textbf{T}_i + \pi \textbf{G}_i, \quad i=1,\dots,N, \quad j=1,\dots,3$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\pi_{j,i} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad j=1,\dots,3, \quad i=1,\dots,N$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,3$$
$$\Omega \sim \mathcal{W}_4(\textbf{S}), \quad \textbf{S} = \textbf{I}_3$$
\subsection{Data}
\code{N <- 22 \\
A <- c(-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10) \\
C <- c(39.8,41.9,45,49.2,50.6,52.6,55.1,56.2,57.3,57.8,55,50.9,45.6,46.5, \\
\hspace*{0.27 in} 48.7,51.3,57.7,58.7,57.5,61.6,65,69.7) \\
G <- c(2.4,3.9,3.2,2.8,3.5,3.3,3.3,4,4.2,4.1,5.2,5.9,4.9,3.7,4,4.4,2.9,4.3, \\
\hspace*{0.27 in} 5.3,6.6,7.4,13.8) \\
I <- c(2.7,-0.2,1.9,5.2,3,5.1,5.6,4.2,3,5.1,1,-3.4,-6.2,-5.1,-3,-1.3,2.1,2, \\
\hspace*{0.27 in} -1.9,1.3,3.3,4.9) \\
K <- c(180.1,182.8,182.6,184.5,189.7,192.7,197.8,203.4,207.6,210.6,215.7, \\
\hspace*{0.27 in} 216.7,213.3,207.1,202,199,197.7,199.8,201.8,199.9,201.2,204.5) \\
P <- c(12.7,12.4,16.9,18.4,19.4,20.1,19.6,19.8,21.1,21.7,15.6,11.4,7,11.2, \\
\hspace*{0.27 in} 12.3,14,17.6,17.3,15.3,19,21.1,23.5) \\
T <- c(3.4,7.7,3.9,4.7,3.8,5.5,7,6.7,4.2,4,7.7,7.5,8.3,5.4,6.8,7.2,8.3,6.7, \\
\hspace*{0.27 in} 7.4,8.9,9.6,11.6) \\
Wg <- c(2.2,2.7,2.9,2.9,3.1,3.2,3.3,3.6,3.7,4,4.2,4.8,5.3,5.6,6,6.1,7.4, \\
\hspace*{0.27 in} 6.7,7.7,7.8,8,8.5) \\
Wp <- c(28.8,25.5,29.3,34.1,33.9,35.4,37.4,37.9,39.2,41.3,37.9,34.5,29,28.5, \\
\hspace*{0.27 in} 30.6,33.2,36.8,41,38.2,41.6,45,53.3) \\
X <- c(44.9,45.6,50.1,57.2,57.1,61,64,64.4,64.5,67,61.2,53.4,44.3,45.1, \\
\hspace*{0.27 in} 49.7,54.4,62.7,65,60.9,69.5,75.7,88.4) \\
year <- c(1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932, \\
\hspace*{0.27 in} 1933,1934,1935,1936,1937,1938,1939,1940,1941) \\
Y <- matrix(c(C,I,Wp),3,N, byrow=TRUE) \\
Z <- matrix(c(P, Wp+Wg, X), 3, N, byrow=TRUE) \\
S <- diag(nrow(Y)) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,4), beta=rep(0,4), \\
\hspace*{0.27 in} gamma=rep(0,4), pi=matrix(0,3,7), log.sigma=rep(0,3), \\
\hspace*{0.27 in} Omega=diag(3)), uppertri=c(0,0,0,0,0,1)) \\
MyData <- list(A=A, C=C, G=G, I=I, K=K, N=N, P=P, S=S, T=T, Wg=Wg, Wp=Wp, \\
\hspace*{0.27 in} X=X, Y=Y, Z=Z, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), rep(0,4), rep(0,4), rep(0,3*7), rep(0,3), \\
\hspace*{0.27 in} upper.triangle(S, diag=TRUE))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:4]; beta <- parm[5:8]; gamma <- parm[9:12] \\
\hspace*{0.27 in} pi <- matrix(interval(parm[grep("pi", Data$parm.names)],-10,10), 3, 7) \\
\hspace*{0.27 in} parm[grep("pi", Data$parm.names)] <- as.vector(pi) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} Omega <- as.parm.matrix(Omega, nrow(Data$S), parm, Data) \\
\hspace*{0.27 in} parm[grep("Omega", Data$parm.names)] <- upper.triangle(Omega, \\
\hspace*{0.62 in} diag=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnormv(gamma, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- sum(dnormv(pi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, nrow(Data$S)+1, Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- nu <- matrix(0,3,Data$N) \\
\hspace*{0.27 in} for (i in 1:3) \{ \\
\hspace*{0.62 in} nu[i,1] <- pi[i,1] + pi[i,3]*Data$K[1] + pi[i,5]*Data$A[1] + \\
\hspace*{0.95 in} pi[i,6]*Data$T[1] + pi[i,7]*Data$G[1] \\
\hspace*{0.62 in} nu[i,-1] <- pi[i,1] + pi[i,2]*Data$P[-Data$N] + \\
\hspace*{0.95 in} pi[i,3]*Data$K[-1] + pi[i,4]*Data$X[-Data$N] + \\
\hspace*{0.95 in} pi[i,5]*Data$A[-1] + pi[i,6]*Data$T[-1] + \\
\hspace*{0.95 in} pi[i,7]*Data$G[-1]\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Z, nu, matrix(sigma, 3, Data$N), log=TRUE)) \\
\hspace*{0.27 in} mu[1,1] <- alpha[1] + alpha[2]*nu[1,1] + alpha[4]*nu[2,1] \\
\hspace*{0.27 in} mu[1,-1] <- alpha[1] + alpha[2]*nu[1,-1] + \\
\hspace*{0.62 in} alpha[3]*Data$P[-Data$N] + alpha[4]*nu[2,-1] \\
\hspace*{0.27 in} mu[2,1] <- beta[1] + beta[2]*nu[1,1] + beta[4]*Data$K[1] \\
\hspace*{0.27 in} mu[2,-1] <- beta[1] + beta[2]*nu[1,-1] + \\
\hspace*{0.62 in} beta[3]*Data$P[-Data$N] + beta[4]*Data$K[-1] \\
\hspace*{0.27 in} mu[3,1] <- gamma[1] + gamma[2]*nu[3,1] + gamma[4]*Data$A[1] \\
\hspace*{0.27 in} mu[3,-1] <- gamma[1] + gamma[2]*nu[3,-1] + \\
\hspace*{0.62 in} gamma[3]*Data$X[-Data$N] + gamma[4]*Data$A[-1] \\
\hspace*{0.27 in} LL <- LL + sum(dmvnp(t(Data$Y), t(mu), Omega, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Space-Time, Dynamic} \label{spacetime.dynamic}
This approach to space-time or spatiotemporal modeling applies kriging to a stationary spatial component for points in space $s=1,\dots,S$ first at time $t=1$, where space is continuous and time is discrete. Vector $\zeta$ contains these spatial effects. Next, SSM (State Space Model) or DLM (Dynamic Linear Model) components are applied to the spatial parameters ($\phi$, $\kappa$, and $\lambda$) and regression effects ($\beta$). These parameters are allowed to vary dynamically with time $t=2,\dots,T$, and the resulting spatial process is estimated for each of these time-periods. When time is discrete, a dynamic space-time process can be applied. The matrix $\Theta$ contains the dynamically varying stationary spatial effects, or space-time effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across discrete time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ (which may also be dynamic, but is static in this example) and dynamic regression effects matrix $\beta_{1:J,1:T}$. For more information on kriging, see section \ref{kriging}. For more information on SSMs or DLMs, see section \ref{ssm.lin.reg}. To extend this to a large spatial data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu_{s,t} = \textbf{X}_{s,1:J} \beta_{1:J,t} + \Theta_{s,t}$$
$$\Theta_{s,t} = \frac{\Sigma_{s,s,t}}{\sum^S_{r=1} \Sigma_{r,s,t}} \Theta_{s,t-1}, \quad s=1,\dots,S, \quad t=2,\dots,T$$
$$\Theta_{s,1} = \zeta_s$$
$$\zeta \sim \mathcal{N}_S(0, \Sigma_{1:S,1:S,1})$$
$$\Sigma_{1:S,1:S,t} = \lambda^2_t \exp(-\phi_t \textbf{D})^{\kappa[t]}$$
$$\sigma_1 \sim \mathcal{HC}(25)$$
$$\beta_{j,1} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$\beta_{1,t} \sim \mathcal{N}(\beta_{1,t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\beta_{2,t} \sim \mathcal{N}(\beta_{2,t-1}, \sigma^2_3), \quad t=2,\dots,T$$
$$\phi_1 \sim \mathcal{HN}(1000)$$
$$\phi_t \sim \mathcal{N}(\phi_{t-1}, \sigma^2_4) \in [0,\infty], \quad t=2,\dots,T$$
$$\kappa_1 \sim \mathcal{HN}(1000)$$
$$\kappa_t \sim \mathcal{N}(\kappa_{t-1}, \sigma^2_5) \in [0,\infty], \quad t=2,\dots,T$$
$$\lambda_1 \sim \mathcal{HN}(1000)$$
$$\lambda_t \sim \mathcal{N}(\lambda_{t-1}, \sigma^2_6) \in [0,\infty], \quad t=2,\dots,T$$
\subsection{Data}
\code{S <- 20 \\
T <- 10 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
beta <- matrix(c(50,2), 2, T) \\
phi <- rep(1,T); kappa <- rep(1.5,T); lambda <- rep(10000,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} beta[1,t-1] <- beta[1,t-1] + rnorm(1,0,1) \\
\hspace*{0.27 in} beta[2,t-1] <- beta[2,t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} phi[t] <- phi[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} if(phi[t] < 0.001) phi[t] <- 0.001 \\
\hspace*{0.27 in} kappa[t] <- kappa[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} lambda[t] <- lambda[t-1] + rnorm(1,0,1000)\} \\
Sigma <- array(0, dim=c(S,S,T)) \\
for (t in 1:T) \{ \\
\hspace*{0.27 in} Sigma[ , ,t] <- lambda[t] * exp(-phi[t] * D)\textasciicircum kappa[t]\} \\
zeta <- as.vector(apply(rmvn(1000, rep(0,S), Sigma[ , ,1]), 2, mean)) \\
Theta <- matrix(zeta,S,T) \\
for (t in 2:T) \{for (s in 1:S) \{ \\
\hspace*{0.27 in} Theta[,t] <- sum(Sigma[,s,t] / sum(Sigma[,s,t]) * Theta[,t-1])\}\} \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- tcrossprod(X, t(beta)) \\
Y <- mu + Theta + matrix(rnorm(S*T,0,0.1),S,T) \\
mon.names <- c("LP", as.parm.names(list(sigma=rep(0,6)))) \\
parm.names <- as.parm.names(list(zeta=rep(0,S), beta=matrix(0,2,T), \\
\hspace*{0.27 in} log.phi=rep(0,T), log.kappa=rep(0,T), log.lambda=rep(0,T), \\
\hspace*{0.27 in} log.sigma=rep(0,6))) \\
MyData <- list(D=D, S=S, T=T, X=X, Y=Y, latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S), rep(c(mean(Y),0),T), log(rep(1,T)), \\
\hspace*{0.27 in} log(rep(1,T)), rep(1,T), log(rep(1,6)))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], 2, Data$T) \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} phi <- exp(parm[grep("log.phi", Data$parm.names)]) \\
\hspace*{0.27 in} kappa <- exp(parm[grep("log.kappa", Data$parm.names)]) \\
\hspace*{0.27 in} lambda <- exp(parm[grep("log.lambda", Data$parm.names)]) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} Sigma <- array(0, dim=c(Data$S, Data$S, Data$T)) \\
\hspace*{0.27 in} for (t in 1:Data$T) \{ \\
\hspace*{0.62 in} Sigma[ , ,t] <- lambda[t]\textasciicircum 2 * exp(-phi[t] * Data$D)\textasciicircum kappa[t]\} \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta[,1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(beta[,-1], beta[,-Data$T], matrix(sigma[2:3], 2, \\
\hspace*{0.62 in} Data$T-1), log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0,Data$S), Sigma[ , , 1], log=TRUE) \\
\hspace*{0.27 in} phi.prior <- sum(dhalfnorm(phi[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(phi[-1], "norm", a=0, b=Inf, mean=phi[-Data$T], \\
\hspace*{0.62 in} sd=sigma[4], log=TRUE)) \\
\hspace*{0.27 in} kappa.prior <- sum(dhalfnorm(kappa[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(kappa[-1], "norm", a=0, b=Inf, mean=kappa[-Data$T], \\
\hspace*{0.62 in} sd=sigma[5], log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dhalfnorm(lambda[1], sqrt(1000), log=TRUE), \\
\hspace*{0.62 in} dtrunc(lambda[-1], "norm", a=0, b=Inf, mean=lambda[-Data$T], \\
\hspace*{0.62 in} sd=sigma[6], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} Theta <- matrix(zeta, Data$S, Data$T) \\
\hspace*{0.27 in} for (t in 2:Data$T) \{ \\
\hspace*{0.62 in} for (s in 1:Data$S) \{ \\
\hspace*{0.98 in} Theta[,t] <- Sigma[,s,t] / sum(Sigma[,s,t]) * Theta[,t-1]\}\} \\
\hspace*{0.27 in} mu <- mu + Theta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sum(phi.prior) + \\
\hspace*{0.62 in} sum(kappa.prior) + sum(lambda.prior) + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
     \}
}

\section{Space-Time, Nonseparable} \label{spacetime.nonsep}
This approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Matrix $\Xi$ contains the space-time effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses a nonseparable, stationary covariance function in which space and time are separable only when $\psi=0$. To extend this to a large space-time data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu = \textbf{X} \beta + \Xi$$
$$\Xi \sim \mathcal{N}_{ST}(\Xi_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp \left (-\frac{\textbf{D}_S}{\phi_1}^\kappa - \frac{\textbf{D}_T}{\phi_2}^\lambda - \psi \frac{\textbf{D}_S}{\phi_1}^\kappa \frac{\textbf{D}_T}{\phi_2}^\lambda \right )$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$\sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,2$$
$$\psi \sim \mathcal{HC}(25)$$
$$\Xi_\mu = 0$$
$$\kappa = 1, \quad \lambda = 1$$
\subsection{Data}
\code{S <- 10 \\
T <- 5 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(rep(longitude,T),rep(latitude,T)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
D.T <- as.matrix(dist(cbind(rep(1:T,each=S),rep(1:T,each=S)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
Sigma <- 10000 * exp(-D.S/3 - D.T/2 - 0.2*(D.S/3)*(D.T/2)) \\
Xi <- as.vector(apply(rmvn(1000, rep(0,S*T), Sigma), 2, mean)) \\
Xi <- matrix(Xi,S,T) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
Y <- mu + Xi \\
mon.names <- c("LP","psi","sigma[1]","sigma[2]") \\
parm.names <- as.parm.names(list(Xi=matrix(0,S,T), beta=rep(0,2), \\
\hspace*{0.27 in} phi=rep(0,2), log.sigma=rep(0,2), log.psi=0)) \\
MyData <- list(D.S=D.S, D.T=D.T, S=S, T=T, X=X, Y=Y, latitude=latitude, \\
\hspace*{0.27 in} longitude=longitude, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S*T), mean(Y), 0, rep(1,2), rep(0,2), 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} Xi.mu <- rep(0,Data$S*Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} Xi <- parm[grep("Xi", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} psi <- exp(parm[grep("log.psi", Data$parm.names)]) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-(Data$D.S / phi[1])\textasciicircum kappa - \\
\hspace*{0.62 in} (Data$D.T / phi[2])\textasciicircum lambda - \\
\hspace*{0.62 in} psi*(Data$D.S / phi[1])\textasciicircum kappa * (Data$D.T / phi[2])\textasciicircum lambda) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Xi.prior <- dmvn(Xi, Xi.mu, Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} psi.prior <- dhalfcauchy(psi, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Xi <- matrix(Xi, Data$S, Data$T) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) + Xi \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Xi.prior + sigma.prior + phi.prior + psi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,psi,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Space-Time, Separable} \label{spacetime.sep}
This introductory approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Vector $\zeta$ contains the spatial effects and vector $\theta$ contains the temporal effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses separable space-time covariances, which is more convenient but usually less appropriate than a nonseparable covariance function. To extend this to a large space-time data set, consider incorporating the predictive process kriging example in section \ref{kriging.pp}.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu_{s,t} = \textbf{X}_{s,1:J} \beta + \zeta_s + \Theta_{s,t}$$
$$\Theta_{s,1:T} = \theta$$
$$\theta \sim \mathcal{N}_N(\theta_\mu, \Sigma_T)$$
$$\Sigma_T = \sigma^2_3 \exp(-\phi_2 \textbf{D}_T)^\lambda$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma_S)$$
$$ \Sigma_S = \sigma^2_2 \exp(-\phi_1 \textbf{D}_S)^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,3$$
$$ \phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$ \zeta_\mu = 0$$
$$ \theta_\mu = 0$$
$$ \kappa = 1, \quad \lambda = 1$$
\subsection{Data}
\code{S <- 20 \\
T <- 10 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma.S <- 10000 * exp(-1.5 * D.S) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,S), Sigma.S), 2, mean)) \\
D.T <- as.matrix(dist(cbind(c(1:T),c(1:T)), diag=TRUE, upper=TRUE)) \\
Sigma.T <- 10000 * exp(-3 * D.T) \\
theta <- as.vector(apply(rmvn(1000, rep(0,T), Sigma.T), 2, mean)) \\
Theta <- matrix(theta,S,T,byrow=TRUE) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(X, t(beta))) \\
Y <- mu + zeta + Theta + matrix(rnorm(S*T,0,0.1),S,T) \\
mon.names <- c("LP","sigma[1]","sigma[2]","sigma[3]") \\
parm.names <- as.parm.names(list(zeta=rep(0,S), theta=rep(0,T), \\
\hspace*{0.27 in} beta=rep(0,2), phi=rep(0,2), log.sigma=rep(0,3))) \\
MyData <- list(D.S=D.S, D.T=D.T, S=S, T=T, X=X, Y=Y, latitude=latitude, \\
\hspace*{0.27 in} longitude=longitude, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S), rep(0,T), rep(0,2), rep(1,2), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} zeta.mu <- rep(0,Data$S) \\
\hspace*{0.27 in} theta.mu <- rep(0,Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} theta <- parm[grep("theta", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} Sigma.S <- sigma[2]\textasciicircum 2 * exp(-phi[1] * Data$D.S)\textasciicircum kappa \\
\hspace*{0.27 in} Sigma.T <- sigma[3]\textasciicircum 2 * exp(-phi[2] * Data$D.T)\textasciicircum lambda \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, zeta.mu, Sigma.S, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dmvn(theta, theta.mu, Sigma.T, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Theta <- matrix(theta, Data$S, Data$T, byrow=TRUE) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(Data$X, t(beta))) + zeta + Theta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + theta.prior + sigma.prior + \\
\hspace*{0.62 in} phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Spatial Autoregression (SAR)} \label{sar}
The spatial autoregressive (SAR) model in this example uses areal data that consists of first-order neighbors that were specified and converted from point-based data with longitude and latitude coordinates.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta + \phi \textbf{z}$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
latitude <- runif(N,0,100); longitude <- runif(N,0,100) \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(runif(N*J,0,3), N, J); X[,1] <- 1 \\
beta.orig <- runif(J,0,3); phi <- runif(1,0,1) \\
D <- as.matrix(dist(cbind(longitude, latitude), diag=TRUE, upper=TRUE)) \\
W <- exp(-D) \#Inverse distance as weights \\
W <- ifelse(D == 0, 0, W) \\
epsilon <- rnorm(N,0,1) \\
y <- tcrossprod(X, t(beta.orig)) + sqrt(latitude) + sqrt(longitude) + \\
\hspace*{0.27 in} epsilon \\
Z <- W / matrix(rowSums(W), N, N) * matrix(y, N, N, byrow=TRUE) \\
z <- as.vector(apply(Z, 1, sum)) \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), phi=0, log.sigma=0)) \\
MyData <- list(J=J, X=X, latitude=latitude, longitude=longitude, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names, y=y, z=z)}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0.5, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} parm[Data$J+1] <- phi <- interval(parm[Data$J+1], -1, 1) \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+2]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) + phi*Data$z \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{STARMA(1,1)} \label{starma}
The data in this example of a space-time autoregressive moving average (STARMA) are coordinate-based, and the adjacency matrix \textbf{A} is created from $K$ nearest neighbors. Otherwise, an adjacency matrix may be specified as usual for areal data. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_{s,t} = \textbf{X}_{s,t} \beta + \phi \textbf{W1}_{s,t-1} + \theta \textbf{W2}_{s,t-1}, \quad s=1,\dots,S, \quad t=2,\dots,T$$
$$\textbf{W1} = \textbf{V} \textbf{Y}$$
$$\textbf{W2} = \textbf{V} \epsilon$$
$$\epsilon = \textbf{Y} - \mu$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{N}(0, 1000)$$
where \textbf{V} is an adjacency matrix that is scaled so that each row sums to one, $\beta$ is a vector of regression effects, $\phi$ is the autoregressive space-time parameter, $\sigma$ is the residual variance, and $\theta$ is the moving average space-time parameter.
\subsection{Data}
\code{S <- 100 \\
T <- 10 \\
K <- 5 \#Number of nearest neighbors \\
latitude <- runif(S,0,100) \\
longitude <- runif(S,0,100) \\
X1 <- matrix(runif(S*T,-2,2), S, T) \\
X2 <- matrix(runif(S*T,-2,2), S, T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} X1[,t] <- X1[,t-1] + runif(S,-0.1,0.1) \\
\hspace*{0.27 in} X2[,t] <- X2[,t-1] + runif(S,-0.1,0.1)\} \\
beta.orig <- runif(3,-2,2); phi.orig <- 0.8; theta.orig <- 1 \\
epsilon <- matrix(rnorm(S*T,0,0.1), S, T) \\
Z <- matrix(rnorm(S*T,0,0.1), S, T) \\
D <- as.matrix(dist(cbind(longitude, latitude), diag=TRUE, upper=TRUE)) \\
A <- exp(-D) \\
A <- ifelse(D == 0, max(D), A) \\
A <- apply(A, 1, rank) \\
A <- ifelse(A <= K, 1, 0) \\
V <- A / rowSums(A) \#Scaled matrix \\
V <- ifelse(is.nan(V), 1/ncol(V), V) \\
Y <- beta.orig[1] + beta.orig[2]*X1 + beta.orig*X2 \\
W1 <- tcrossprod(V, t(Y)) \\
Y <- Y + phi.orig*cbind(rep(0,S), W1[,-T]) \\
W2 <- tcrossprod(V, t(epsilon)) \\
Y <- Y + theta.orig*cbind(rep(0,S), W2[,-T]) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(beta=rep(0,3), phi=0, log.sigma=0, \\
\hspace*{0.27 in} theta=0)) \\
MyData <- list(K=K, S=S, T=T, V=V, X1=X1, X2=X2, Y=Y, latitude=latitude, \\
\hspace*{0.27 in} longitude=longitude, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), 0, 1, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:3] \\
\hspace*{0.27 in} parm[4] <- phi <- interval(parm[4], -1, 1) \\
\hspace*{0.27 in} sigma <- exp(parm[5]) \\
\hspace*{0.27 in} theta <- parm[6] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnorm(theta, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} W1 <- tcrossprod(Data$V, t(Data$Y)) \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$X1 + beta[3]*Data$X2 + \\
\hspace*{0.62 in} phi*cbind(rep(0, Data$S), W1[,-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$Y - mu \\
\hspace*{0.27 in} W2 <- tcrossprod(Data$V, t(epsilon)) \\
\hspace*{0.27 in} mu <- mu + theta*cbind(rep(0, Data$S), W2[,-Data$T]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y[,-1], mu[,-1], sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior + sigma.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{State Space Model (SSM), Linear Regression} \label{ssm.lin.reg}
The data is presented so that the time-series is subdivided into three 
sections: modeled ($t=1,\dots,T_m$), one-step ahead forecast ($t=T_m+1$), 
and future forecast [$t=(T_m+2),\dots,T$]. Note that \code{Dyn} must also be specified for the SAMWG and SMWG MCMC algorithms.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T_m$$
$$\textbf{y}^{new}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=(T_m+1),\dots,T$$
$$\mu_t = \alpha + \textbf{x}_t \beta_t, \quad t=1,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_t \sim \mathcal{N}(\beta_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
beta.orig <- x <- rep(0,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} beta.orig[t] <- beta.orig[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} x[t] <- x[t-1] + rnorm(1,0,0.1)\} \\
y <- 10 + beta.orig*x + rnorm(T,0,0.01) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) {mon.names[i] <- paste("mu[",(T.m+i),"]", sep="")} \\
parm.names <- as.parm.names(list(alpha=0, beta=rep(0,T), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\ 
\hspace*{0.27 in} x=x, y=y) \\
Dyn <- matrix(paste("beta[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+3)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2:(Data$T+1)] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$T+2:3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(beta[-1], beta[-Data$T], sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta*Data$x \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- alpha + c(beta[1], rnorm(Data$T-1, beta[-Data$T], \\
\hspace*{0.62 in} sigma[2])) * Data$x \#One-step ahead prediction throughout \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{State Space Model (SSM), Local Level} \label{ssm.ll}
The local level model is the simplest, non-trivial example of a state space model (SSM). As such, this version of a local level SSM has static variance parameters.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T$$
$$\mu_t \sim \mathcal{N}(\mu_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\mu_1 \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
mu.orig <- rep(0,T) \\
for (t in 2:T) \{mu.orig[t] <- mu.orig[t-1] + rnorm(1,0,1)\} \\
y <- mu.orig + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) mon.names[i] <- paste("yhat[",(T.m+i),"]", sep="") \\
parm.names <- as.parm.names(list(mu=rep(0,T), log.sigma=rep(0,2))) \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
Dyn <- matrix(paste("mu[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+2)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} mu <- parm[1:Data$T] \\
\hspace*{0.27 in} sigma <- exp(parm[-c(1:Data$T)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} mu.prior <- sum(dnormv(mu[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(mu[-1], mu[-Data$T], sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- c(mu[1], rnorm(Data$T-1, mu[-Data$T], sigma[2])) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + mu.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{State Space Model (SSM), Local Linear Trend} \label{ssm.llt}
The local linear trend model is a state space model (SSM) that extends the local level model to include a dynamic slope parameter. For more information on the local level model, see section \ref{ssm.ll}. This example has static variance parameters.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_1), \quad t=1,\dots,T$$
$$\mu_t \sim \mathcal{N}(\mu_{t-1} + \delta_{t-1}, \sigma^2_2), \quad t=2,\dots,T$$
$$\mu_1 \sim \mathcal{N}(0, 1000)$$
$$\delta_t \sim \mathcal{N}(\delta_{t-1}, \sigma^2_3), \quad t=2,\dots,T$$
$$\delta_1 \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,3$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
mu.orig <- delta.orig <- rep(0,T) \\
for (t in 2:T) \{ \\
\hspace*{0.27 in} delta.orig[t] <- delta.orig[t-1] + rnorm(1,0,0.1) \\
\hspace*{0.27 in} mu.orig[t] <- mu.orig[t-1] + delta.orig[t-1] + rnorm(1,0,1)\} \\
y <- mu.orig + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) mon.names[i] <- paste("yhat[",(T.m+i),"]", sep="") \\
parm.names <- as.parm.names(list(mu=rep(0,T), delta=rep(0,T), \\
\hspace*{0.27 in} log.sigma=rep(0,3))) \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T*2+3)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} mu <- parm[1:Data$T] \\
\hspace*{0.27 in} delta <- parm[Data$T+(1:Data$T)] \\
\hspace*{0.27 in} sigma <- exp(parm[2*Data$T+c(1:3)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} mu.prior <- sum(dnormv(mu[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(mu[-1], mu[-Data$T]+delta[-Data$T], sigma[2], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnormv(delta[1], 0, 1000, log=TRUE), \\
\hspace*{0.62 in} dnorm(delta[-1], delta[-Data$T], sigma[3], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], sigma[1], \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} yhat <- c(mu[1], rnorm(Data$T-1, mu[-Data$T], sigma[2])) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + mu.prior + delta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=yhat, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{State Space Model (SSM), Stochastic Volatility (SV)} \label{sv}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(0, \sigma^2)$$
$$\sigma^2 = \frac{1}{\exp(\theta)}$$
$$\beta = \exp(\mu / 2)$$
$$\theta_1 \sim \mathcal{N}(\mu + \phi (\alpha - \mu), \tau)$$
$$\theta_t \sim \mathcal{N}(\mu + \phi (\theta_{t-1} - \mu), \tau), \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{N}(\mu, \tau)$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\mu \sim \mathcal{N}(0, 10)$$
$$\tau \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 20 \\
y <- rep(10,T); epsilon <- rnorm(T,0,1) \\
for (t in 2:T) \{y[t] <- 0.8*y[t-1] + epsilon[t-1]\} \\
mon.names <- c("LP","tau", paste("sigma2[",1:T,"]",sep="")) \\
parm.names <- as.parm.names(list(theta=rep(0,T), alpha=0, phi=0, mu=0, \\
\hspace*{0.27 in} log.tau=0)) \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
Dyn <- matrix(paste("theta[",1:T,"]",sep=""), T, 1) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+4)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[1:Data$T] \\
\hspace*{0.27 in} alpha <- parm[Data$T+1] \\
\hspace*{0.27 in} parm[Data$T+2] <- phi <- interval(parm[Data$T+2], -1, 1) \\
\hspace*{0.27 in} mu <- parm[Data$T+3] \\
\hspace*{0.27 in} tau <- exp(parm[Data$T+4]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, mu, tau, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnormv(theta[1], mu + phi*(alpha-mu), tau, \\
\hspace*{0.62 in} log=TRUE), dnormv(theta[-1], mu + phi*(theta[-Data$T]-mu), tau, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} mu.prior <- dnormv(mu, 0, 10, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} beta <- exp(mu / 2) \\
\hspace*{0.27 in} sigma2 <- 1 / exp(theta) \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y, 0, sigma2, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + theta.prior + phi.prior + mu.prior + \\
\hspace*{0.62 in} tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, tau, sigma2), \\
\hspace*{0.62 in} yhat=rep(0,T), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Threshold Autoregression (TAR)} \label{tar}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\nu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha_2 + \phi_2 \textbf{y}_T$$
\[\nu_t = \left\{
\begin{array}{l l}
  \alpha_1 + \phi_1 \textbf{y}_{t-1}, \quad t=1,\dots,T & \quad \mbox{if $t \ge \theta$}\\
  \alpha_2 + \phi_2 \textbf{y}_{t-1}, \quad t=1,\dots,T & \quad \mbox{if $t < \theta$} \\ \end{array} \right. \]
$$\alpha_j \sim \mathcal{N}(0, 1000) \in [-1,1], \quad j=1,\dots,2$$
$$\phi_j \sim \mathcal{N}(0, 1000), \in [-1,1], \quad j=1,\dots,2$$
$$\theta \sim \mathcal{U}(2, T-1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in}  2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\ 
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\ 
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\ 
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\ 
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\ 
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\ 
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\ 
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\ 
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\ 
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\ 
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\ 
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\ 
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\ 
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\ 
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\ 
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\ 
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\ 
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\ 
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\ 
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\ 
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\ 
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\ 
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\ 
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\ 
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew") \\
parm.names <- as.parm.names(list(alpha=rep(0,2), phi=rep(0,2), theta=0, \\
\hspace*{0.27 in} log.sigma=0)) \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), T/2, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1:2], -1, 1); parm[1:2] <- alpha \\
\hspace*{0.27 in} phi <- interval(parm[3:4], -1, 1); parm[3:4] <- phi \\
\hspace*{0.27 in} theta <- interval(parm[5], 2, Data$T-1); parm[5] <- theta \\
\hspace*{0.27 in} sigma <- exp(parm[6]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dtrunc(alpha, "norm", a=-1, b=1, mean=0, \\
\hspace*{0.62 in} sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dtrunc(phi, "norm", a=-1, b=1, mean=0, \\
\hspace*{0.62 in} sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dnormv(phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- dunif(theta, 2, Data$T-1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0, Data$T, 2) \\
\hspace*{0.27 in} mu[,1] <- c(alpha[1], alpha[1] + phi[1]*Data$y[-Data$T]) \\
\hspace*{0.27 in} mu[,2] <- c(alpha[2], alpha[2] + phi[2]*Data$y[-Data$T]) \\
\hspace*{0.27 in} nu <- ifelse(1:Data$T < theta, mu[,1], mu[,2]) \\
\hspace*{0.27 in} ynew <- alpha[2] + phi[2]*Data$y[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, nu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + theta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew), \\
\hspace*{0.62 in} yhat=nu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{TARCH(1)} \label{tarch}
In this TARCH example, there are two regimes, one for positive residuals in the previous time-period, and the other for negative. The TARCH parameters are the $\theta$ vector.
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=2,\dots,T$$
$$\sigma^2_t = \omega + \theta_1 \delta_{t-1} \epsilon^2_{t-1} + \theta_2 (1-\delta_{t-1}) \epsilon^2_{t-1}, \quad t=2,\dots,T$$
\[\delta_t = \left\{ 
\begin{array}{l l}
  1 & \quad \mbox{if $\epsilon_t > 0$}\\
  0 \\ \end{array} \right. \]
$$\epsilon = \textbf{y} - \mu$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{U}(-1, 1)$$
$$\omega \sim \mathcal{HC}(25)$$
$$\theta_j \sim \mathcal{U}(0, 1), \quad j=1,\dots,2$$
\subsection{Data}
\code{T <- 20 \\
phi <- 0.8 \\
epsilon <- rnorm(T) \\
epsilon <- ifelse(epsilon < 0, epsilon * 2, epsilon) \\
y <- rep(0,T) \\
for (t in 2:T) \{y[t] <- phi*y[t-1] + epsilon[t]\} \\
mon.names <- c("LP","ynew","sigma2.new") \\
parm.names <- as.parm.names(list(alpha=0, phi=0, log.omega=0, \\
\hspace*{0.27 in} theta=rep(0,2))) \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, 0, 1, 0.5, 0.5)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} parm[2] <- phi <- interval(parm[2], -1, 1) \\
\hspace*{0.27 in} omega <- exp(parm[3]) \\
\hspace*{0.27 in} parm[4:5] <- theta <- interval(parm[4:5], 0.001, 0.999) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnormv(alpha, 0, 1000, log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, -1, 1, log=TRUE) \\
\hspace*{0.27 in} omega.prior <- dhalfcauchy(omega, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dunif(theta, 0, 1, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + c(0, Data$y[-Data$T]) * phi \\
\hspace*{0.27 in} ynew <- alpha + Data$y[Data$T] * phi \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} delta <- (epsilon > 0) * 1 \\
\hspace*{0.27 in} sigma2 <- omega + theta[1] * c(0,delta[-Data$T]) * \\
\hspace*{0.62 in} c(0, epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[2] * (1 - delta[-Data$T]) * \\
\hspace*{0.62 in} epsilon[-Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} sigma2.new <- omega + theta[1] * delta[Data$T] * epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[2] * (1 - delta[Data$T]) * epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} LL <- sum(dnormv(Data$y[-1], mu[-1], sigma2[-1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + omega.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Variable Selection, BAL} \label{bal}
This approach to variable selection is one of several forms of the Bayesian Adaptive Lasso (BAL). The lasso applies shrinkage to exchangeable scale parameters, $\gamma$, for the regression effects, $\beta$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, \gamma_j), \quad j=1,\dots,J$$
$$\gamma_j \sim \mathcal{G}^{-1}(\delta, \tau)$$
$$\delta \sim \mathcal{HC}(25)$$
$$\tau \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.gamma=rep(0,J), \\
\hspace*{0.27 in} log.delta=0, log.tau=0, log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J), rep(1,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} delta <- exp(parm[2*Data$J+1]) \\
\hspace*{0.27 in} tau <- exp(parm[2*Data$J+2]) \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma <- exp(parm[Data$J+1:Data$J]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[2*Data$J+3]) \\
\hspace*{0.27 in} \#\#\# Log(Hyperhyperprior Densities) \\
\hspace*{0.27 in} delta.prior <- dhalfcauchy(delta, 25, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dhalfcauchy(tau, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} gamma.prior <- sum(dinvgamma(gamma, delta, tau, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, gamma, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + delta.prior + tau.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Variable Selection, SSVS} \label{ssvs}
This example uses a modified form of the random-effects (or global adaptation) Stochastic Search Variable Selection (SSVS) algorithm presented in \citet{ohara09}, which selects variables according to practical significance rather than statistical significance. Here, SSVS is applied to linear regression, though this method is widely applicable. For $J$ variables, each regression effects vector $\beta_j$ is conditional on $\gamma_j$, a binary inclusion variable. Each $\beta_j$ is a discrete mixture distribution with respect to $\gamma_j = 0$ or $\gamma_j = 1$, with precision 100 or $\beta_\sigma = 0.1$, respectively. As with other representations of SSVS, these precisions may require tuning.

With other representations of SSVS, each $\gamma_j$ is Bernoulli-distributed, though this would be problematic in Laplace's Demon, because $\gamma_j$ would be in the list of parameters (rather than monitors), and would not be stationary due to switching behavior. To keep $\gamma$ in the monitors, an uninformative normal density is placed on each prior $\delta_j$, with mean $1/J$ for $J$ variables and variance $1000$. Each $\delta_j$ is transformed with the inverse logit and rounded to $\gamma_j$. Note that $\lfloor x + 0.5 \rfloor$ means to round $x$. The prior for $\delta$ can be manipulated to influence sparseness.

When the goal is to select the best model, each $\textbf{X}_{1:N,j}$ is retained for a future run when the posterior mean of $\gamma_j \ge 0.5$. When the goal is model-averaging, the results of this model may be used directly, which would please L. J. Savage, who said that ``models should be as big as an elephant'' \citep{draper95}.

\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X} \beta$$
$$(\beta_j | \gamma_j) \sim (1 - \gamma_j)\mathcal{N}(0, 0.01) + \gamma_j \mathcal{N}(0, \beta^2_\sigma) \quad j=1,\dots,J$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_j = \lfloor \frac{1}{1 + \exp(-\delta_j)} + 0.5 \rfloor, \quad j=1,...,J$$
$$\delta_j \sim \mathcal{N}(0, 10) \in [-100,100], \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", "min.beta.sigma", "sigma", \\
\hspace*{0.27 in} as.parm.names(list(gamma=rep(0,J)))) \\
parm.names <- as.parm.names(list(beta=rep(0,J), delta=rep(0,J), \\
\hspace*{0.27 in} log.beta.sigma=0, log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J), log(1), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- exp(parm[grep("log.beta.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} delta <- interval(parm[grep("delta", Data$parm.names)],-100,100) \\
\hspace*{0.27 in} parm[grep("delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} gamma <- round(invlogit(delta)) \\
\hspace*{0.27 in} beta.sigma <- ifelse(gamma == 0, 0.1, beta.sigma) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior Densities) \\
\hspace*{0.27 in} beta.sigma.prior <- sum(dhalfcauchy(beta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=-100, b=100, \\
\hspace*{0.62 in} mean=logit(1/Data$J), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + beta.sigma.prior + delta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, min(beta.sigma), \\
\hspace*{0.62 in} sigma, gamma), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Vector Autoregression, VAR(1)} \label{var1}
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=1,\dots,T, \quad j=1,\dots,J$$
$$\mu_{t,j} = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{t-1,j}$$
$$\textbf{y}^{new}_j = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{T,j}$$
$$\alpha_j \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25)$$
$$\Phi_{i,k} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,J, \quad k=1,\dots,J$$ 
\subsection{Data}
\code{T <- 100 \\
J <- 3 \\
Y <- matrix(0,T,J) \\
for (j in 1:J) \{for (t in 2:T) \{ \\
\hspace*{0.27 in} Y[t,j] <- Y[t-1,j] + rnorm(1,0,0.1)\}\} \\
mon.names <- c("LP", as.parm.names(list(ynew=rep(0,J)))) \\
parm.names <- as.parm.names(list(alpha=rep(0,J), Phi=matrix(0,J,J), \\
\hspace*{0.27 in} log.sigma=rep(0,J))) \\
MyData <- list(J=J, T=T, Y=Y, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(colMeans(Y), rep(0,J*J), rep(log(1),J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$J] \\
\hspace*{0.27 in} Phi <- matrix(parm[grep("Phi", Data$parm.names)], Data$J, Data$J) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} Phi.prior <- sum(dnormv(Phi, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha,Data$T,Data$J,byrow=TRUE) \\
\hspace*{0.62 in} mu[-1,] <- mu[-1,] + tcrossprod(Data$Y[-Data$T,], Phi) \\
\hspace*{0.27 in} ynew <- alpha + as.vector(crossprod(Phi, Data$Y[Data$T,])) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, \\
\hspace*{0.62 in} matrix(sigma,Data$T,Data$J,byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + Phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Weighted Regression} \label{weighted.reg}
It is easy enough to apply record-level weights to the likelihood. Here, weights are applied to the linear regression example in section \ref{linear.reg}.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- nrow(demonsnacks) \\
J <- ncol(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
w <- c(rep(1,5), 0.2, 1, 0.01, rep(1,31)) \\
w <- w * (sum(w) / N) \\
mon.names <- c("LP","sigma") \\
parm.names <- as.parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, w=w, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 1000, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(Data$X, t(beta)) \\
\hspace*{0.27 in} LL <- sum(w * dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Zero-Inflated Poisson (ZIP)} \label{zip}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\Lambda_{1:N,2})$$
$$\textbf{z} \sim \mathcal{BERN}(\Lambda_{1:N,1})$$
\[\textbf{z}_i = \left\{ 
\begin{array}{l l}
  1 & \quad \mbox{if $\textbf{y}_i = 0$}\\
  0 \\ \end{array} \right. \]
\[\Lambda_{i,2} = \left\{ 
\begin{array}{l l}
  0 & \quad \mbox{if $\Lambda_{i,1} \ge 0.5$}\\
  \Lambda_{i,2} \\ \end{array} \right. \]
$$\Lambda_{1:N,1} = \frac{1}{1 + \exp(-\textbf{X}_1 \alpha)}$$
$$\Lambda_{1:N,2} = \exp(\textbf{X}_2 \beta)$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_1$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_2$$
\subsection{Data}
\code{N <- 1000 \\
J1 <- 4 \\
J2 <- 3 \\
X1 <- matrix(runif(N*J1,-2,2),N,J1); X1[,1] <- 1 \\
X2 <- matrix(runif(N*J2,-2,2),N,J2); X2[,1] <- 1 \\
alpha <- runif(J1,-1,1) \\
beta <- runif(J2,-1,1) \\
p <- invlogit(tcrossprod(X1, t(alpha)) + rnorm(N,0,0.1)) \\
mu <- round(exp(tcrossprod(X2, t(beta)) + rnorm(N,0,0.1))) \\
y <- ifelse(p > 0.5, 0, mu) \\
z <- ifelse(y == 0, 1, 0) \\
mon.names <- "LP" \\
parm.names <- as.parm.names(list(alpha=rep(0,J1), beta=rep(0,J2))) \\
MyData <- list(J1=J1, J2=J2, N=N, X1=X1, X2=X2, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y, z=z)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J1+J2)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} parm[1:Data$J1] <- alpha <- interval(parm[1:Data$J1], -5, 5) \\
\hspace*{0.27 in} beta <- parm[Data$J1+1:Data$J2] \\
\hspace*{0.27 in} parm[Data$J1+1:Data$J2] <- beta \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnormv(alpha, 0, 5, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnormv(beta, 0, 5, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- matrix(NA, Data$N, 2) \\
\hspace*{0.27 in} Lambda[,1] <- invlogit(tcrossprod(Data$X1, t(alpha))) \\
\hspace*{0.27 in} Lambda[,2] <- exp(tcrossprod(Data$X2, t(beta))) \\
\hspace*{0.27 in} Lambda[,2] <- ifelse(Lambda[,1] >= 0.5, 0, Lambda[,2]) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$z, Lambda[,1], log=TRUE), \\
\hspace*{0.62 in} dpois(Data$y, Lambda[,2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=Lambda[,2], parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\bibliography{References.bib}

\end{document}
