\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Examples}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Byron Hall\\STATISTICAT, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon} Examples}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Byron Hall} %% comma-separated
\Plaintitle{LaplacesDemon Examples} %% without formatting
\Shorttitle{Examples} %% a short title (if necessary)

\Abstract{The \pkg{LaplacesDemon} package in \proglang{R} enables Bayesian inference with any Bayesian model, provided the user specifies the likelihood.  This vignette is a compendium of examples of how to specify different model forms.
}
\Keywords{Bayesian, Bayesian Inference, Laplace's Demon, LaplacesDemon, R, 
STATISTICAT}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
  Byron Hall\\
  STATISTICAT, LLC\\
  Farmington, CT\\
  E-mail: \email{statisticat@gmail.com}\\
  URL: \url{http://www.statisticat.com/laplacesdemon.html}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
\pkg{LaplacesDemon} \citep{r:laplacesdemon}, usually referred to as Laplace's Demon, is an \proglang{R} package that is available on CRAN \citep{rdct:r}. A formal introduction to Laplace's Demon is provided in an accompanying vignette entitled ``\pkg{LaplacesDemon} Tutorial'', and an introduction to Bayesian inference is provided in the ``Bayesian Inference'' vignette.

The purpose of this document is to provide users of the \pkg{LaplacesDemon} package with examples of a variety of Bayesian methods. To conserve space, the examples are not worked out in detail, and only the minimum of necessary materials is provided for using the various methodologies. Necessary materials include the form expressed in notation, data (which is often simulated), initial values, and the \code{Model} function. This vignette will grow over time as examples of more methods become included. Contributed examples are welcome. Please send contributed examples in a similar format in an email to \email{statisticat@gmail.com} for review and testing. All accepted contributions are, of course, credited.

\begin{center} \textbf{Contents} \end{center}
\begin{itemize}
\item ANOVA, One-Way \ref{anova.one.way}
\item Autoregression, AR(1) \ref{ar1}
\item Binary Logit \ref{binary.logit}
\item Binary Probit \ref{binary.probit}
\item Binomial Logit \ref{binomial.logit}
\item Binomial Probit \ref{binomial.probit}
\item Contingency Table \ref{contingency.table}
\item Dynamic Linear Model (DLM) \ref{dlm}
\item Factor Analysis, Confirmatory (CFA) \ref{cfa}
\item Factor Analysis, Exploratory (EFA) \ref{efa}
\item Laplace Regression \ref{laplace.reg}
\item Linear Regression \ref{linear.reg}
\item Linear Regression, Multilevel \ref{linear.reg.ml}
\item Linear Regression with Full Missingness \ref{linear.reg.full.miss}
\item Linear Regression with Missing Response \ref{linear.reg.miss.resp}
\item Model Averaging \ref{variable.selection}
\item Multinomial Logit \ref{mnl}
\item Multinomial Logit, Nested \ref{nmnl}
\item Normal, Multilevel \ref{norm.ml}
\item Panel, Autoregressive Poisson \ref{panel.ap}
\item Poisson Regression \ref{poisson.reg}
\item Seemingly Unrelated Regression (SUR) \ref{sur}
\item T-test \ref{anova.one.way}
\item Variable Selection \ref{variable.selection}
\item Zero-Inflated Poisson (ZIP) \ref{zip}
\end{itemize}

\section{ANOVA, One-Way} \label{anova.one.way}
When $J=2$, this is a Bayesian form of a t-test.
\subsection{Form}
$$y \sim \mathcal{N}(\mu, \tau^{-1})$$
$$\mu_i = \alpha + \beta[x_i], \quad i=1,\dots,N$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \displaystyle\sum^{J-1}_{j=1} \beta_j$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
x <- round(runif(N, 0.5, J+0.49)) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[1] <- -sum(beta[2:J]) \\
sigma <- runif(J,0.1,0.5) \\
y <- rep(NA, N) \\
for (i in 1:N) \{y[i] <- alpha + beta[x[i]] + runif(1,0,sigma[x[i]])\} \\
mon.names <- c("LP","beta[1]","tau") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J-1), log.tau=0)) \\
MyData <- list(J=J, N=N, mon.names=mon.names, parm.names=parm.names, x=x, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- 0 \\
\hspace*{0.27 in} alpha.tau <- 1.0E-3 \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J-1) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J-1) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rep(NA, Data$N) \\
\hspace*{0.27 in} for (j in 1:Data$J) \{ \\
\hspace*{0.62 in} mu <- ifelse(Data$x == j, alpha + beta[j], mu)\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,beta[Data$J],tau), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Autoregression, AR(1)} \label{ar1}
\subsection{Form}
$$y_t \sim \mathcal{N}(\mu_{t-1}, \tau^{-1}), \quad t=2,\dots,(T-1)$$
$$y^{new}_T \sim \mathcal{N}(\mu_T, \tau^{-1})$$
$$\mu_t = \alpha + \phi y_t, \quad t=1,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{T <- 100 \\
y <- rep(0,T) \\
y[1] <- 0 \\
for (t in 2:T) \{y[t] <- y[t-1] + rnorm(1,0,0.1)\} \\
mon.names <- c("LP", "tau", paste("mu[",T,"]", sep="")) \\
parm.names <- c("alpha","phi","log.tau") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- 0; alpha.tau <- 1.0E-3 \\
\hspace*{0.27 in} phi.mu <- 0; phi.tau <- 1.0E-3 \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3; tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; tau <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, phi.mu, 1/sqrt(phi.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + phi*Data$y \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[2:(Data$T-1)], mu[1:(Data$T-2)], \\ 
\hspace*{0.62 in} 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,tau,mu[Data$T]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Logit} \label{binary.logit}
\subsection{Form}
$$y \sim \mathrm{Bern}(\eta)$$
$$\eta = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} eta <- invlogit(mu) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=eta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Probit} \label{binary.probit}
\subsection{Form}
$$y \sim \mathrm{Bern}(p)$$
$$p = \phi(\mu)$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=p, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
     \}
}

\section{Binomial Logit} \label{binomial.logit}
\subsection{Form}
$$y \sim \mathrm{Bin}(p, n)$$
$$p = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \beta_1 + \beta_2 x$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the inverse CDF, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} p <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=p, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Binomial Probit} \label{binomial.probit}
\subsection{Form}
$$y \sim \mathrm{Bin}(p, n)$$
$$p = \phi(\mu)$$
$$\mu = \beta_1 + \beta_2 x$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the inverse CDF, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=p, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Contingency Table} \label{contingency.table}
The two-way contingency table, matrix $\textbf{Y}$, can easily be extended to more dimensions. For this example, it is vectorized as $y$, and used like an ANOVA data set. Contingency table $\textbf{Y}$ has J rows and K columns. The cell counts are fit with Poisson regression, according to intercept $\alpha$, main effects $\beta_j$ for each row, main effects $\gamma_k$ for each column, and interaction effects $\delta_{j,k}$ for dependence effects. An omnibus (all cells) test of independence is done by estimating two models (one with $\delta$, and one without), and a large enough Bayes Factor indicates a violation of independence when the model with $\delta$ fits better than the model without $\delta$. In an ANOVA-like style, main effects contrasts can be used to distinguish rows or groups of rows from each other, as well as with columns.  Likewise, interaction effects contrasts can be used to test independence in groups of $\delta_{j,k}$ elements. Finally, single-cell interactions can be used to indicate violations of independence for a given cell, such as when zero is not within its 95\% probability interval. Although a little different, this example is similar to a method presented by \citet{albert97}. 
\subsection{Form}
$$\textbf{Y}_{j,k} \sim \mathrm{Pois}(\lambda_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\lambda_{j,k} = \exp(\alpha + \beta_j + \gamma_k + \delta_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \beta^{-1}_\tau), \quad j=1,\dots,J$$
$$\beta_\tau \sim \Gamma(0.001, 0.001)$$
$$\gamma_k \sim \mathcal{N}(0, \gamma^{-1}_\tau), \quad k=1,\dots,K$$
$$\gamma_\tau \sim \Gamma(0.001, 0.001)$$
$$\delta_{j,k} \sim \mathcal{N}(0, \delta^{-1}_\tau)$$
$$\delta_\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{J <- 4 \#Rows \\
K <- 4 \#Columns \\
Y <- matrix(c(10,20,60,20, 40,30,10,40, 10,10,40,10, 40,50,1,40), J, K, \\
\hspace*{0.27 in} dimnames=list(c("Chrysler","Ford","Foreign","GM"), \\
\hspace*{0.27 in} c("I-4","I-6","V-6","V-8"))) \\
y <- as.vector(Y) \\
N <- length(y) \#Cells \\
r <- rep(1:J, N/J) \\
c <- rep(1,K) \\
for (k in 2:K) \{c <- c(c, rep(k, K))\} \\
mon.names <- c("LP","beta.tau","gamma.tau","delta.tau") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J), gamma=rep(0,J), \\
\hspace*{0.27 in} log.b.tau=0, log.g.tau=0, log.d.tau=0, delta=matrix(0,J,K)) \\
MyData <- list(J=J, K=K, N=N, c=c, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, r=r, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,J), rep(0,K), rep(0,3), rep(0,J*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.tau <- exp(parm[grep("log.b.tau", Data$parm.names)]) \\
\hspace*{0.27 in} gamma.tau <- exp(parm[grep("log.g.tau", Data$parm.names)]) \\
\hspace*{0.27 in} delta.tau <- exp(parm[grep("log.d.tau", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- parm[min(grep("beta", Data$parm.names)):max( \\
\hspace*{0.62 in} grep("beta", Data$parm.names))] \\
\hspace*{0.27 in} gamma <- parm[min(grep("gamma", Data$parm.names)):max( \\
\hspace*{0.62 in} grep("gamma", Data$parm.names))] \\
\hspace*{0.27 in} delta <- matrix(parm[min(grep("delta", \\ 
\hspace*{0.62 in} Data$parm.names)):max(grep("delta", Data$parm.names))], \\
\hspace*{0.62 in} Data$J, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, 1/sqrt(0.001), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, 0, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} beta.tau.prior <- dgamma(beta.tau, 1.0E-3, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} gamma.prior <- dnorm(gamma, 0, 1/sqrt(gamma.tau), log=TRUE) \\
\hspace*{0.27 in} gamma.tau.prior <- dgamma(gamma.tau, 1.0E-3, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnorm(delta, 0, 1/sqrt(delta.tau), log=TRUE) \\
\hspace*{0.27 in} delta.tau.prior <- dgamma(delta.tau, 1.0E-3, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- rep(NA, Data$N) \\
\hspace*{0.27 in} for (i in 1:Data$N) \{ \\
\hspace*{0.62 in} lambda[i] <- exp(alpha + beta[r[i]] + gamma[c[i]] + \\
\hspace*{1.02 in} delta[r[i],c[i]])\} \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sum(beta.prior) + beta.tau.prior + \\
\hspace*{0.62 in} sum(gamma.prior) + gamma.tau.prior + sum(delta.prior) + \\
\hspace*{0.62 in} delta.tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta.tau, \\
\hspace*{0.62 in} gamma.tau, delta.tau), yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Dynamic Linear Model (DLM)} \label{dlm}
The data is presented so that the time-series is subdivided into three 
sections: modeled ($t=1,\dots,T_m$), one-step ahead forecast ($t=T_m+1$), 
and future forecast [$t=(T_m+2),\dots,T$].
\subsection{Form}
$$y_t \sim \mathcal{N}(\mu_t, \tau^{-1}_V), \quad t=1,\dots,T_m$$
$$y^{new}_t \sim \mathcal{N}(\mu_t, \tau^{-1}_V), \quad t=(T_m+1),\dots,T$$
$$\mu_t = \alpha + x_t \beta_t, \quad t=1,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_t \sim \mathcal{N}(\beta_{t-1}, \tau^{-1}_W), \quad t=2,\dots,T$$
$$\tau_V \sim \Gamma(0.001, 0.001)$$
$$\tau_W \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
beta.orig <- x <- rep(0,T) \\
for (t in 2:T) \{ \\
     beta.orig[t] <- beta.orig[t-1] + rnorm(1,0,0.1) \\
     x[t] <- x[t-1] + rnorm(1,0,0.1)\} \\
y <- 10 + beta.orig*x + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) {mon.names[i] <- paste("mu[",(T.m+i),"]", sep="")} \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,T), log.beta.w.tau=0, \\
\hspace*{0.27 in} log.v.tau=0)) \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\ 
\hspace*{0.27 in} x=x, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+3)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2:(Data$T+1)] \\
\hspace*{0.27 in} beta.w.tau <- exp(parm[Data$T+2]) \\
\hspace*{0.27 in} v.tau <- exp(parm[Data$T+3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, 1/sqrt(1.0E-3), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- rep(0,Data$T) \\
\hspace*{0.27 in} beta.prior[1] <- dnorm(beta[1], 0, 1/sqrt(1.0E-3), log=TRUE) \\
\hspace*{0.27 in} beta.prior[2:Data$T] <- dnorm(beta[2:Data$T], beta[1:(Data$T-1)], \\
\hspace*{0.62 in} 1/sqrt(beta.w.tau), log=TRUE) \\
\hspace*{0.27 in} beta.w.tau.prior <- dgamma(beta.w.tau, 0.001, 0.001, log=TRUE) \\
\hspace*{0.27 in} v.tau.prior <- dgamma(v.tau, 1.0E-3, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta*Data$x \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], 1/sqrt(v.tau), \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sum(beta.prior) + beta.w.tau.prior + \\
\hspace*{0.62 in} v.tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Analysis, Confirmatory} \label{cfa}
Factor scores are in matrix \textbf{F}, factor loadings for each variable are in vector $\lambda$, and $f$ is vector that indicates which variable loads on which factor.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \tau^{-1}_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu_{i,m} = \alpha_m + \lambda_m \textbf{F}_{i,f[m]}, \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\textbf{F}_{i,1:P} \sim N_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\lambda_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\Omega \sim \mathrm{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_P$$ 
\subsection{Data}
\code{data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
\hspace*{0.27 in} swiss$Catholic, swiss$Infant.Mortality) \\
M <- NCOL(Y) \#Number of variables \\
N <- NROW(Y) \#Number of records \\
P <- 3 \#Number of factors \\
f <- c(1,3,2,2,1) \#Indicator f for the factor for each variable m \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- c("LP","mu[1,1]") \\
parm.names <- parm.names(list(F=matrix(0,N,P), lambda=rep(0,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.tau=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, f=f, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, N*P), rep(0, M), \\
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)], rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- rep(0, Data$M) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} lambda.mu <- rep(0, Data$M) \\
\hspace*{0.27 in} lambda.tau <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} tau.alpha <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} tau.beta <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[min(grep("alpha", Data$parm.names)):max(grep("alpha", \\
\hspace*{0.62 in} Data$parm.names))] \\
\hspace*{0.27 in} lambda <- parm[min(grep("lambda", Data$parm.names)):max(grep("lambda", \\
\hspace*{0.62 in} Data$parm.names))] \\
\hspace*{0.27 in} tau <- exp(parm[min(grep("log.tau", Data$parm.names)):max(grep( \\
\hspace*{0.62 in} "log.tau", Data$parm.names))]) \\
\hspace*{0.27 in} F <- matrix(parm[min(grep("F", Data$parm.names)):max(grep("F", \\
\hspace*{0.62 in} Data$parm.names))], Data$N, Data$P) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$P, Data$P) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[min(grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)):max(grep("Omega", Data$parm.names))] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} lambda.prior <- dnorm(lambda, lambda.mu, 1/sqrt(lambda.tau), \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- rep(NA, Data$N) \\
\hspace*{0.27 in} for (i in 1:Data$N) \{ \\
\hspace*{0.62 in} F.prior[i] <- dmvn(F[i,], Data$gamma, Sigma, log=TRUE)\} \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} for (m in 1:Data$M) \{ mu[,m] <- alpha[m] + lambda[m] * F[,Data$f[m]]\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- sum(LL) + sum(alpha.prior) + sum(lambda.prior) + \\
\hspace*{0.62 in} sum(tau.prior) + sum(F.prior) + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,mu[1,1]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Analysis, Exploratory} \label{efa}
Factor scores are in matrix \textbf{F} and factor loadings are in matrix $\Lambda$. Although the calculation for the recommended number of factors to explore $P$ is also provided below \citep{fokoue04}, this example sets $P=3$.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \tau^{-1}_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu_{i,m} = \alpha_m + \displaystyle\sum^P_{p=1} \nu_{i,m,p}, \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\nu_{i,m,p} = \textbf{F}_{i,p} \Lambda_{p,m}, \quad i=1,\dots,N, \quad m=1,\dots,M, \quad p=1,\dots,P$$
$$\textbf{F}_{i,1:P} \sim N_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\gamma_p = 0, \quad p=1,\dots,P$$
$$\Lambda_{p,m} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad m=1,\dots,M$$
$$\Omega \sim \mathrm{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\tau_m \sim \Gamma(0.001, 0.001), \quad m=1,\dots,M$$
\subsection{Data}
\code{
data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
          swiss$Catholic, swiss$Infant.Mortality) \\
M <- NCOL(Y) \#Number of variables \\
N <- NROW(Y) \#Number of records \\
P <- trunc(0.5*(2*M + 1 - sqrt(8*M + 1))) \#Number of factors to explore \\
P <- 3 \#Number of factors to explore (override for this example) \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- c("LP","mu[1,1]") \\
parm.names <- parm.names(list(F=matrix(0,N,P), Lambda=matrix(0,P,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.tau=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, gamma=gamma, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0, (N*P + P*M)), \\
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)], rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- rep(0, Data$M) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} Lambda.mu <- 0 \\
\hspace*{0.27 in} Lambda.tau <- 1.0E-3 \\
\hspace*{0.27 in} tau.alpha <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} tau.beta <- rep(1.0E-3, Data$M) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[min(grep("alpha", Data$parm.names)):max(grep("alpha", \\
\hspace*{0.62 in} Data$parm.names))] \\
\hspace*{0.27 in} tau <- exp(parm[min(grep("log.tau", Data$parm.names)):max(grep( \\
\hspace*{0.62 in} "log.tau", Data$parm.names))]) \\
\hspace*{0.27 in} F <- matrix(parm[min(grep("F", Data$parm.names)):max(grep("F", \\
\hspace*{0.62 in} Data$parm.names))], Data$N, Data$P) \\
\hspace*{0.27 in} Lambda <- matrix(parm[min(grep("Lambda", Data$parm.names)):max(grep( \\
\hspace*{0.62 in} "Lambda", Data$parm.names))], Data$P, Data$M) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$P, Data$P) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[min(grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)):max(grep("Omega", Data$parm.names))] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- rep(NA, Data$N) \\
\hspace*{0.27 in} for (i in 1:Data$N) \{ \\
\hspace*{0.62 in} F.prior[i] <- dmvn(F[i,], Data$gamma, Sigma, log=TRUE)\} \\
\hspace*{0.27 in} Lambda.prior <- dnorm(Lambda, Lambda.mu, 1/sqrt(Lambda.tau), \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} nu <- array(NA, dim=c(Data$N, Data$M, Data$P)) \\
\hspace*{0.27 in} for (p in 1:Data$P) \{nu[, ,p] <- F[,p, drop=FALSE] \%*\% Lambda[p,]\} \\
\hspace*{0.27 in} for (m in 1:Data$M) \{mu[,m] <- alpha[m] + apply(nu[,1,],1,sum)\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- sum(LL) + sum(alpha.prior) + sum(tau.prior) + Omega.prior + \\
\hspace*{0.62 in} sum(F.prior) + sum(Lambda.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,mu[1,1]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Laplace Regression} \label{laplace.reg}
This linear regression specifies that $y$ is Laplace-distributed, where it is usually Gaussian or normally-distributed.  It has been claimed that it should be surprising that the normal distribution became the standard, when the Laplace distribution usually fits better and has wider tails \citep{kotz01}. Another popular alternative is to use the t-distribution, though it is more computationally expensive to estimate, because it has three parameters.  The Laplace distribution has only two parameters, location and scale like the normal distribution, and is computationally easier to fit.  This example could be taken one step further, and the parameter vector $\beta$ could be Laplace-distributed.  Laplace's Demon recommends that users experiment with replacing the normal distribution with the Laplace distribution.
\subsection{Form}
$$y \sim \mathrm{L}(\mu, \tau^{-1})$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rlaplace(N,0,0.1) \\
y <- as.vector(beta \%*\% t(X) + e) \\
mon.names <- c("LP", "tau") \\
parm.names <- parm.names(list(beta=rep(0,J), log.tau=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} LL <- sum(dlaplace(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, tau), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression} \label{linear.reg}
\subsection{Form}
$$y \sim \mathcal{N}(\mu, \tau^{-1})$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(beta \%*\% t(X) + e) \\
mon.names <- c("LP", "tau") \\
parm.names <- parm.names(list(beta=rep(0,J), log.tau=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, tau), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression, Multilevel} \label{linear.reg.ml}
\subsection{Form}
$$y \sim \mathcal{N}(\mu, \tau^{-1})$$
$$\mu_i = \textbf{X} \beta_{m[i],1:J}$$
$$\beta_{g,1:J} \sim \mathcal{N}_J(\gamma, \Sigma), \quad g=1,\dots,G$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathrm{W}(J, \textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
where $m$ is a vector of length $N$, and each element indicates the multilevel group ($g=1,\dots,G$) for the associated record.
\subsection{Data}
\code{N <- 30 \\
J <- 2 \#\#\# Number of predictors (including intercept) \\
G <- 2 \#\#\# Number of Multilevel Groups \\
X <- matrix(rnorm(N,0,1),N,J); X[,1] <- 1 \\
Sigma <- matrix(runif(J*J,-1,1),J,J) \\
diag(Sigma) <- runif(J,1,5) \\
gamma <- runif(J,-1,1) \\
beta <- matrix(NA,G,J) \\
for (g in 1:G) \{beta[g,] <- rmvn(1, gamma, Sigma)\} \\
m <- round(runif(N,0.5,(G+0.49))) \#\#\# Multilevel group indicator \\
y <- rep(NA,N) \\
for (i in 1:N) \{y[i] = sum(beta[m[i],] * X[i,]) + rnorm(1,0,0.1)\} \\
S <- diag(J) \\
mon.names <- c("LP","tau") \\
parm.names <- parm.names(list(beta=matrix(0,G,J), log.tau=0, \\
\hspace*{0.27 in} gamma=rep(0,J), Omega=diag(P)), uppertri=c(0,0,0,1)) \\
MyData <- list(G=G, J=J, N=N, S=S, X=X, m=m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y)
}
\subsection{Initial.Values}
\code{Initial.Values <- c(rep(0,G*J), log(1), rep(0,J), \\
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)])}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} gamma.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} gamma.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[1:(Data$G * Data$J)], Data$G, Data$J) \\
\hspace*{0.27 in} gamma <- parm[min(grep("gamma", Data$parm.names)):max(grep( \\
\hspace*{0.62 in} "gamma", Data$parm.names))] \\
\hspace*{0.27 in} tau <- exp(parm[grep("log.tau", Data$parm.names)]) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$J, Data$J) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[min(grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)): max(grep("Omega", Data$parm.names))] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$J, Data$S, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- rep(0,Data$G) \\
\hspace*{0.27 in} for (g in 1:Data$G) \{ \\
\hspace*{0.62 in} beta.prior[g] <- dmvn(beta[g,], gamma, Sigma, log=TRUE)\} \\
\hspace*{0.27 in} gamma.prior <- dnorm(gamma, gamma.mu, 1/sqrt(gamma.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$y \\
\hspace*{0.27 in} for (i in 1:Data$N) \{mu[i] <- sum(beta[Data$m[i],] * Data$X[i,])\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + Omega.prior + sum(beta.prior) + sum(gamma.prior) + \\
\hspace*{0.62 in} tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,tau), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression with Full Missingness} \label{linear.reg.full.miss}
With `full missingness', there are missing values for both the response and at least one predictor. This is a minimal example, since there are missing values in only one of the predictors. Initial values do not need to be specified for missing values in a predictor, unless another predictor variable with missing values is used to predict the missing values of a predictor. More effort is involved in specifying a model with a missing predictor that is predicted by another missing predictor. The full likelihood approach to full missingness is excellent as long as the model is identifiable. When it is not identifiable, then imputation may be done in a previous stage. In this example, \code{X[,2]} is the only predictor with missing values.
\subsection{Form}
$$y \sim \mathcal{N}(\mu_2, \tau^{-1}_2)$$
$$\mu_2 = \textbf{X}\beta$$
$$X_{1:N,2} \sim \mathcal{N}(\mu_1, \tau^{-1}_1)$$
$$\mu_1 = \textbf{X}_{1:N,(1,3:J)}\alpha$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J-1$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau_k \sim \Gamma(0.001, 0.001), \quad k=1,\dots,2$$
\subsection{Data}
\code{N <- 1000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J) \\
X[,1] <- 1 \\
alpha <- runif((J-1),-2,2) \\
X[,2] <- alpha \%*\% t(X[,-2]) + rnorm(N,0,0.1) \\
beta <- runif(J,-2,2) \\
y <- as.vector(beta \%*\% t(X) + rnorm(N,0,0.1)) \\
y[sample(1:N, round(N*0.05))] <- NA \\
X[sample(1:N, round(N*0.05)),2] <- NA \\
mon.names <- c("LP","tau[1]","tau[2]") \\
parm.names <- parm.names(list(alpha=rep(0,J-1), beta=rep(0,J), \\
\hspace*{0.27 in} log.tau=rep(0,2))) \\
MyData <- list(J=J, N=N, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)), rep(0,J), rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- rep(0,(Data$J-1)) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3,(Data$J-1)) \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- rep(1.0E-3,2) \\
\hspace*{0.27 in} tau.beta <- rep(1.0E-3,2) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:(Data$J-1)] \\
\hspace*{0.27 in} beta <- parm[Data$J:(2*Data$J - 1)] \\
\hspace*{0.27 in} tau <- exp(parm[(2*Data$J):(2*Data$J+1)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu1 <- alpha \%*\% t(Data$X[,-2]) \\
\hspace*{0.27 in} X.imputed <- Data$X \\
\hspace*{0.27 in} X.imputed[,2] <- ifelse(is.na(Data$X[,2]), mu1, Data$X[,2]) \\
\hspace*{0.27 in} LL1 <- sum(dnorm(X.imputed[,2], mu1, 1/sqrt(tau[1]), log=TRUE)) \\
\hspace*{0.27 in} mu2 <- beta \%*\% t(X.imputed) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), mu2, Data$y) \\
\hspace*{0.27 in} LL2 <- sum(dnorm(y.imputed, mu2, 1/sqrt(tau[2]), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL1 + LL2 + sum(alpha.prior) + sum(beta.prior) + sum(tau.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL2, Monitor=c(LP,tau), \\
\hspace*{0.62 in} yhat=mu2, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression with Missing Response} \label{linear.reg.miss.resp}
Initial values do not need to be specified for missing values in this response, $y$. Instead, at each iteration, missing values in $y$ are replaced with their estimate in $\mu$.
\subsection{Form}
$$y \sim \mathcal{N}(\mu, \tau^{-1})$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- NCOL(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
y[sample(1:N, round(N*0.05))] <- NA \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","tau") \\
parm.names <- parm.names(list(beta=rep(0,J), log.tau=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), mu, Data$y) \\
\hspace*{0.27 in} LL <- sum(dnorm(y.imputed, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,tau), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Logit} \label{mnl}
\subsection{Form}
$$y_i \sim \mathrm{Cat}(p_{i,1:J}), \quad i=1,\dots,N$$
$$p_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}, \quad \sum^J_{j=1} p_{i,j} = 1$$
$$\phi = \exp(\mu)$$
$$\mu_{i,J} = 0, \quad i=1,\dots,N$$
$$\mu_{i,j} = \textbf{X}_{i,1:K} \beta_{j,1:K}, \quad j=1,\dots,(J-1)$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- "LP" \\
parm.names <- c("beta[1,1]","beta[1,2]","beta[1,3]","beta[2,1]", \\
\hspace*{0.27 in} "beta[2,2]","beta[2,3]") \#\#\# Parameter Names [J,K] \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,(Data$J-1)*Data$K) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,(Data$J-1)*Data$K) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} mu <- Y <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} mu[,1] <- beta[1:3] \%*\% t(Data$X) \\
\hspace*{0.27 in} mu[,2] <- beta[4:6] \%*\% t(Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / apply(phi,1,sum) \\
\hspace*{0.27 in} for (j in 1:Data$J) \{Y[,j] <- ifelse(Data$y == j, 1, 0)\} \\
\hspace*{0.27 in} LL <- sum(Y * log(p)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=p, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Logit, Nested} \label{nmnl}
\subsection{Form}
$$y_i \sim \mathrm{Cat}(P_{i,1:J}), \quad i=1,\dots,N$$
$$P_{1:N,1} = \frac{R}{R + \exp(\alpha I)}$$
$$P_{1:N,2} = \frac{(1 - P_{1:N,1}) S_{1:N,1}}{V}$$
$$P_{1:N,3} = \frac{(1 - P_{1:N,1}) S_{1:N,2}}{V}$$
$$R_{1:N} = \exp(\mu_{1:N,1})$$
$$S_{1:N,1:2} = \exp(\mu_{1:N,2:3})$$
$$I = \log(V)$$
$$V_i = \displaystyle\sum^K_{k=1} S_{i,k}, \quad i=1,\dots,N$$
$$\mu_{1:N,1} = \textbf{X} \iota$$
$$\mu_{1:N,2} = \textbf{X} \beta_{2,1:K}$$
$$\iota = \alpha \beta_{1,1:K}$$
$$\alpha \sim \mathrm{Exp}(1) \quad T[0,2]$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1) \quad k=1,\dots,K$$
where there are $J=3$ categories of $y$, $K=3$ predictors, $R$ is the non-nested alternative, $S$ is the nested alternative, $V$ is the observed utility in the nest, $\alpha$ is effectively 1 - correlation and has a truncated exponential distribution, and $\iota$ is a vector of regression effects for the isolated alternative after $\alpha$ is taken into account. The third alternative is the reference category.
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- c("LP",parm.names(list(iota=rep(0,K)))) \\
parm.names <- parm.names(list(alpha=0, beta=matrix(0,J-1,K))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, rep(0.1,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.rate <- 1 \\
\hspace*{0.27 in} beta.mu <- 0; beta.tau <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1],0,2); parm[1] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dtrunc(alpha, "exp", a=0, b=2, rate=alpha.rate, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- P <- Y <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} iota <- alpha * beta[1,] \\
\hspace*{0.27 in} mu[,1] <- iota \%*\% t(Data$X) \\
\hspace*{0.27 in} mu[,2] <- beta[2,] \%*\% t(Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} R <- exp(mu[,1]) \\
\hspace*{0.27 in} S <- exp(mu[,2:3]) \\
\hspace*{0.27 in} V <- apply(S,1,sum) \\
\hspace*{0.27 in} I <- log(V) \\
\hspace*{0.27 in} P[,1] <- R / (R + exp(alpha*I)) \\
\hspace*{0.27 in} P[,2] <- (1 - P[,1]) * S[,1] / V \\
\hspace*{0.27 in} P[,3] <- (1 - P[,1]) * S[,2] / V \\
\hspace*{0.27 in} for (j in 1:Data$J) \{Y[,j] <- ifelse(Data$y == j, 1, 0)\} \\
\hspace*{0.27 in} LL <- sum(Y * log(P)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,iota), yhat=P, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Normal, Multilevel} \label{norm.ml}
This is Gelman's school example \citep{gelman04}.  Note that \pkg{LaplacesDemon} is much slower to converge compared to this example that uses the \pkg{R2WinBUGS} package \citep{r:r2winbugs}, an \proglang{R} package on CRAN.  However, also note that Laplace's Demon (eventually) provides a better answer (higher ESS, lower DIC, etc.).

\subsection{Form}
$$y_j \sim \mathcal{N}(\theta_j, \tau^{-1}_j), \quad j=1,\dots,J$$
$$\theta_j \sim \mathcal{N}(\theta_{\mu}, \theta_\tau^{-1}), \quad j=1,\dots,J$$
$$\theta_{\mu} \sim \mathcal{N}(0, 1000)$$
$$\theta_{\tau} = \frac{1}{\theta^2_\sigma}$$
$$\sigma \sim \mathrm{U}(1.0E-100, 100)$$
$$\tau_j =\sigma^{-2}_j, \quad j=1,\dots,J$$
\subsection{Data}
\code{J <- 8 \\
y <- c(28.4, 7.9, -2.8, 6.8, -0.6, 0.6, 18.0, 12.2) \\
sd <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6) \\
mon.names <- c("LP","theta.tau") \\
parm.names <- parm.names(list(theta=rep(0,J), theta.mu=0, sigma=0)) \\
MyData <- list(J=J, mon.names=mon.names, parm.names=parm.names, sd=sd, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0, 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} theta.mu.mu <- 0 \\
\hspace*{0.27 in} theta.mu.tau <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} theta.mu <- parm[Data$J+1] \\
\hspace*{0.27 in} sigma <- interval(parm[grep("sigma", Data$parm.names)], 1.0E-100, 100) \\
\hspace*{0.27 in} parm[grep("sigma", Data$parm.names)] <- sigma \\
\hspace*{0.27 in} theta.tau <- 1 / sigma\textasciicircum 2 \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[1:Data$J]; tau <- 1/(Data$sd*Data$sd) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.mu.prior <- dnorm(theta.mu, theta.mu.mu, \\ 
\hspace*{0.62 in} 1/sqrt(theta.mu.tau), log=TRUE) \\ 
\hspace*{0.27 in} sigma.prior <- dunif(sigma, 1.0E-100, 100, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- sum(dgamma(tau, tau.alpha, tau.beta, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, theta.mu, 1/sqrt(theta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, theta, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.mu.prior + sigma.prior + theta.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, theta.tau), \\
\hspace*{0.62 in} yhat=theta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Panel, Autoregressive Poisson} \label{panel.ap}
\subsection{Form}
$$\textbf{Y} \sim \mathrm{Pois}(\Lambda)$$
$$\Lambda_{1:N,1} = \exp(\alpha + \beta x)$$
$$\Lambda_{1:N,t} = \exp(\alpha + \beta x + \rho \textbf{Y}_{1:N,t-1}), \quad t=2,\dots,T$$
$$\alpha_i \sim \mathcal{N}_N(\alpha_\mu, \Sigma), \quad i=1,\dots,N$$
$$\alpha_\mu \sim \mathcal{N}(0, 1000)$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathrm{W}(N,\textbf{S}), \quad \textbf{S} = \textbf{I}_N$$
$$\beta \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{N <- 10 \\
T <- 5 \\
alpha <- runif(N,0,1) \\
rho <- 0.2 \\
beta <- 0.1 \\
x <- runif(N,0,1) \\
Y <- matrix(NA,N,T) \\
Y[,1] <- exp(alpha + beta*x) \\
for (t in 2:T) \{Y[,t] <- exp(alpha + beta*x + rho*Y[,t-1])\} \\
Y <- round(Y) \\
S <- diag(N) \\
mon.names <- c("LP") \\
parm.names <- parm.names(list(alpha=rep(0,N), alpha.mu=0, Omega=diag(N), \\
\hspace*{0.27 in} beta=0, rho=0), uppertri=c(0,0,1,0,0)) \\
MyData <- list(N=N, S=S, T=T, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, x=x) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), 0, S[upper.tri(S, diag=TRUE)], 0, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu.mu <- 0 \\
\hspace*{0.27 in} alpha.mu.tau <- 1.0E-3 \\
\hspace*{0.27 in} alpha.mu <- parm[Data$N+1] \\
\hspace*{0.27 in} beta.mu <- 0; beta.tau <- 1.0E-3 \\
\hspace*{0.27 in} rho.mu <- 0; rho.tau <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$N] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} rho <- parm[grep("rho", Data$parm.names)] \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$N, Data$N) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[min(grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)):max(grep("Omega", Data$parm.names))] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.mu.prior <- dnorm(alpha.mu, alpha.mu.mu, 1/sqrt(alpha.mu.tau), \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- dmvn(alpha, rep(alpha.mu, Data$N), Sigma, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} rho.prior <- dnorm(rho, rho.mu, 1/sqrt(rho.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- Data$Y \\
\hspace*{0.27 in} Lambda[,1] <- exp(alpha + beta*x) \\
\hspace*{0.27 in} Lambda[,2:Data$T] <- exp(alpha + beta*Data$x + \\
\hspace*{0.62 in} rho*Data$Y[,1:(Data$T-1)]) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$Y, Lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + alpha.mu.prior + Omega.prior + rho.prior + \\
\hspace*{0.62 in} beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=Lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Poisson Regression} \label{poisson.reg}
\subsection{Form}
$$y \sim \mathrm{Pois}(\lambda)$$
$$\lambda = \exp(\textbf{X}\beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- as.vector(round(exp(beta \%*\% t(X)))) \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(beta \%*\% t(Data$X)) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Seemingly Unrelated Regression (SUR)} \label{sur}
The following data was used by \citet{zellner62} when introducing the Seemingly Unrelated Regression methodology.
\subsection{Form}
$$Y_{t,k} \sim \mathcal{N}_K(\mu_{t,k}, \Sigma), \quad t=1,\dots,T; \quad k=1,\dots,K$$
$$\mu_{1,t} = \alpha_1 + \alpha_2 \textbf{X}_{t,1} + \alpha_3 \textbf{X}_{t,2}, \quad t=1,\dots,T$$
$$\mu_{2,t} = \beta_1 + \beta_2 \textbf{X}_{t,3} + \beta_3 \textbf{X}_{t,4}, \quad t=1,\dots,T$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathrm{W}(K, \textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where J=3, K=2, and T=20.
\subsection{Data}
\code{T <- 20 \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
Y <- matrix(c(IG,IW),T,2) \\
S <- diag(NCOL(Y)) \\
mon.names <- c("LP","Sigma[1,1]","Sigma[2,1]","Sigma[1,2]","Sigma[2,2]") \\
parm.names <- parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} Omega=diag(2)), uppertri=c(0,0,1)) \\
MyData <- list(S=S, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, VW=VW, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), S[upper.tri(S, diag=TRUE)])}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- rep(0,3) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3,3) \\
\hspace*{0.27 in} beta.mu <- rep(0,3) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,3) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:3] \\
\hspace*{0.27 in} beta <- parm[4:6] \\
\hspace*{0.27 in} Omega <- matrix(parm[c(7,8,8,9)], NROW(Data$S), NROW(Data$S)) \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, NROW(Data$S), Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$T,2) \\
\hspace*{0.27 in} mu[,1] <- alpha[1] + alpha[2]*Data$CG + alpha[3]*Data$VG \\
\hspace*{0.27 in} mu[,2] <- beta[1] + beta[2]*Data$CW + beta[3]*Data$VW \\
\hspace*{0.27 in} LL <- rep(0, Data$T) \\
\hspace*{0.27 in} for (t in 1:Data$T) \{ \\
\hspace*{0.62 in} LL[t] <- sum(dmvn(Data$Y[t,], mu[t,], Sigma, log=TRUE))\} \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- sum(LL) + sum(alpha.prior) + sum(beta.prior) + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*sum(LL), \\
\hspace*{0.62 in} Monitor=c(LP, as.vector(Sigma)), yhat=mu, parm=parm)  \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Variable Selection} \label{variable.selection}
This example uses a modified form of the random-effects (or global adaptation) Stochastic Search Variable Selection (SSVS) algorithm presented in \citet{ohara09}. Here, SSVS is applied to linear regression, though this method is widely applicable. For $J$ variables, each regression effects vector $\beta_j$ is conditional on $\gamma_j$, a binary inclusion variable. Each $\beta_j$ is a discrete mixture distribution with respect to $\gamma_j = 0$ or $\gamma_j = 1$, with precision 100 or $\beta_\tau$, respectively. As with other representations of SSVS, these precisions may require tuning.

With other representations of SSVS, each $\gamma_j$ is Bernoulli-distributed, though this would be problematic in Laplace's Demon, because $\gamma_j$ would be in the list of parameters (rather than monitors), and would not be stationary due to switching behavior. To keep $\gamma$ in the monitors, an uninformative normal density is placed on each prior $\delta_j$, with mean $1/J$ for $J$ variables and precision $1.0E-3$. Each $\delta_j$ is transformed with the inverse logit and rounded to $\gamma_j$. Note that $\lfloor x + 0.5 \rfloor$ means to round $x$. The prior for $\delta$ can be manipulated to influence sparseness.

When the goal is to select the best model, each $\textbf{X}_{1:N,j}$ is retained for a future run when the posterior mean of $\gamma_j \ge 0.5$. When the goal is model-averaging, the results of this model may be used directly.

\subsection{Form}
$$y \sim \mathcal{N}(\mu, \tau^{-1})$$
$$\mu = \textbf{X} \beta$$
$$(\beta_j | \gamma_j) \sim (1 - \gamma_j)\mathcal{N}(0, 100) + \gamma_j \mathcal{N}(0, \beta^{-1}_\tau) \quad j=1,\dots,J$$
$$\beta_\tau = \frac{1}{\sigma^2}$$
$$\sigma \sim \mathrm{U}(1.0E-100, 100)$$
$$\gamma_j = \lfloor \frac{1}{1 + \exp(-\delta_j)} + 0.5 \rfloor, \quad j=1,...,J$$
$$\delta_j \sim \mathcal{N}(0, 10), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- NCOL(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", "beta.tau", "tau", parm.names(list(gamma=rep(0,J)))) \\
parm.names <- parm.names(list(beta=rep(0,J), delta=rep(0,J), sigma=0, \\
\hspace*{0.27 in} log.tau=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J), 1, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperhyperparameters \\
\hspace*{0.27 in} delta.mu <- logit(1/Data$J) \\
\hspace*{0.27 in} delta.tau <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.mu <- rep(0, Data$J) \\
\hspace*{0.27 in} beta.sigma <- interval(parm[grep("sigma", Data$parm.names)], 1.0E-100, 100) \\
\hspace*{0.27 in} parm[grep("sigma", Data$parm.names)] <- beta.sigma
\hspace*{0.27 in} beta.tau <- 1 / beta.sigma\textasciicircum 2 \\
\hspace*{0.27 in} delta <- parm[(Data$J+1):(2*Data$J)] \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} gamma <- round(invlogit(delta)) \\
\hspace*{0.27 in} beta.tau <- ifelse(gamma == 0, 100, beta.tau) \\
\hspace*{0.27 in} tau <- exp(parm[grep("log.tau", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE)) \\
\hspace*{0.27 in} beta.sigma.prior <- dunif(beta.sigma, 1.0E-100, 100, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- sum(dnorm(delta, delta.mu, 1/sqrt(delta.tau), \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + beta.sigma.prior + delta.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, min(beta.tau), \\
\hspace*{0.62 in} tau, gamma), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Zero-Inflated Poisson (ZIP)} \label{zip}
\subsection{Form}
$$y \sim \mathrm{Pois}(\Lambda_{1:N,2})$$
$$z \sim \mathrm{Bern}(\Lambda_{1:N,1})$$
$$z_i = 1 \quad \mathrm{when} \quad y_i = 0, \quad \mathrm{else} \quad z_i = 0, \quad i=1,\dots,N$$
$$\Lambda_{i,2} = 0 \quad \mathrm{if} \quad \Lambda_{i,1} \ge 0.5, \quad i=1,\dots,N$$
$$\Lambda_{1:N,1} = \frac{1}{1 + \exp(-\textbf{X}_1 \alpha)}$$
$$\Lambda_{1:N,2} = \exp(\textbf{X}_2 \beta)$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_1$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_2$$
\subsection{Data}
\code{N <- 1000 \\
J1 <- 4 \\
J2 <- 3 \\
X1 <- matrix(runif(N*J1,-2,2),N,J1); X1[,1] <- 1 \\
X2 <- matrix(runif(N*J2,-2,2),N,J2); X2[,1] <- 1 \\
alpha <- runif(J1,-1,1) \\
beta <- runif(J2,-1,1) \\
p <- as.vector(invlogit(alpha \%*\% t(X1) + rnorm(N,0,0.1))) \\
mu <- as.vector(round(exp(beta \%*\% t(X2) + rnorm(N,0,0.1)))) \\
y <- ifelse(p > 0.5, 0, mu) \\
z <- ifelse(y == 0, 1, 0) \\
mon.names <- "LP" \\
parm.names <- parm.names(list(alpha=rep(0,J1), beta=rep(0,J2))) \\
MyData <- list(J1=J1, J2=J2, N=N, X1=X1, X2=X2, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y, z=z)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J1+J2)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- rep(0, Data$J1) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3, Data$J1) \\
\hspace*{0.27 in} beta.mu <- rep(0, Data$J2) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3, Data$J2) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$J1] \\
\hspace*{0.27 in} beta <- parm[min(grep("beta", Data$parm.names)):max(grep( \\
\hspace*{0.62 in} "beta", Data$parm.names))] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- matrix(NA, Data$N, 2) \\
\hspace*{0.27 in} Lambda[,1] <- invlogit(alpha \%*\% t(Data$X1)) \\
\hspace*{0.27 in} Lambda[,2] <- exp(beta \%*\% t(Data$X2)) \\
\hspace*{0.27 in} Lambda[,2] <- ifelse(Lambda[,1] >= 0.5, 0, Lambda[,2]) \\
\hspace*{0.27 in} LL1 <- sum(dbern(Data$z, Lambda[,1], log=TRUE)) \\
\hspace*{0.27 in} LL2 <- sum(dpois(Data$y, Lambda[,2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL1 + LL2 + sum(alpha.prior) + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL2, Monitor=LP, \\
\hspace*{0.62 in} yhat=Lambda[,2], parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\bibliography{References.bib}

\end{document}
