\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Examples}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Byron Hall\\STATISTICAT, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon} Examples}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Byron Hall} %% comma-separated
\Plaintitle{LaplacesDemon Examples} %% without formatting
\Shorttitle{Examples} %% a short title (if necessary)

\Abstract{The \pkg{LaplacesDemon} package in \proglang{R} enables Bayesian inference with any Bayesian model, provided the user specifies the likelihood.  This vignette is a compendium of examples of how to specify different model forms.
}
\Keywords{Bayesian, Bayesian Inference, Laplace's Demon, LaplacesDemon, R, 
STATISTICAT}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
  Byron Hall\\
  STATISTICAT, LLC\\
  Farmington, CT\\
  E-mail: \email{laplacesdemon@statisticat.com}\\
  URL: \url{http://www.statisticat.com/laplacesdemon.html}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
\pkg{LaplacesDemon} \citep{r:laplacesdemon}, usually referred to as Laplace's Demon, is an \proglang{R} package that is available on CRAN \citep{rdct:r}. A formal introduction to Laplace's Demon is provided in an accompanying vignette entitled ``\pkg{LaplacesDemon} Tutorial'', and an introduction to Bayesian inference is provided in the ``Bayesian Inference'' vignette.

The purpose of this document is to provide users of the \pkg{LaplacesDemon} package with examples of a variety of Bayesian methods. To conserve space, the examples are not worked out in detail, and only the minimum of necessary materials is provided for using the various methodologies. Necessary materials include the form expressed in notation, data (which is often simulated), initial values, and the \code{Model} function.

Notation in this vignette follows these standards: Greek letters represent parameters, lower case letters represent indices, lower case bold face letters represent scalars or vectors, probability distributions are represented with calligraphic font, upper case letters represent index limits, and upper case bold face letters represent matrices.

This vignette will grow over time as examples of more methods become included. Contributed examples are welcome. Please send contributed examples in a similar format in an email to \email{laplacesdemon@statisticat.com} for review and testing. All accepted contributions are, of course, credited.

\begin{center} \Large{\textbf{Contents}} \end{center}
\begin{itemize}
\item ANCOVA \ref{ancova}
\item ANOVA, One-Way \ref{anova.one.way}
\item ANOVA, Two-Way \ref{anova.two.way}
\item ARCH-M(1,1) \ref{archm}
\item Autoregression, AR(1) \ref{ar1}
\item Autoregressive Conditional Heteroskedasticity, ARCH(1,1) \ref{arch11}
\item Autoregressive Moving Average, ARMA(1,1) \ref{arma11}
\item Beta Regression \ref{betareg}
\item Binary Logit \ref{binary.logit}
\item Binary Probit \ref{binary.probit}
\item Binomial Logit \ref{binomial.logit}
\item Binomial Probit \ref{binomial.probit}
\item Cluster Analysis \ref{cluster.analysis}
\item Conditional Autoregression (CAR), Poisson \ref{car.poisson}
\item Contingency Table \ref{contingency.table}
\item Discrete Choice, Conditional Logit \ref{conditional.logit}
\item Discrete Choice, Mixed Logit \ref{mixed.logit}
\item Discrete Choice, Multinomial Probit \ref{dc.mnp}
\item Dynamic Linear Model (DLM) \ref{dlm}
\item Exponential Smoothing \ref{exp.smo}
\item Factor Analysis, Confirmatory (CFA) \ref{cfa}
\item Factor Analysis, Exploratory (EFA) \ref{efa}
\item GARCH(1,1) \ref{garch}
\item GARCH-M(1,1) \ref{garchm}
\item Geographically Weighted Regression \ref{gwr}
\item Kriging \ref{kriging}
\item Laplace Regression \ref{laplace.reg}
\item Linear Regression \ref{linear.reg}
\item Linear Regression, Frequentist \ref{linear.reg.freq}
\item Linear Regression, Multilevel \ref{linear.reg.ml}
\item Linear Regression with Full Missingness \ref{linear.reg.full.miss}
\item Linear Regression with Missing Response \ref{linear.reg.miss.resp}
\item MANCOVA \ref{mancova}
\item MANOVA \ref{manova}
\item Mixture Model, Finite \ref{fmm}
\item Model Averaging \ref{variable.selection}
\item Multinomial Logit \ref{mnl}
\item Multinomial Logit, Nested \ref{nmnl}
\item Multinomial Probit \ref{mnp}
\item Normal, Multilevel \ref{norm.ml}
\item Panel, Autoregressive Poisson \ref{panel.ap}
\item Poisson Regression \ref{poisson.reg}
\item Revision, Normal \ref{revision.normal}
\item Robust Regression \ref{robust.reg}
\item Seemingly Unrelated Regression (SUR) \ref{sur}
\item Simultaneous Equations \ref{simultaneous}
\item Space-Time, Nonseparable \ref{spacetime.nonsep}
\item Space-Time, Separable \ref{spacetime.sep}
\item Survival Analysis \ref{survival}
\item T-test \ref{anova.one.way}
\item Variable Selection \ref{variable.selection}
\item Vector Autoregression, VAR(1) \ref{var1}
\item Zero-Inflated Poisson (ZIP) \ref{zip}
\end{itemize}

\section{ANCOVA} \label{ancova}
This example is essentially the same as the two-way ANOVA (see section \ref{anova.two.way}), except that a covariate $\textbf{X}_{,3}$ has been added, and its parameter is $\delta$.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}] + \delta \textbf{X}_{i,2}, \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,(K-1)$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- matrix(cbind(round(runif(N,0.5,J+0.49)),round(runif(N,0.5,K+0.49)), \\
\hspace*{0.27 in} runif(N,-2,2)), N, 3) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
gamma <- runif(K,-2,2) \\
gamma[J] <- -sum(gamma[1:(K-1)]) \\
delta <- runif(1,-2,2) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + delta*X[,3] + rnorm(N,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","sigma[1]","sigma[2]","sigma[3]", \\
\hspace*{0.27 in} "s.beta","s.gamma","s.epsilon") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} delta=0, log.sigma=rep(0,3))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), 0, rep(log(1),3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- rep(NA,Data$K) \\
\hspace*{0.27 in} gamma[1:(Data$K-1)] <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma[K] <- -sum(gamma[1:(Data$K-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} delta <- parm[grep("delta", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- dnorm(delta, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] + \\
\hspace*{0.62 in} delta*Data$X[,3] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], sigma, s.beta, s.gamma, s.epsilon), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ANOVA, One-Way} \label{anova.one.way}
When $J=2$, this is a Bayesian form of a t-test.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{x}_i], \quad i=1,\dots,N$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \displaystyle\sum^{J-1}_{j=1} \beta_j$$
$$\sigma_{1:2} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \\
x <- round(runif(N, 0.5, J+0.49)) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
y <- rep(NA, N) \\
for (i in 1:N) \{y[i] <- alpha + beta[x[i]] + rnorm(1,0,0.2)\} \\
mon.names <- c("LP","beta[1]","sigma[1]","sigma[2]") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J-1), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
MyData <- list(J=J, N=N, mon.names=mon.names, parm.names=parm.names, x=x, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(log(1),2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$x] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,beta[Data$J], \\
\hspace*{0.62 in} sigma), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ANOVA, Two-Way} \label{anova.two.way}
In this representation, $\sigma^m$ are the superpopulation variance components, \code{s.beta} and \code{s.gamma} are the finite-population within-variance components of the factors or treatments, and \code{s.epsilon} is the finite-population between-variance component.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{N}(\mu_i, \sigma^2_1)$$
$$\mu_i = \alpha + \beta[\textbf{X}_{i,1}] + \gamma[\textbf{X}_{i,2}], \quad i=1,\dots,N$$
$$\epsilon_i = \textbf{y}_i - \mu_i$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \sigma^2_2), \quad j=1,\dots,(J-1)$$
$$\beta_J = - \sum^{J-1}_{j=1} \beta_j$$
$$\gamma_k \sim \mathcal{N}(0, \sigma^2_3), \quad k=1,\dots,(K-1)$$
$$\gamma_K = - \sum^{K-1}_{k=1} \gamma_k$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,3$$
\subsection{Data}
\code{N <- 100 \\
J <- 5 \#Number of levels in factor (treatment) 1 \\
K <- 3 \#Number of levels in factor (treatment) 2 \\
X <- matrix(cbind(round(runif(N, 0.5, J+0.49)),round(runif(N,0.5,K+0.49))), \\
\hspace*{0.27 in} N, 2) \\
alpha <- runif(1,-1,1) \\
beta <- runif(J,-2,2) \\
beta[J] <- -sum(beta[1:(J-1)]) \\
gamma <- runif(K,-2,2) \\
gamma[J] <- -sum(gamma[1:(K-1)]) \\
y <- alpha + beta[X[,1]] + gamma[X[,2]] + rnorm(1,0,0.1) \\
mon.names <- c("LP","beta[5]","gamma[3]","sigma[1]","sigma[2]","sigma[3]", \\
\hspace*{0.27 in} "s.beta","s.gamma","s.epsilon") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J-1), gamma=rep(0,K-1), \\
\hspace*{0.27 in} log.sigma=rep(0,3))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,(J-1)), rep(0,(K-1)), rep(log(1),3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- rep(NA,Data$J) \\
\hspace*{0.27 in} beta[1:(Data$J-1)] <- parm[2:Data$J] \\
\hspace*{0.27 in} beta[J] <- -sum(beta[1:(Data$J-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} gamma <- rep(NA,Data$K) \\
\hspace*{0.27 in} gamma[1:(Data$K-1)] <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma[K] <- -sum(gamma[1:(Data$K-1)]) \#Sum-to-zero constraint \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[3], log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta[Data$X[,1]] + gamma[Data$X[,2]] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components \\
\hspace*{0.27 in} s.beta <- sd(beta) \\
\hspace*{0.27 in} s.gamma <- sd(gamma) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta[Data$J], \\
\hspace*{0.62 in} gamma[Data$K], sigma, s.beta, s.gamma, s.epsilon), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{ARCH-M(1,1)} \label{archm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\delta \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1}$$
$$\theta_k = \frac{1}{1 + \exp(-\theta_k)}, \quad k=1,\dots,2$$
$$\theta_k \sim \mathcal{N}(0, 1000) \in [-10,10], \quad k=1,\dots,2$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","logit.theta[1]","logit.theta[2]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0.5,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; delta <- parm[3] \\
\hspace*{0.27 in} theta <- invlogit(interval(parm[grep("logit.theta", \\
\hspace*{0.62 in} Data$parm.names)], -10, 10)) \\
\hspace*{0.27 in} parm[grep("logit.theta", Data$parm.names)] <- logit(theta) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnorm(delta, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(theta[1], theta[1] + theta[2]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T] \\
\hspace*{0.27 in} sigma2.new <- theta[1] + theta[2]*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sqrt(sigma2), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + theta.prior + \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregression, AR(1)} \label{ar1}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \mu_{T+1}$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew") \\
parm.names <- c("alpha","phi","log.sigma") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; sigma <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregressive Conditional Heteroskedasticity, ARCH(1,1)} \label{arch11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1},$$
$$\theta_1 \sim \mathcal{N}(0, 1000) \in [0,\infty]$$
$$\theta_2 \sim \mathcal{U}(1.0E-100, 1)$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","logit.theta[1]","logit.theta[2]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0.5,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2] \\
\hspace*{0.27 in} theta <- invlogit(interval(parm[grep("logit.theta", \\
\hspace*{0.62 in} Data$parm.names)], -10, 10)) \\
\hspace*{0.27 in} parm[grep("logit.theta", Data$parm.names)] <- logit(theta) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(theta[1], theta[1] + theta[2]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2.new <- theta[1] + theta[2]*epsilon[Data$T]\textasciicircum 2 \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sqrt(sigma2), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, \\
\hspace*{0.62 in} sigma2.new), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Autoregressive Moving Average, ARMA(1,1)} \label{arma11}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2), \quad t=1,\dots,T$$
$$\textbf{y}^{new} = \alpha + \phi \textbf{y}_T + \theta \epsilon_T$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \theta \epsilon_{t-1}$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\theta \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "sigma", "ynew") \\
parm.names <- c("alpha","phi","sigma","theta") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), 0, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; theta <- parm[3] \\
\hspace*{0.27 in} sigma <- exp(parm[4]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnorm(theta, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} mu <- c(mu[1], mu[-1] + theta * epsilon[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + theta*epsilon[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + sigma.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma, ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Beta Regression} \label{betareg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BETA}(a,b)$$
$$a = \mu \phi$$
$$b = (1 - \mu) \phi$$
$$\mu = \Phi(\beta_1 + \beta_2 \textbf{x})$$
$$\beta_j \sim \mathcal{N}(0, 10), \quad j=1,\dots,J$$
$$\phi \sim \mathcal{G}(1,1)$$
where $\Phi$ is the normal CDF.
\subsection{Data}
\code{N <- 10 \\
x <- runif(N) \\
y <- qbeta(0.5, pnorm(2-3*x)*4, (1-pnorm(2-3*x))*4) \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]","log.phi") \\
MyData <- list(x=x, y=y, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(0.01))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:2]; phi <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(10), log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dgamma(phi, 1, 1, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- pnorm(beta[1] + beta[2]*Data$x) \\
\hspace*{0.27 in} a <- mu * phi \\
\hspace*{0.27 in} b <- (1-mu) * phi \\
\hspace*{0.27 in} LL <- sum(dbeta(Data$y, a, b, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Logit} \label{binary.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\eta)$$
$$\eta = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} eta <- invlogit(mu) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(eta >= (sum(Data$y)/length(Data$y)),1,0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Probit} \label{binary.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BERN}(\textbf{p})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \textbf{X} \beta \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where $\phi$ is the inverse CDF, and $J$=3.
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- ifelse(p >= (sum(Data$y)/length(Data$y)),1,0) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Binomial Logit} \label{binomial.logit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \frac{1}{1 + \exp(-\mu)}$$
$$\mu = \beta_1 + \beta_2 \textbf{x}$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} p <- invlogit(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- p * Data$n \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Binomial Probit} \label{binomial.probit}
\subsection{Form}
$$\textbf{y} \sim \mathcal{BIN}(\textbf{p}, \textbf{n})$$
$$\textbf{p} = \phi(\mu)$$
$$\mu = \beta_1 + \beta_2 \textbf{x} \in [-10,10]$$
$$\beta_j \sim \mathcal{N}(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the inverse CDF, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- interval(mu, -10, 10) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- p * Data$n \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Cluster Analysis} \label{cluster.analysis}
This is a parametric model-based cluster analysis, also called a finite mixture model or latent class cluster analysis.
\subsection{Form}
$$\textbf{Y}_{i,j} \sim \mathcal{N}(\mu_{\theta[i],j}, \sigma^2_{\theta[i]}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\theta_i = \mathrm{Max}(\textbf{p}_{i,1:C})$$
$$\textbf{p}_{i,c} = \frac{\delta_{i,c}}{\sum^C_{c=1} \delta_{i,c}}$$
$$\pi_{1:C} \sim \mathcal{D}(\alpha_{1:C})$$
$$\pi_c = \frac{\sum^N_{i=1} \delta_{i,c}}{\sum \delta}$$
$$\alpha_c = 1$$
$$\delta_{i,C} = 1$$
$$\delta_{i,c} \sim \mathcal{N}(\log(\frac{1}{C}), 1000) \in [\exp(-10),\exp(10)], \quad c=1,\dots,(C-1)$$
$$\mu_{c,j} \sim \mathcal{N}(0, \nu^2_j)$$
$$\sigma_c \sim \mathcal{HC}(25)$$
$$\nu_j \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 3 \#Number of clusters \\
alpha <- rep(1,C) \#Prior probability of cluster proportion \\
\# Create a Y matrix \\
n <- 100; N <- 15 \#Full sample; model sample \\
J <- 5 \#Number of predictor variables \\
cluster <- round(runif(n,0.5,C+0.49)) \\
centers <- matrix(runif(C*J, 0, 10), C, J) \\
Y.Full <- matrix(0, n, J) \\
for (i in 1:n) \{for (j in 1:J) \\
\hspace*{0.27 in} \{Y.Full[i,j] <- rnorm(1,centers[cluster[i],j],1)\}\} \\
mean.temp <- colMeans(Y.Full) \\
sigma.temp <- apply(Y.Full,2,sd) \\
centers.cs <- (centers - matrix(rep(mean.temp,C), C, J, byrow=TRUE)) / \\
\hspace*{0.27 in} (2 * matrix(rep(sigma.temp,C), C, J, byrow=TRUE)) \\
for (j in 1:J) \{Y.Full[,j] <- scale(Y.Full[,j],2)\} \\
\#summary(Y.Full) \\
MySample <- sample(1:n, N) \\
Y <- Y.Full[MySample,] \\
mon.names <- c("LP", parm.names(list(nu=rep(0,J), pi=rep(0,C), \\
\hspace*{0.27 in} sigma=rep(0,C), theta=rep(0,N)))) \\
parm.names <- parm.names(list(log.delta=matrix(0,N,C-1), mu=matrix(0,C,J), \\
\hspace*{0.27 in} log.nu=rep(0,J), log.sigma=rep(0,C))) \\
MyData <- list(C=C, J=J, N=N, Y=Y, alpha=alpha, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(N*(C-1),-1,1), rep(0,C*J), rep(0,J), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$C) \\
\hspace*{0.27 in} mu <- matrix(parm[grep("mu", Data$parm.names)], Data$C, Data$J) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu",Data$parm.names)]) \\
\hspace*{0.27 in} pi <- colSums(delta) / sum(delta) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$C), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} mu.prior <- sum(dnorm(mu, 0, matrix(rep(nu,Data$C), Data$C, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} theta <- apply(p,1,which.max) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu[theta,], sigma[theta], log=TRUE)) \\
\hspace*{0.27 in} Yrep <- mu[theta,] \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + delta.prior + mu.prior + nu.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,nu,pi,sigma,theta), \\
\hspace*{0.62 in} yhat=Yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Conditional Autoregression (CAR), Poisson} \label{car.poisson}
This CAR example is a slightly modified form of example 7.3 (Model A) in \citet{congdon03}. The Scottish lip cancer data also appears in the WinBUGS \citep{spiegelhalter03} examples and is a widely analyzed example. The data $\textbf{y}$ consists of counts for $i=1,\dots,56$ counties in Scotland. A single predictor $\textbf{x}$ is provided. The errors, $\epsilon$, are allowed to include spatial effects as smoothing by spatial effects from areal neighbors. Interactions $\textbf{w}$ between counties are in terms of dummy indicators for contiguity (areal neighbors). The list of $NN$ areal neighbors is in the $adj$ variable, and cumulative positions are in variable $C$. The vector $\epsilon_\mu$ is the mean of each area's error, and is a weighted average of errors in contiguous areas.
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\log(\textbf{E}) + \beta_1 + \beta_2 \textbf{x} + \epsilon)$$
$$\epsilon \sim \mathcal{N}(\epsilon_\mu, \sigma^2)$$
$$\epsilon_{\mu[i]} = \rho \sum^J_{j=1} \textbf{w}_{i,j} \epsilon_j, \quad i=1,\dots,N$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\rho \sim \mathcal{U}(-1,1)$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 56 \#Number of areas \\
NN <- 264 \#Number of adjacent areas \\
y <- c(9,39,11,9,15,8,26,7,6,20,13,5,3,8,17,9,2,7,9,7,16,31,11,7,19,15,7, \\
\hspace*{0.27 in} 10,16,11,5,3,7,8,11,9,11,8,6,4,10,8,2,6,19,3,2,3,28,6,1,1,1,1,0,0) \\
E <- c( 1.4,8.7,3.0,2.5,4.3,2.4,8.1,2.3,2.0,6.6,4.4,1.8,1.1,3.3,7.8,4.6, \\
\hspace*{0.27 in} 1.1,4.2,5.5,4.4,10.5,22.7,8.8,5.6,15.5,12.5,6.0,9.0,14.4,10.2,4.8, \\
\hspace*{0.27 in} 2.9,7.0,8.5,12.3,10.1,12.7,9.4,7.2,5.3,18.8,15.8,4.3,14.6,50.7,8.2, \\
\hspace*{0.27 in} 5.6,9.3,88.7,19.6,3.4,3.6,5.7,7.0,4.2,1.8) \#Expected \\
x <- c(16,16,10,24,10,24,10,7,7,16,7,16,10,24,7,16,10,7,7,10,7,16,10,7,1,1, \\
\hspace*{0.27 in} 7,7,10,10,7,24,10,7,7,0,10,1,16,0,1,16,16,0,1,7,1,1,0,1,1,0,1,1,16,10) \\
adj <- c(5,9,11,19, \#Area 1 is adjacent to areas 5, 9, 11, and 19 \\
\hspace*{0.64 in} 7,10, \#Area 2 is adjacent to areas 7 and 10 \\
\hspace*{0.64 in} 6,12, \\
\hspace*{0.64 in} 18,20,28, \\
\hspace*{0.64 in} 1,11,12,13,19, \\
\hspace*{0.64 in} 3,8, \\
\hspace*{0.64 in} 2,10,13,16,17, \\
\hspace*{0.64 in} 6, \\
\hspace*{0.64 in} 1,11,17,19,23,29, \\
\hspace*{0.64 in} 2,7,16,22, \\
\hspace*{0.64 in} 1,5,9,12, \\
\hspace*{0.64 in} 3,5,11, \\
\hspace*{0.64 in} 5,7,17,19, \\
\hspace*{0.64 in} 31,32,35, \\
\hspace*{0.64 in} 25,29,50, \\
\hspace*{0.64 in} 7,10,17,21,22,29, \\
\hspace*{0.64 in} 7,9,13,16,19,29, \\
\hspace*{0.64 in} 4,20,28,33,55,56, \\
\hspace*{0.64 in} 1,5,9,13,17, \\
\hspace*{0.64 in} 4,18,55, \\
\hspace*{0.64 in} 16,29,50, \\
\hspace*{0.64 in} 10,16, \\
\hspace*{0.64 in} 9,29,34,36,37,39, \\
\hspace*{0.64 in} 27,30,31,44,47,48,55,56, \\
\hspace*{0.64 in} 15,26,29, \\
\hspace*{0.64 in} 25,29,42,43, \\
\hspace*{0.64 in} 24,31,32,55, \\
\hspace*{0.64 in} 4,18,33,45, \\
\hspace*{0.64 in} 9,15,16,17,21,23,25,26,34,43,50, \\
\hspace*{0.64 in} 24,38,42,44,45,56, \\
\hspace*{0.64 in} 14,24,27,32,35,46,47, \\
\hspace*{0.64 in} 14,27,31,35, \\
\hspace*{0.64 in} 18,28,45,56, \\
\hspace*{0.64 in} 23,29,39,40,42,43,51,52,54, \\
\hspace*{0.64 in} 14,31,32,37,46, \\
\hspace*{0.64 in} 23,37,39,41, \\
\hspace*{0.64 in} 23,35,36,41,46, \\
\hspace*{0.64 in} 30,42,44,49,51,54, \\
\hspace*{0.64 in} 23,34,36,40,41, \\
\hspace*{0.64 in} 34,39,41,49,52, \\
\hspace*{0.64 in} 36,37,39,40,46,49,53, \\
\hspace*{0.64 in} 26,30,34,38,43,51, \\
\hspace*{0.64 in} 26,29,34,42, \\
\hspace*{0.64 in} 24,30,38,48,49, \\
\hspace*{0.64 in} 28,30,33,56, \\
\hspace*{0.64 in} 31,35,37,41,47,53, \\
\hspace*{0.64 in} 24,31,46,48,49,53, \\
\hspace*{0.64 in} 24,44,47,49, \\
\hspace*{0.64 in} 38,40,41,44,47,48,52,53,54, \\
\hspace*{0.64 in} 15,21,29, \\
\hspace*{0.64 in} 34,38,42,54, \\
\hspace*{0.64 in} 34,40,49,54, \\
\hspace*{0.64 in} 41,46,47,49, \\
\hspace*{0.64 in} 34,38,49,51,52, \\
\hspace*{0.64 in} 18,20,24,27,56, \\
\hspace*{0.64 in} 18,24,30,33,45,55) \\
\# C has length N+1 and refers to cumulative position (-1) in the adj \\
\# variable. For example, area 1 begins at 0 (position 1-1), and \\
\# area 2 begins at 4 (position 5-1), etc. \\
C <- c(0,4,6,8,11,16,18,23,24,30,34,38,41,45,48,51,57,63,69,74,77,80,82, \\
\hspace*{0.27 in} 88,96,99,103,107,111,122,128,135,139,143,152,157,161,166,172,177,182, \\
\hspace*{0.27 in} 189,195,199,204,208,214,220,224,233,236,240,244,248,253,258,264) \\
mon.names <- c("LP","sigma") \\
parm.names <- parm.names(list(beta=rep(0,2), epsilon=rep(0,N), rho=0, \\
\hspace*{0.27 in} log.sigma=0)) \\
MyData <- list(C=C, E=E, N=N, NN=NN, adj=adj, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, x=x, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0,N), 0, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:2] \\
\hspace*{0.27 in} epsilon <- parm[grep("epsilon", Data$parm.names)] \\
\hspace*{0.27 in} rho <- interval(parm[grep("rho", Data$parm.names)], -1, 1) \\
\hspace*{0.27 in} parm[grep("rho", Data$parm.names)] <- rho \\
\hspace*{0.27 in} w <- epsilon[Data$adj] \\
\hspace*{0.27 in} epsilon.mu <- epsilon \\
\hspace*{0.27 in} for (i in 1:N) \{ \\
\hspace*{0.62 in} epsilon.mu[i] <- rho * sum(w[(Data$C[i]+1):(Data$C[i+1])])\} \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} epsilon.prior <- sum(dnorm(epsilon, epsilon.mu, sigma, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} rho.prior <- dunif(rho, -1, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(log(Data$E) + beta[1] + beta[2]*Data$x/10 + epsilon) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + epsilon.prior + rho.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=lambda, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Contingency Table} \label{contingency.table}
The two-way contingency table, matrix $\textbf{Y}$, can easily be extended to more dimensions. For this example, it is vectorized as $y$, and used like an ANOVA data set. Contingency table $\textbf{Y}$ has J rows and K columns. The cell counts are fit with Poisson regression, according to intercept $\alpha$, main effects $\beta_j$ for each row, main effects $\gamma_k$ for each column, and interaction effects $\delta_{j,k}$ for dependence effects. An omnibus (all cells) test of independence is done by estimating two models (one with $\delta$, and one without), and a large enough Bayes Factor indicates a violation of independence when the model with $\delta$ fits better than the model without $\delta$. In an ANOVA-like style, main effects contrasts can be used to distinguish rows or groups of rows from each other, as well as with columns.  Likewise, interaction effects contrasts can be used to test independence in groups of $\delta_{j,k}$ elements. Finally, single-cell interactions can be used to indicate violations of independence for a given cell, such as when zero is not within its 95\% probability interval. Although a little different, this example is similar to a method presented by \citet{albert97}. 
\subsection{Form}
$$\textbf{Y}_{j,k} \sim \mathcal{P}(\lambda_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\lambda_{j,k} = \exp(\alpha + \beta_j + \gamma_k + \delta_{j,k}), \quad j=1,\dots,J, \quad k=1,\dots,K$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_j \sim \mathcal{N}(0, \beta^2_\sigma), \quad j=1,\dots,J$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_k \sim \mathcal{N}(0, \gamma^2_\sigma), \quad k=1,\dots,K$$
$$\gamma_\sigma \sim \mathcal{HC}(25)$$
$$\delta_{j,k} \sim \mathcal{N}(0, \delta^2_\sigma)$$
$$\delta_\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 4 \#Rows \\
K <- 4 \#Columns \\
Y <- matrix(c(10,20,60,20, 40,30,10,40, 10,10,40,10, 40,50,1,40), J, K, \\
\hspace*{0.27 in} dimnames=list(c("Chrysler","Ford","Foreign","GM"), \\
\hspace*{0.27 in} c("I-4","I-6","V-6","V-8"))) \\
y <- as.vector(Y) \\
N <- length(y) \#Cells \\
r <- rep(1:J, N/J) \\
c <- rep(1,K) \\
for (k in 2:K) \{c <- c(c, rep(k, K))\} \\
mon.names <- c("LP","beta.sigma","gamma.sigma","delta.sigma") \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,J), gamma=rep(0,J), \\
\hspace*{0.27 in} log.b.sigma=0, log.g.sigma=0, log.d.sigma=0, \\
\hspace*{0.27 in} delta=matrix(0,J,K))) \\
MyData <- list(J=J, K=K, N=N, c=c, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, r=r, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0, rep(0,J), rep(0,K), rep(0,3), rep(0,J*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- exp(parm[grep("log.b.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} gamma.sigma <- exp(parm[grep("log.g.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} delta.sigma <- exp(parm[grep("log.d.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} delta <- matrix(parm[grep("delta", Data$parm.names)], \\
\hspace*{0.62 in} Data$J, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} beta.sigma.prior <- dhalfcauchy(beta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, gamma.sigma, log=TRUE)) \\
\hspace*{0.27 in} gamma.sigma.prior <- dhalfcauchy(gamma.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} delta.prior <- sum(dnorm(delta, 0, delta.sigma, log=TRUE)) \\
\hspace*{0.27 in} delta.sigma.prior <- dhalfcauchy(delta.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(alpha + beta[Data$r] + gamma[Data$c] + \\
\hspace*{0.62 in} diag(delta[Data$r,Data$c])) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + beta.sigma.prior + \\
\hspace*{0.62 in} gamma.prior + gamma.sigma.prior + delta.prior + \\
\hspace*{0.62 in} delta.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, beta.sigma, \\
\hspace*{0.62 in} gamma.sigma, delta.sigma), yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Discrete Choice, Conditional Logit} \label{conditional.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C))) \\
MyData <- list(C=C, J=J, K=K, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(tcrossprod(gamma, Data$Z),J),Data$N,Data$J) \\
\hspace*{0.27 in} mu[,1] <- mu[,1] + tcrossprod(beta[1,], Data$X) \\
\hspace*{0.27 in} mu[,2] <- mu[,2] + tcrossprod(beta[2,], Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(p,1,which.max) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Discrete Choice, Mixed Logit} \label{mixed.logit}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}$$
$$\phi = \exp(\mu)$$
$$\mu_{i,j} = \beta_{j,1:K} \textbf{X}_{i,1:K} + \gamma \textbf{Z}_{i,1:C} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\mu_{i,J} = \gamma \textbf{Z}_{i,1:C}$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\gamma_c \sim \mathcal{N}(\zeta_{\mu[c]}, \zeta^2_{\sigma[c]})$$
$$\zeta_{\mu[c]} \sim \mathcal{N}(0, 1000)$$
$$\zeta_{\sigma[c]} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{y <- x01 <- x02 <- z01 <- z02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
z01[1:100] <- 1 \\
z01[101:200] <- 2 \\
z01[201:300] <- 3 \\
z02[1:100] <- 40 \\
z02[101:200] <- 50 \\
z02[201:300] <- 100 \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of individual attributes (including the intercept) \\
C <- 2 \#Number of choice-based attributes (intercept is not included) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \#Design matrix of individual attrib. \\
Z <- matrix(c(z01,z02),N,C) \#Design matrix of choice-based attributes \\
mon.names <- c("LP", parm.names(list(zeta.sigma=rep(0,C)))) \\
parm.names <- parm.names(list(beta=matrix(0,J-1,K), gamma=rep(0,C), \\
\hspace*{0.27 in} zeta.mu=rep(0,C), log.zeta.sigma=rep(0,C))) \\
MyData <- list(C=C, J=J, K=K, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K), rep(0,N*C), rep(0,C), rep(0,C))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} zeta.mu <- parm[grep("zeta.mu", Data$parm.names)] \\
\hspace*{0.27 in} zeta.sigma <- exp(parm[grep("log.zeta.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} zeta.mu.prior <- sum(dnorm(zeta.mu, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} zeta.sigma.prior <- sum(dhalfcauchy(zeta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(rep(rowSums(gamma * Data$Z),J),Data$N,Data$J) \\
\hspace*{0.27 in} mu[,1] <- mu[,1] + tcrossprod(beta[1,], Data$X) \\
\hspace*{0.27 in} mu[,2] <- mu[,2] + tcrossprod(beta[2,], Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(p,1,which.max) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + zeta.mu.prior + zeta.sigma.prior\\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,zeta.sigma.prior), \\
\hspace*{0.62 in} yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Discrete Choice, Multinomial Probit} \label{dc.mnp}
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
 \[\textbf{Z}_{i,j} \in \left\{
 \begin{array}{l l}
  $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
  $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K} + \textbf{W} \gamma[a,1:C]$$
 \[\textbf{a} = \left\{
 \begin{array}{l l}
  $1$ & \quad \mbox{if $\textbf{y}_i < J$}\\
  $2$ \\ \end{array} \right. \]
$$\Sigma \sim \mathcal{IW}(J, \textbf{R}), \quad \textbf{R} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\gamma_{1,1:C} \sim \mathcal{N}(0, 1000)$$
$$\gamma_{2,c} = - \gamma_{1,c}, \quad c=1,\dots,C$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{y <- x1 <- x2 <- w1 <- w2 <- c(1:30) \\
y[1:10] <- 1 \\
y[11:20] <- 2 \\
y[21:30] <- 3 \\
x1[1:10] <- rnorm(10, 25, 2.5) \\
x1[11:20] <- rnorm(10, 40, 4.0) \\
x1[21:30] <- rnorm(10, 35, 3.5) \\
x2[1:10] <- rnorm(10, 2.51, 0.25) \\
x2[11:20] <- rnorm(10, 2.01, 0.20) \\ 
x2[21:30] <- rnorm(10, 2.70, 0.27) \\
w1[1:10] <- 10 \\
w1[11:20] <- 4 \\
w1[21:30] <- 1 \\
w2[1:10] <- 40 \\
w2[11:20] <- 50 \\
w2[21:30] <- 100 \\
N <- length(y) \\
J <- length(unique(y)) \#Number of categories in y \\
K <- 3 \#Number of columns to be in design matrix X \\
R <- diag(J) \\
X <- matrix(c(rep(1,N),x1,x2),N,K) \\
C <- 2 \#Number of choice-based attributes \\
W <- matrix(c(w1,w2),N,C) \#Design matrix of choice-based attributes \\
mon.names <- "LP" \\
sigma.temp <- parm.names(list(Sigma=diag(J)), uppertri=1) \\
parm.names <- c(sigma.temp[2:length(sigma.temp)], \\
\hspace*{0.27 in} parm.names(list(beta=matrix(0,(J-1),K), gamma=rep(0,C), \\
\hspace*{0.27 in} Z=matrix(0,N,J)))) \\
MyData <- list(J=J, K=K, N=N, R=R, W=W, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,length(R[upper.tri(R, diag=TRUE)])-1), \\
\hspace*{0.27 in} rep(0,(J-1)*K), rep(0,C), rep(0,N,J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} beta <- rbind(beta, colSums(beta)*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} gamma <- rbind(gamma, gamma*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} Sigma <- matrix(NA, Data$J, Data$J) \\
\hspace*{0.27 in} Sigma[upper.tri(Sigma, diag=TRUE)] <- c(0, parm[grep("Sigma", \\
\hspace*{0.62 in} Data$parm.names)]) \\
\hspace*{0.27 in} Sigma[lower.tri(Sigma)] <- Sigma[upper.tri(Sigma)] \\
\hspace*{0.27 in} diag(Sigma) <- exp(diag(Sigma)) \\
\hspace*{0.27 in} Z <- matrix(parm[grep("Z", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Sigma.prior <- dinvwishart(Sigma, Data$J, Data$R, log=TRUE) \\
\hspace*{0.27 in} Z.prior <- sum(dnorm(Z, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} mu <- matrix(c(rep(tcrossprod(gamma[1,], Data$W),J), \\
\hspace*{0.62 in} tcrossprod(gamma[2,], Data$W)),Data$N,Data$J) \\
\hspace*{0.27 in} for (j in 1:Data$J) \{mu[,j] <- mu[,j] + tcrossprod(beta[j,], Data$X)\} \\
\hspace*{0.27 in} Y <- indmat(Data$y) \\
\hspace*{0.27 in} Z <- ifelse(Z > 10, 10, Z); Z <- ifelse(\{Y == 0\} \& \{Z > 0\}, 0, Z) \\
\hspace*{0.27 in} Z <- ifelse(Z < -10, -10, Z); Z <- ifelse(\{Y == 1\} \& \{Z < 0\}, 0, Z) \\
\hspace*{0.27 in} parm[grep("Z", Data$parm.names)] <- as.vector(Z) \\
\hspace*{0.27 in} LL <- sum(dmvn(Z, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(Z, 1, which.max) \\
\hspace*{0.27 in} \#eta <- exp(mu) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior + Sigma.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Dynamic Linear Model (DLM)} \label{dlm}
The data is presented so that the time-series is subdivided into three 
sections: modeled ($t=1,\dots,T_m$), one-step ahead forecast ($t=T_m+1$), 
and future forecast [$t=(T_m+2),\dots,T$].
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_V), \quad t=1,\dots,T_m$$
$$\textbf{y}^{new}_t \sim \mathcal{N}(\mu_t, \sigma^2_V), \quad t=(T_m+1),\dots,T$$
$$\mu_t = \alpha + \textbf{x}_t \beta_t, \quad t=1,\dots,T$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\beta_1 \sim \mathcal{N}(0, 1000)$$
$$\beta_t \sim \mathcal{N}(\beta_{t-1}, \sigma^2_W), \quad t=2,\dots,T$$
$$\sigma_V \sim \mathcal{HC}(25)$$
$$\sigma_W \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
beta.orig <- x <- rep(0,T) \\
for (t in 2:T) \{ \\
     beta.orig[t] <- beta.orig[t-1] + rnorm(1,0,0.1) \\
     x[t] <- x[t-1] + rnorm(1,0,0.1)\} \\
y <- 10 + beta.orig*x + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) {mon.names[i] <- paste("mu[",(T.m+i),"]", sep="")} \\
parm.names <- parm.names(list(alpha=0, beta=rep(0,T), log.beta.w.sigma=0, \\
\hspace*{0.27 in} log.v.sigma=0)) \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\ 
\hspace*{0.27 in} x=x, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+3)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2:(Data$T+1)] \\
\hspace*{0.27 in} beta.w.sigma <- exp(parm[Data$T+2]) \\
\hspace*{0.27 in} v.sigma <- exp(parm[Data$T+3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- rep(0,Data$T) \\
\hspace*{0.27 in} beta.prior[1] <- dnorm(beta[1], 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} beta.prior[2:Data$T] <- dnorm(beta[2:Data$T], beta[1:(Data$T-1)], \\
\hspace*{0.62 in} beta.w.sigma, log=TRUE) \\
\hspace*{0.27 in} beta.w.sigma.prior <- dhalfcauchy(beta.w.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} v.sigma.prior <- dhalfcauchy(v.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta*Data$x \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], v.sigma, \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sum(beta.prior) + beta.w.sigma.prior + \\
\hspace*{0.62 in} v.sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Exponential Smoothing} \label{exp.smo}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_t = \alpha \textbf{y}_{t-1} + (1 - \alpha) \mu_{t-1}, \quad t=2,\dots,T$$
$$\alpha \sim \mathcal{U}(0,1)$$
$$\sigma \sim \mathcal{HC}$$
\subsection{Data}
\code{T <- 10 \\
y <- rep(0,T) \\
y[1] <- 0 \\
for (t in 2:T) \{y[t] <- y[t-1] + rnorm(1,0,0.1)\} \\
mon.names <- c("LP", "sigma") \\
parm.names <- c("alpha","log.sigma") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1], 0, 1); parm[1] <- alpha \\
\hspace*{0.27 in} sigma <- exp(parm[2]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 0, 1, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- y \\
\hspace*{0.27 in} mu[-1] <- alpha*Data$y[-1] \\
\hspace*{0.27 in} mu[-1] <- mu[-1] + (1 - alpha) * mu[-Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[-1], mu[-Data$T], sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Factor Analysis, Confirmatory} \label{cfa}
Factor scores are in matrix \textbf{F}, factor loadings for each variable are in vector $\lambda$, and $\textbf{f}$ is a vector that indicates which variable loads on which factor.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu_{i,m} = \alpha_m + \lambda_m \textbf{F}_{i,\textbf{f}[m]}, \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\lambda_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_P$$ 
\subsection{Data}
\code{data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
\hspace*{0.27 in} swiss$Catholic, swiss$Infant.Mortality) \\
M <- NCOL(Y) \#Number of variables \\
N <- NROW(Y) \#Number of records \\
P <- 3 \#Number of factors \\
f <- c(1,3,2,2,1) \#Indicator f for the factor for each variable m \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- c("LP","mu[1,1]") \\
parm.names <- parm.names(list(F=matrix(0,N,P), lambda=rep(0,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.sigma=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, f=f, gamma=gamma, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,M), S[upper.tri(S, diag=TRUE)], \\
\hspace*{0.27 in} rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} lambda <- parm[grep("lambda", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} F <- matrix(parm[grep("F", Data$parm.names)], Data$N, Data$P) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$P, Data$P) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} lambda.prior <- sum(dnorm(lambda, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- sum(dmvn(F, Data$gamma, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} for (m in 1:Data$M) \{mu[,m] <- alpha[m] + lambda[m] * F[,Data$f[m]]\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + lambda.prior + sigma.prior + F.prior + \\
\hspace*{0.62 in} Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,mu[1,1]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Factor Analysis, Exploratory} \label{efa}
Factor scores are in matrix \textbf{F} and factor loadings are in matrix $\Lambda$. Although the calculation for the recommended number of factors to explore $P$ is also provided below \citep{fokoue04}, this example sets $P=3$.
\subsection{Form}
$$\textbf{Y}_{i,m} \sim \mathcal{N}(\mu_{i,m}, \sigma^2_m), \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\mu_{i,m} = \alpha_m + \displaystyle\sum^P_{p=1} \nu_{i,m,p}, \quad i=1,\dots,N, \quad m=1,\dots,M$$
$$\nu_{i,m,p} = \textbf{F}_{i,p} \Lambda_{p,m}, \quad i=1,\dots,N, \quad m=1,\dots,M, \quad p=1,\dots,P$$
$$\textbf{F}_{i,1:P} \sim \mathcal{N}_P(\gamma, \Omega^{-1}), \quad i=1,\dots,N$$
$$\alpha_m \sim \mathcal{N}(0, 1000), \quad m=1,\dots,M$$
$$\gamma_p = 0, \quad p=1,\dots,P$$
$$\Lambda_{p,m} \sim \mathcal{N}(0, 1000), \quad p=1,\dots,P, \quad m=1,\dots,M$$
$$\Omega \sim \mathcal{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_P$$
$$\sigma_m \sim \mathcal{HC}(25), \quad m=1,\dots,M$$
\subsection{Data}
\code{
data(swiss) \\
Y <- cbind(swiss$Agriculture, swiss$Examination, swiss$Education, \\
          swiss$Catholic, swiss$Infant.Mortality) \\
M <- NCOL(Y) \#Number of variables \\
N <- NROW(Y) \#Number of records \\
P <- trunc(0.5*(2*M + 1 - sqrt(8*M + 1))) \#Number of factors to explore \\
P <- 3 \#Number of factors to explore (override for this example) \\
gamma <- rep(0,P) \\
S <- diag(P) \\
mon.names <- c("LP","mu[1,1]") \\
parm.names <- parm.names(list(F=matrix(0,N,P), Lambda=matrix(0,P,M), \\
\hspace*{0.27 in} Omega=diag(P), alpha=rep(0,M), log.sigma=rep(0,M)), \\
\hspace*{0.27 in} uppertri=c(0,0,1,0,0)) \\
MyData <- list(M=M, N=N, P=P, S=S, Y=Y, gamma=gamma, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N*P), rep(0,P*M), S[upper.tri(S, diag=TRUE)], \\
\hspace*{0.27 in} rep(0,M), rep(0,M))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} F <- matrix(parm[grep("F", Data$parm.names)], Data$N, Data$P) \\
\hspace*{0.27 in} Lambda <- matrix(parm[grep("Lambda", Data$parm.names)], \\
\hspace*{0.62 in} Data$P, Data$M) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$P, Data$P) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$N, Data$S, log=TRUE) \\
\hspace*{0.27 in} F.prior <- sum(dmvn(F, Data$gamma, Sigma, log=TRUE)) \\
\hspace*{0.27 in} Lambda.prior <- sum(dnorm(Lambda, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- Data$Y \\
\hspace*{0.27 in} nu <- array(NA, dim=c(Data$N, Data$M, Data$P)) \\
\hspace*{0.27 in} for (p in 1:Data$P) \{nu[, ,p] <- F[,p, drop=FALSE] \%*\% Lambda[p,]\} \\
\hspace*{0.27 in} for (m in 1:Data$M) \{mu[,m] <- alpha[m] + rowSums(nu[,1,])\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sigma.prior + Omega.prior + F.prior + \\
\hspace*{0.62 in} Lambda.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,mu[1,1]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{GARCH(1,1)} \label{garch}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T + \theta_3 \sigma^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1} + \theta_3 \sigma^2_{t-1}$$
$$\theta_k = \frac{1}{1 + \exp(-\theta_k)}, \quad k=1,\dots,3$$
$$\theta_k \sim \mathcal{N}(0, 1000) \in [-10,10], \quad k=1,\dots,3$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","logit.theta[1]","logit.theta[2]", \\
\hspace*{0.27 in} "logit.theta[3]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2] \\
\hspace*{0.27 in} theta <- invlogit(interval(parm[grep("logit.theta", \\
\hspace*{0.62 in} Data$parm.names)], -10, 10)) \\
\hspace*{0.27 in} parm[grep("logit.theta", Data$parm.names)] <- logit(theta) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(theta[1], theta[1] + theta[2]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[3]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- theta[1] + theta[2]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[3]*sigma2[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sqrt(sigma2), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{GARCH-M(1,1)} \label{garchm}
\subsection{Form}
$$\textbf{y}_t \sim \mathcal{N}(\mu_t, \sigma^2_t), \quad t=1,\dots,T$$
$$\textbf{y}^{new} \sim \mathcal{N}(\mu_{T+1}, \sigma^2_{new})$$
$$\mu_t = \alpha + \phi \textbf{y}_{t-1} + \delta \sigma^2_{t-1}, \quad t=1,\dots,(T+1)$$
$$\epsilon_t = \textbf{y}_t - \mu_t$$
$$\alpha \sim \mathcal{N}(0, 1000)$$
$$\phi \sim \mathcal{N}(0, 1000)$$
$$\sigma^2_{new} = \theta_1 + \theta_2 \epsilon^2_T + \theta_3 \sigma^2_T$$
$$\sigma^2_t = \theta_1 + \theta_2 \epsilon^2_{t-1} + \theta_3 \sigma^2_{t-1}$$
$$\theta_k = \frac{1}{1 + \exp(-\theta_k)}, \quad k=1,\dots,3$$
$$\theta_k \sim \mathcal{N}(0, 1000) \in [-10,10], \quad k=1,\dots,3$$
\subsection{Data}
\code{y <- c(0.02, -0.51, -0.30,  1.46, -1.26, -2.15, -0.91, -0.53, -1.91, \\
\hspace*{0.27 in} 2.64,  1.64,  0.15,  1.46,  1.61,  1.96, -2.67, -0.19, -3.28, \\
\hspace*{0.27 in}  1.89,  0.91, -0.71,  0.74, -0.10,  3.20, -0.80, -5.25,  1.03, \\
\hspace*{0.27 in} -0.40, -1.62, -0.80,  0.77,  0.17, -1.39, -1.28,  0.48, -1.02, \\
\hspace*{0.27 in}  0.09, -1.09,  0.86,  0.36,  1.51, -0.02,  0.47,  0.62, -1.36, \\
\hspace*{0.27 in}  1.12,  0.42, -4.39, -0.87,  0.05, -5.41, -7.38, -1.01, -1.70, \\
\hspace*{0.27 in}  0.64,  1.16,  0.87,  0.28, -1.69, -0.29,  0.13, -0.65,  0.83, \\
\hspace*{0.27 in}  0.62,  0.05, -0.14,  0.01, -0.36, -0.32, -0.80, -0.06,  0.24, \\
\hspace*{0.27 in}  0.23, -0.37,  0.00, -0.33,  0.21, -0.10, -0.10, -0.01, -0.40, \\
\hspace*{0.27 in} -0.35,  0.48, -0.28,  0.08,  0.28,  0.23,  0.27, -0.35, -0.19, \\
\hspace*{0.27 in}  0.24,  0.17, -0.02, -0.23,  0.03,  0.02, -0.17,  0.04, -0.39, \\
\hspace*{0.27 in} -0.12,  0.16,  0.17,  0.00,  0.18,  0.06, -0.36,  0.22,  0.14, \\
\hspace*{0.27 in} -0.17,  0.10, -0.01,  0.00, -0.18, -0.02,  0.07, -0.06,  0.06, \\
\hspace*{0.27 in} -0.05, -0.08, -0.07,  0.01, -0.06,  0.01,  0.01, -0.02,  0.01, \\
\hspace*{0.27 in}  0.01,  0.12, -0.03,  0.08, -0.10,  0.01, -0.03, -0.08,  0.04, \\
\hspace*{0.27 in} -0.09, -0.08,  0.01, -0.05,  0.08, -0.14,  0.06, -0.11,  0.09, \\
\hspace*{0.27 in}  0.06, -0.12, -0.01, -0.05, -0.15, -0.05, -0.03,  0.04,  0.00, \\
\hspace*{0.27 in} -0.12,  0.04, -0.06, -0.05, -0.07, -0.05, -0.14, -0.05, -0.01, \\
\hspace*{0.27 in} -0.12,  0.05,  0.06, -0.10,  0.00,  0.01,  0.00, -0.08,  0.00, \\
\hspace*{0.27 in}  0.00,  0.07, -0.01,  0.00,  0.09,  0.33,  0.13,  0.42,  0.24, \\
\hspace*{0.27 in} -0.36,  0.22, -0.09, -0.19, -0.10, -0.08, -0.07,  0.05,  0.07, \\
\hspace*{0.27 in}  0.07,  0.00, -0.04, -0.05,  0.03,  0.08,  0.26,  0.10,  0.08, \\
\hspace*{0.27 in}  0.09, -0.07, -0.33,  0.17, -0.03,  0.07, -0.04, -0.06, -0.06, \\
\hspace*{0.27 in}  0.07, -0.03,  0.00,  0.08,  0.27,  0.11,  0.11,  0.06, -0.11, \\
\hspace*{0.27 in} -0.09, -0.21,  0.24, -0.12,  0.11, -0.02, -0.03,  0.02, -0.10, \\
\hspace*{0.27 in}  0.00, -0.04,  0.01,  0.02, -0.03, -0.10, -0.09,  0.17,  0.07, \\
\hspace*{0.27 in} -0.05, -0.01, -0.05,  0.01,  0.00, -0.08, -0.05, -0.08,  0.07, \\
\hspace*{0.27 in}  0.06, -0.14,  0.02,  0.01,  0.04,  0.00, -0.13, -0.17) \\
T <- length(y) \\
mon.names <- c("LP", "ynew", "sigma2.new") \\
parm.names <- c("alpha","phi","delta","logit.theta[1]","logit.theta[2]", \\
\hspace*{0.27 in} "logit.theta[3]") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; delta <- parm[3] \\
\hspace*{0.27 in} theta <- invlogit(interval(parm[grep("logit.theta", \\
\hspace*{0.62 in} Data$parm.names)], -10, 10)) \\
\hspace*{0.27 in} parm[grep("logit.theta", Data$parm.names)] <- logit(theta) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} delta.prior <- dnorm(delta, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- c(alpha, alpha + phi*Data$y[-Data$T]) \\
\hspace*{0.27 in} epsilon <- Data$y - mu \\
\hspace*{0.27 in} sigma2 <- c(theta[1], theta[1] + theta[2]*epsilon[-Data$T]\textasciicircum 2) \\
\hspace*{0.27 in} sigma2[-1] <- sigma2[-1] + theta[3]*sigma2[-Data$T] \\
\hspace*{0.27 in} sigma2.new <- theta[1] + theta[2]*epsilon[Data$T]\textasciicircum 2 + \\
\hspace*{0.62 in} theta[3]*sigma2[Data$T] \\
\hspace*{0.27 in} mu <- mu + delta*sigma2 \\
\hspace*{0.27 in} ynew <- alpha + phi*Data$y[Data$T] + delta*sigma2[Data$T] \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sqrt(sigma2), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + delta.prior + theta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, ynew, sigma2.new), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Geographically Weighted Regression} \label{gwr}
\subsection{Form}
$$\textbf{y}_{i,k} \sim \mathcal{N}(\mu_{i,k}, \tau^{-1}_{i,k}), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\mu_{i,1:N} = \textbf{X} \beta_{i,1:J}$$
$$\tau = \frac{1}{\sigma^2} \textbf{w} \nu$$
$$\textbf{w} = \frac{\exp(-0.5 \textbf{Z}^2)}{\textbf{h}}$$
$$\alpha \sim \mathcal{U}(1.5, 100)$$
$$\beta_{i,j} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,N, \quad j=1,\dots,J$$
$$\textbf{h} \sim \mathcal{N}(0.1, 1000) \in [0.1, \infty]$$
$$\nu_{i,k} \sim \mathcal{G}(\alpha, 2), \quad i=1,\dots,N, \quad k=1,\dots,N$$
$$\sigma_i \sim \mathcal{HC}(25), \quad i=1,\dots,N$$
\subsection{Data}
\code{crime <-   c(18.802, 32.388, 38.426,  0.178, 15.726, 30.627, 50.732, \\
\hspace*{0.27 in} 26.067, 48.585, 34.001, 36.869, 20.049, 19.146, 18.905, 27.823, \\
\hspace*{0.27 in} 16.241,  0.224, 30.516, 33.705, 40.970, 52.794, 41.968, 39.175, \\
\hspace*{0.27 in} 53.711, 25.962, 22.541, 26.645, 29.028, 36.664, 42.445, 56.920, \\
\hspace*{0.27 in} 61.299, 60.750, 68.892, 38.298, 54.839, 56.706, 62.275, 46.716, \\
\hspace*{0.27 in} 57.066, 54.522, 43.962, 40.074, 23.974, 17.677, 14.306, 19.101, \\
\hspace*{0.27 in} 16.531, 16.492) \\
income <-  c(21.232,  4.477, 11.337,  8.438, 19.531, 15.956, 11.252, \\
\hspace*{0.27 in} 16.029,  9.873, 13.598,  9.798, 21.155, 18.942, 22.207, 18.950, \\
\hspace*{0.27 in} 29.833, 31.070, 17.586, 11.709,  8.085, 10.822,  9.918, 12.814, \\
\hspace*{0.27 in} 11.107, 16.961, 18.796, 11.813, 14.135, 13.380, 17.017,  7.856, \\
\hspace*{0.27 in}  8.461,  8.681, 13.906, 14.236,  7.625, 10.048,  7.467,  9.549, \\
\hspace*{0.27 in}  9.963, 11.618, 13.185, 10.655, 14.948, 16.940, 18.739, 18.477, \\
\hspace*{0.27 in} 18.324, 25.873) \\
housing <- c(44.567, 33.200, 37.125, 75.000, 80.467, 26.350, 23.225, \\
\hspace*{0.27 in} 28.750, 18.000, 96.400, 41.750, 47.733, 40.300, 42.100, 42.500, \\
\hspace*{0.27 in} 61.950, 81.267, 52.600, 30.450, 20.300, 34.100, 23.600, 27.000, \\
\hspace*{0.27 in} 22.700, 33.500, 35.800, 26.800, 27.733, 25.700, 43.300, 22.850, \\
\hspace*{0.27 in} 17.900, 32.500, 22.500, 53.200, 18.800, 19.900, 19.700, 41.700, \\
\hspace*{0.27 in} 42.900, 30.600, 60.000, 19.975, 28.450, 31.800, 36.300, 39.600, \\
\hspace*{0.27 in} 76.100, 44.333) \\
easting <- c(35.62, 36.50, 36.71, 33.36, 38.80, 39.82, 40.01, 43.75, \\
\hspace*{0.27 in} 39.61, 47.61, 48.58, 49.61, 50.11, 51.24, 50.89, 48.44, 46.73, \\
\hspace*{0.27 in} 43.44, 43.37, 41.13, 43.95, 44.10, 43.70, 41.04, 43.23, 42.67, \\
\hspace*{0.27 in} 41.21, 39.32, 41.09, 38.3,  41.31, 39.36, 39.72, 38.29, 36.60, \\
\hspace*{0.27 in} 37.60, 37.13, 37.85, 35.95, 35.72, 35.76, 36.15, 34.08, 30.32, \\
\hspace*{0.27 in} 27.94, 27.27, 24.25, 25.47, 29.02) \\
northing <- c(42.38, 40.52, 38.71, 38.41, 44.07, 41.18, 38.00, 39.28, \\
\hspace*{0.27 in} 34.91, 36.42, 34.46, 32.65, 29.91, 27.80, 25.24, 27.93, 31.91, \\
\hspace*{0.27 in} 35.92, 33.46, 33.14, 31.61, 30.40, 29.18, 28.78, 27.31, 24.96, \\
\hspace*{0.27 in} 25.90, 25.85, 27.49, 28.82, 30.90, 32.88, 30.64, 30.35, 32.09, \\
\hspace*{0.27 in} 34.08, 36.12, 36.30, 36.40, 35.60, 34.66, 33.92, 30.42, 28.26, \\
\hspace*{0.27 in} 29.85, 28.21, 26.69, 25.71, 26.58) \\
N <- length(crime) \\
J <- 3 \#Number of predictors, including the intercept \\
X <- matrix(c(rep(1,N), income, housing),N,J) \\
D <- as.matrix(dist(cbind(northing,easting), diag=TRUE, upper=TRUE)) \\
Z <- D / sd(as.vector(D)) \\
y <- matrix(0,N,N); for (i in 1:N) \{for (k in 1:N) \{y[i,k] <- crime[k]\}\} \\
mon.names <- c("LP",parm.names(list(LAR2=rep(0,N)))) \\
parm.names <- parm.names(list(alpha=0, beta=matrix(0,N,J), log.h=0, \\
\hspace*{0.27 in} log.nu=matrix(0,N,N), log.sigma=rep(0,N))) \\
MyData <- list(J=J, N=N, X=X, Z=Z, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(1,1.5,100), rep(0,N*J), log(1), rep(0,N*N), \\
\hspace*{0.27 in} log(rep(1,N)))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[grep("alpha", Data$parm.names)], 1.5, 100) \\
\hspace*{0.27 in} parm[grep("alpha", Data$parm.names)] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} h <- exp(parm[grep("log.h", Data$parm.names)]) + 0.1 \\
\hspace*{0.27 in} nu <- exp(matrix(parm[grep("log.nu", Data$parm.names)], \\
\hspace*{0.62 in} Data$N, Data$N)) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dunif(alpha, 1.5, 100, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} h.prior <- dtrunc(h, "norm", a=0.1, b=Inf, mean=0.1, sd=sqrt(1000), \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dgamma(nu, alpha, 2, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} w <- exp(-0.5 * Z\textasciicircum 2) / h \\
\hspace*{0.27 in} tau <- (1/sigma\textasciicircum 2) * w * nu \\
\hspace*{0.27 in} mu <- matrix(NA, Data$N, Data$N) \\
\hspace*{0.27 in} for (i in 1:N) \{mu[i,] <- tcrossprod(beta[i,], Data$X)\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sqrt(1/tau), log=TRUE)) \\
\hspace*{0.27 in} WSE <- w * nu * (Data$y - mu)\textasciicircum 2; w.y <- w * nu * Data$y \\
\hspace*{0.27 in} WMSE <- rowMeans(WSE); y.w <- rowSums(w.y) / rowSums(w) \\
\hspace*{0.27 in} LAR2 <- 1 - WMSE / sd(y.w)\textasciicircum 2 \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + h.prior + nu.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,LAR2), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Kriging} \label{kriging}
This is an example of universal kriging of $\textbf{y}$ given $\textbf{X}$, regression effects $\beta$, and spatial effects $\zeta$. Euclidean distance between spatial coordinates (longitude and latitude) is used for each of $i=1,\dots,N$ records of $\textbf{y}$. An additional record is created from the same data-generating process to compare the accuracy of interpolation. For the spatial component, $\phi$ is the rate of spatial decay and $\kappa$ is the scale. $\kappa$ is often difficult to identify, so it is set to 1 (Gaussian), but may be allowed to vary up to 2 (Exponential). In practice, $\phi$ is also often difficult to identify. While $\Sigma$ is spatial covariance, spatial correlation is $\rho = \exp(-\phi \textbf{D})$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2_1)$$
$$ \mu = \textbf{X} \beta + \zeta$$
$$ \textbf{y}^{new} = \textbf{X} \beta + \sum^N_{i=1} \left ( \frac{\rho_i}{\sum \rho} \zeta_i \right )$$
$$ \rho = \exp(-\phi \textbf{D}^{new})^\kappa$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp(-\phi \textbf{D})^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,2$$
$$ \phi \sim \mathcal{U}(1, 5)$$
$$ \zeta_\mu = 0$$
$$ \kappa = 1$$
\subsection{Data}
\code{N <- 20 \\
longitude <- runif(N+1,0,100) \\
latitude <- runif(N+1,0,100) \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma <- 10000 * exp(-1.5 * D) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,N+1), Sigma), 2, mean)) \\
beta <- c(50,2) \\
X <- matrix(runif((N+1)*2,-2,2),(N+1),2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(beta, X)) \\
y <- mu + zeta \\
longitude.new <- longitude[N+1]; latitude.new <- latitude[N+1] \\
Xnew <- X[N+1,]; ynew <- y[N+1] \\
longitude <- longitude[1:N]; latitude <- latitude[1:N] \\
X <- X[1:N,]; y <- y[1:N] \\
D <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
D.new <- sqrt((longitude - longitude.new)\textasciicircum 2 + (latitude - latitude.new)\textasciicircum 2) \\
mon.names <- c("LP","sigma[1]","sigma[2]","ynew") \\
parm.names <- parm.names(list(zeta=rep(0,N), beta=rep(0,2), \\
\hspace*{0.27 in} log.sigma=rep(0,2), phi=0)) \\
MyData <- list(D=D, D.new=D.new, N=N, X=X, Xnew=Xnew, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), rep(0,2), rep(0,2), 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1 \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-phi * Data$D)\textasciicircum kappa \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, rep(0, Data$N), Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- dunif(phi, 1, 5, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Interpolation \\
\hspace*{0.27 in} rho <- exp(-phi * Data$D.new)\textasciicircum kappa \\
\hspace*{0.27 in} ynew <- sum(beta * Data$Xnew) + sum(rho / sum(rho) * zeta) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) + zeta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + sigma.prior + phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,ynew), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Laplace Regression} \label{laplace.reg}
This linear regression specifies that $\textbf{y}$ is Laplace-distributed, where it is usually Gaussian or normally-distributed.  It has been claimed that it should be surprising that the normal distribution became the standard, when the Laplace distribution usually fits better and has wider tails \citep{kotz01}. Another popular alternative is to use the t-distribution (see Robust Regression in section \ref{robust.reg}), though it is more computationally expensive to estimate, because it has three parameters.  The Laplace distribution has only two parameters, location and scale like the normal distribution, and is computationally easier to fit.  This example could be taken one step further, and the parameter vector $\beta$ could be Laplace-distributed.  Laplace's Demon recommends that users experiment with replacing the normal distribution with the Laplace distribution.
\subsection{Form}
$$\textbf{y} \sim \mathcal{L}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rlaplace(N,0,0.1) \\
y <- as.vector(tcrossprod(beta, X) + e) \\
mon.names <- c("LP", "sigma") \\
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dlaplace(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression} \label{linear.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(tcrossprod(beta, X) + e) \\
mon.names <- c("LP", "sigma") \\
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression, Frequentist} \label{linear.reg.freq}
By eliminating prior probabilities, a frequentist linear regression example is presented. Although frequentism is not endorsed here, the purpose of this example is to illustrate how the \pkg{LaplacesDemon} package can be used for Bayesian or frequentist inference.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(tcrossprod(beta, X) + e) \\
mon.names <- c("LL", "sigma") \\
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} Modelout <- list(LP=LL, Dev=-2*LL, Monitor=c(LL, sigma), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression, Multilevel} \label{linear.reg.ml}
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu_i = \textbf{X} \beta_{\textbf{m}[i],1:J}$$
$$\beta_{g,1:J} \sim \mathcal{N}_J(\gamma, \Sigma), \quad g=1,\dots,G$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathcal{W}(J, \textbf{S}), \quad \textbf{S} = \textbf{I}_J$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
where $\textbf{m}$ is a vector of length $N$, and each element indicates the multilevel group ($g=1,\dots,G$) for the associated record.
\subsection{Data}
\code{N <- 30 \\
J <- 2 \#\#\# Number of predictors (including intercept) \\
G <- 2 \#\#\# Number of Multilevel Groups \\
X <- matrix(rnorm(N,0,1),N,J); X[,1] <- 1 \\
Sigma <- matrix(runif(J*J,-1,1),J,J) \\
diag(Sigma) <- runif(J,1,5) \\
gamma <- runif(J,-1,1) \\
beta <- matrix(NA,G,J) \\
for (g in 1:G) \{beta[g,] <- rmvn(1, gamma, Sigma)\} \\
m <- round(runif(N,0.5,(G+0.49))) \#\#\# Multilevel group indicator \\
y <- rowSums(beta[m,] * X) + rnorm(N,0,0.1) \\
S <- diag(J) \\
mon.names <- c("LP","sigma") \\
parm.names <- parm.names(list(beta=matrix(0,G,J), log.sigma=0, \\
\hspace*{0.27 in} gamma=rep(0,J), Omega=S), uppertri=c(0,0,0,1)) \\
MyData <- list(G=G, J=J, N=N, S=S, X=X, m=m, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial.Values}
\code{Initial.Values <- c(rep(0,G*J), log(1), rep(0,J), \\
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)])}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[1:(Data$G * Data$J)], Data$G, Data$J) \\
\hspace*{0.27 in} gamma <- parm[grep("gamma", Data$parm.names)] \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$J, Data$J) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[min(grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)): max(grep("Omega", Data$parm.names))] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, Data$J, Data$S, log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dmvn(beta, gamma, Sigma, log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sqrt(100), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- rowSums(beta[Data$m,] * Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + Omega.prior + beta.prior + gamma.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Linear Regression with Full Missingness} \label{linear.reg.full.miss}
With `full missingness', there are missing values for both the response and at least one predictor. This is a minimal example, since there are missing values in only one of the predictors. Initial values do not need to be specified for missing values in a predictor, unless another predictor variable with missing values is used to predict the missing values of a predictor. More effort is involved in specifying a model with a missing predictor that is predicted by another missing predictor. The full likelihood approach to full missingness is excellent as long as the model is identifiable. When it is not identifiable, then imputation may be done in a previous stage. In this example, \code{X[,2]} is the only predictor with missing values.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_2, \sigma^2_2)$$
$$\mu_2 = \textbf{X}\beta$$
$$\textbf{X}_{1:N,2} \sim \mathcal{N}(\mu_1, \sigma^2_1)$$
$$\mu_1 = \textbf{X}_{1:N,(1,3:J)}\alpha$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,2$$
\subsection{Data}
\code{N <- 1000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J) \\
X[,1] <- 1 \\
alpha <- runif((J-1),-2,2) \\
X[,2] <- tcrossprod(alpha, X[,-2]) + rnorm(N,0,0.1) \\
beta <- runif(J,-2,2) \\
y <- as.vector(tcrossprod(beta, X) + rnorm(N,0,0.1)) \\
y[sample(1:N, round(N*0.05))] <- NA \\
M <- ifelse(is.na(y), 1, 0) \\
X[sample(1:N, round(N*0.05)),2] <- NA \\
mon.names <- c("LP","sigma[1]","sigma[2]") \\
parm.names <- parm.names(list(alpha=rep(0,J-1), beta=rep(0,J), \\
\hspace*{0.27 in} log.sigma=rep(0,2))) \\
MyData <- list(J=J, M=M, N=N, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)), rep(0,J), rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:(Data$J-1)] \\
\hspace*{0.27 in} beta <- parm[Data$J:(2*Data$J - 1)] \\
\hspace*{0.27 in} sigma <- exp(parm[(2*Data$J):(2*Data$J+1)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu1 <- tcrossprod(alpha, Data$X[,-2]) \\
\hspace*{0.27 in} X.imputed <- Data$X \\
\hspace*{0.27 in} X.imputed[,2] <- ifelse(is.na(Data$X[,2]), mu1, Data$X[,2]) \\
\hspace*{0.27 in} LL1 <- sum(dnorm(X.imputed[,2], mu1, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} mu2 <- tcrossprod(beta, X.imputed) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), mu2, Data$y) \\
\hspace*{0.27 in} LL2 <- sum((1-Data$M) * dnorm(y.imputed, mu2, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL1 + LL2 + alpha.prior + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL2, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu2, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression with Missing Response} \label{linear.reg.miss.resp}
Initial values do not need to be specified for missing values in this response, $\textbf{y}$. Instead, at each iteration, missing values in $\textbf{y}$ are replaced with their estimate in $\mu$.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- NCOL(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
y[sample(1:N, round(N*0.05))] <- NA \\
M <- ifelse(is.na(y), 1, 0) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP","sigma") \\
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0)) \\
MyData <- list(J=J, M=M, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dgamma(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} y.imputed <- ifelse(is.na(Data$y), mu, Data$y) \\
\hspace*{0.27 in} LL <- sum((1-Data$M) * dnorm(y.imputed, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{MANCOVA} \label{mancova}
Since this is a multivariate extension of ANCOVA, please see the ANCOVA example in section \ref{ancova} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]} + \textbf{X}_{1:N,3:(C+J)} \delta_{k,1:C}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\delta_{k,c} \sim \mathcal{N}(0, 1000)$$
$$\Omega \sim \mathcal{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\Sigma = \Omega^{-1}$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{C <- 2 \#Number of covariates \\
J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- matrix(cbind(round(runif(N, 0.5, L+0.49)),round(runif(N,0.5,M+0.49)), \\
\hspace*{0.27 in} runif(C*N,0,1)), N, J + C) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
delta <- matrix(runif(K*C), K, C) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + \\
\hspace*{0.27 in} tcrossprod(delta[k,], X[,-c(1,2)]) + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), delta=matrix(0,K,C), Omega=diag(K), \\
\hspace*{0.27 in} log.sigma=rep(0,2)), uppertri=c(0,0,0,0,1,0)) \\
MyData <- list(C=C, J=J, K=K, L=L, M=M, N=N, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} rep(0,C*K), S[upper.tri(S, diag=TRUE)], rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- matrix(c(parm[grep("beta", Data$parm.names)], rep(0,K)), \\
\hspace*{0.27 in} Data$K, Data$L) \\
\hspace*{0.27 in} beta[,L] <- -rowSums(beta[,-L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[grep("gamma", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,M] <- -rowSums(gamma[,-M]) \\
\hspace*{0.27 in} delta <- matrix(parm[grep("delta", Data$parm.names)], Data$K, Data$C) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$K, Data$K) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dnorm(delta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, NROW(Data$S), Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]] + \\
\hspace*{0.62 in} tcrossprod(delta[k,], Data$X[,-c(1,2)])\} \\
\hspace*{0.27 in} LL <- sum(dmvn(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- sd(t(beta)) \\
\hspace*{0.27 in} s.gamma <- sd(t(gamma)) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$Y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + delta.prior + \\
\hspace*{0.62 in} Omega.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{MANOVA} \label{manova}
Since this is a multivariate extension of ANOVA, please see the two-way ANOVA example in section \ref{anova.two.way} for a univariate introduction.
\subsection{Form}
$$\textbf{Y}_{i,1:J} \sim \mathcal{N}(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
$$\mu_{i,k} = \alpha_k + \beta_{k,\textbf{X}[i,1]} + \gamma_{k,\textbf{X}[i,1]}$$
$$\epsilon_{i,k} = \textbf{Y}_{i,k} - \mu_{i,k}$$
$$\alpha_k \sim \mathcal{N}(0, 1000), \quad k=1,\dots,K$$
$$\beta_{k,l} \sim \mathcal{N}(0, \sigma^2_1), \quad l=1,\dots,(L-1)$$
$$\beta_{1:K,L} = - \sum^{L-1}_{l=1} \beta_{1:K,l}$$
$$\gamma_{k,m} \sim \mathcal{N}(0, \sigma^2_2), \quad m=1,\dots,(M-1)$$
$$\gamma_{1:K,M} = - \sum^{M-1}_{m=1} \beta_{1:K,m}$$
$$\Omega \sim \mathcal{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\Sigma = \Omega^{-1}$$
$$\sigma_{1:J} \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{J <- 2 \#Number of factors (treatments) \\
K <- 3 \#Number of endogenous (dependent) variables \\
L <- 4 \#Number of levels in factor (treatment) 1 \\
M <- 5 \#Number of levels in factor (treatment) 2 \\
N <- 100 \\
X <- matrix(cbind(round(runif(N, 0.5, L+0.49)),round(runif(N,0.5,M+0.49))), \\
\hspace*{0.27 in} N, J) \\
alpha <- runif(K,-1,1) \\
beta <- matrix(runif(K*L,-2,2), K, L) \\
beta[,L] <- -rowSums(beta[,-L]) \\
gamma <- matrix(runif(K*M,-2,2), K, M) \\
gamma[,M] <- -rowSums(gamma[,-M]) \\
Y <- matrix(NA,N,K) \\
for (k in 1:K) \{ \\
\hspace*{0.27 in} Y[,k] <- alpha[k] + beta[k,X[,1]] + gamma[k,X[,2]] + rnorm(1,0,0.1)\} \\
S <- diag(K) \\
mon.names <- c("LP", "s.o.beta", "s.o.gamma", "s.o.epsilon", \\
\hspace*{0.27 in} parm.names(list(s.beta=rep(0,K), s.gamma=rep(0,K), \\
\hspace*{0.27 in} s.epsilon=rep(0,K)))) \\
parm.names <- parm.names(list(alpha=rep(0,K), beta=matrix(0,K,(L-1)), \\
\hspace*{0.27 in} gamma=matrix(0,K,(M-1)), Omega=diag(K), log.sigma=rep(0,2)), \\
\hspace*{0.27 in} uppertri=c(0,0,0,1,0)) \\
MyData <- list(J=J, K=K, L=L, M=M, N=N, S=S, X=X, Y=Y, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,K), rep(0,K*(L-1)), rep(0,K*(M-1)), \\
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)], rep(0,2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[grep("alpha", Data$parm.names)] \\
\hspace*{0.27 in} beta <- matrix(c(parm[grep("beta", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$L) \\
\hspace*{0.27 in} beta[,L] <- -rowSums(beta[,-L]) \\
\hspace*{0.27 in} gamma <- matrix(c(parm[grep("gamma", Data$parm.names)], rep(0,K)), \\
\hspace*{0.62 in} Data$K, Data$M) \\
\hspace*{0.27 in} gamma[,M] <- -rowSums(gamma[,-M]) \\
\hspace*{0.27 in} Omega <- matrix(NA, Data$K, Data$K) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sigma[2], log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, NROW(Data$S), Data$S, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$K) \\
\hspace*{0.27 in} for (k in 1:K) \{ \\
\hspace*{0.62 in} mu[,k] <- alpha[k] + beta[k,Data$X[,1]] + gamma[k,Data$X[,2]]\} \\
\hspace*{0.27 in} LL <- sum(dmvn(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Omnibus \\
\hspace*{0.27 in} s.o.beta <- sd(as.vector(beta)) \\
\hspace*{0.27 in} s.o.gamma <- sd(as.vector(gamma)) \\
\hspace*{0.27 in} s.o.epsilon <- sd(as.vector(Data$Y - mu)) \\
\hspace*{0.27 in} \#\#\# Variance Components, Univariate \\
\hspace*{0.27 in} s.beta <- sd(t(beta)) \\
\hspace*{0.27 in} s.gamma <- sd(t(gamma)) \\
\hspace*{0.27 in} s.epsilon <- sd(Data$Y - mu) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + Omega.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, s.o.beta, s.o.gamma, \\
\hspace*{0.62 in} s.o.epsilon, s.beta, s.gamma, s.epsilon), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Mixture Model, Finite} \label{fmm}
This finite mixture model (FMM) imposes a multilevel structure on each of the $J$ regression effects in $\beta$, so that mixture components share a common residual variance, $\nu_j$. Identifiability is gained at the expense of some shrinkage.
\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu_{1:N,m}, \sigma^2)$$
$$\mu_{1:N,m} = \textbf{X}\beta_{m,1:J}, \quad m=1,\dots,M$$
$$\beta_{m,j} \sim \mathcal{N}(0, \nu^2_j), \quad j=1,\dots,J$$
$$\nu_j \sim \mathcal{HC}(25)$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\pi_{1:M} \sim \mathcal{D}(\alpha_{1:M})$$
$$\pi_m = \frac{\sum^N_{i=1} \delta_{i,m}}{\sum \delta}$$
$$\textbf{p}_{i,m} = \frac{\delta_{i,m}}{\sum^M_{m=1} \delta_{i,m}}$$
$$\delta_{i,m} = \exp(\textbf{X}\delta_{i,m}), \quad m=1,\dots,(M-1)$$
$$\delta_{1:N,M} = 1$$
$$\delta_{i,m} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad m=1,\dots,(M-1)$$
$$\alpha_m = 1$$
\subsection{Data}
\code{M <- 2 \#Number of mixtures \\
alpha <- rep(1,M) \#Prior probability of mixing probabilities \\
data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- NCOL(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", parm.names(list(pi=rep(0,M), sigma=0))) \\
parm.names <- parm.names(list(beta=matrix(0,M,J), log.nu=rep(0,J), \\
\hspace*{0.27 in} log.delta=matrix(0,N,M-1), log.sigma=0)) \\
MyData <- list(J=J, M=M, N=N, X=X, alpha=alpha, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(runif(M*J), rep(0,J), runif(N*(M-1),-1,1), 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$M, Data$J) \\
\hspace*{0.27 in} delta <- interval(parm[grep("log.delta", Data$parm.names)], -10, 10) \\
\hspace*{0.27 in} parm[grep("log.delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} delta <- matrix(c(exp(delta), rep(1, Data$N)), Data$N, Data$M) \\
\hspace*{0.27 in} pi <- colSums(delta) / sum(delta) \\
\hspace*{0.27 in} nu <- exp(parm[grep("log.nu", Data$parm.names)]) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, matrix(rep(nu, Data$M), Data$M, \\
\hspace*{0.62 in} Data$J, byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=exp(-10), b=exp(10), \\
\hspace*{0.62 in} mean=log(1/Data$M), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- ddirichlet(pi, Data$alpha, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- sum(dhalfcauchy(nu, 25, log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} p <- delta / rowSums(delta) \\
\hspace*{0.27 in} LL <- mu <- matrix(NA, Data$N, Data$M) \\
\hspace*{0.27 in} for (m in 1:M) \{mu[,m] <- tcrossprod(beta[m,], Data$X)\} \\
\hspace*{0.27 in} p <- apply(p, 1, which.max) \\
\hspace*{0.27 in} mu <- diag(mu[,p]) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + delta.prior + pi.prior + nu.prior + \\
\hspace*{0.62 in} sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,pi,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Multinomial Logit} \label{mnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{p}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{p}_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}, \quad \sum^J_{j=1} \textbf{p}_{i,j} = 1$$
$$\phi = \exp(\mu)$$
$$\mu_{i,J} = 0, \quad i=1,\dots,N$$
$$\mu_{i,j} = \textbf{X}_{i,1:K} \beta_{j,1:K} \in [-700,700], \quad j=1,\dots,(J-1)$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- "LP" \\
parm.names <- c("beta[1,1]","beta[1,2]","beta[1,3]","beta[2,1]", \\
\hspace*{0.27 in} "beta[2,2]","beta[2,3]") \#\#\# Parameter Names [J,K] \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:({Data$J-1}*Data$K)] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} mu[,1] <- tcrossprod(beta[1:3], Data$X) \\
\hspace*{0.27 in} mu[,2] <- tcrossprod(beta[4:6], Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} phi <- exp(mu) \\
\hspace*{0.27 in} p <- phi / rowSums(phi) \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, p, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(p,1,which.max) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Logit, Nested} \label{nmnl}
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{CAT}(\textbf{P}_{i,1:J}), \quad i=1,\dots,N$$
$$\textbf{P}_{1:N,1} = \frac{\textbf{R}}{\textbf{R} + \exp(\alpha \textbf{I})}$$
$$\textbf{P}_{1:N,2} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,1}}{\textbf{V}}$$
$$\textbf{P}_{1:N,3} = \frac{(1 - \textbf{P}_{1:N,1}) \textbf{S}_{1:N,2}}{\textbf{V}}$$
$$\textbf{R}_{1:N} = \exp(\mu_{1:N,1})$$
$$\textbf{S}_{1:N,1:2} = \exp(\mu_{1:N,2:3})$$
$$\textbf{I} = \log(\textbf{V})$$
$$\textbf{V}_i = \displaystyle\sum^K_{k=1} \textbf{S}_{i,k}, \quad i=1,\dots,N$$
$$\mu_{1:N,1} = \textbf{X} \iota \in [-700,700]$$
$$\mu_{1:N,2} = \textbf{X} \beta_{2,1:K} \in [-700,700]$$
$$\iota = \alpha \beta_{1,1:K}$$
$$\alpha \sim \mathcal{EXP}(1) \in [0,2]$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1) \quad k=1,\dots,K$$
where there are $J=3$ categories of $\textbf{y}$, $K=3$ predictors, $\textbf{R}$ is the non-nested alternative, $\textbf{S}$ is the nested alternative, $\textbf{V}$ is the observed utility in the nest, $\alpha$ is effectively 1 - correlation and has a truncated exponential distribution, and $\iota$ is a vector of regression effects for the isolated alternative after $\alpha$ is taken into account. The third alternative is the reference category.
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- c("LP",parm.names(list(iota=rep(0,K)))) \\
parm.names <- parm.names(list(alpha=0, beta=matrix(0,J-1,K))) \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(0.5, rep(0.1,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.rate <- 1 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- interval(parm[1],0,2); parm[1] <- alpha \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dtrunc(alpha, "exp", a=0, b=2, rate=alpha.rate, \\
\hspace*{0.62 in} log=TRUE) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- P <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} iota <- alpha * beta[1,] \\
\hspace*{0.27 in} mu[,1] <- tcrossprod(iota, Data$X) \\
\hspace*{0.27 in} mu[,2] <- tcrossprod(beta[2,], Data$X) \\
\hspace*{0.27 in} mu <- interval(mu, -700, 700) \\
\hspace*{0.27 in} R <- exp(mu[,1]) \\
\hspace*{0.27 in} S <- exp(mu[,2:3]) \\
\hspace*{0.27 in} V <- rowSums(S) \\
\hspace*{0.27 in} I <- log(V) \\
\hspace*{0.27 in} P[,1] <- R / (R + exp(alpha*I)) \\
\hspace*{0.27 in} P[,2] <- (1 - P[,1]) * S[,1] / V \\
\hspace*{0.27 in} P[,3] <- (1 - P[,1]) * S[,2] / V \\
\hspace*{0.27 in} LL <- sum(dcat(Data$y, P, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(P,1,which.max) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,iota), yhat=yrep, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Probit} \label{mnp}
In this form of MNP, the $\beta$ parameters are sum-to-zero constraints in the reference category, and covariance matrix $\Sigma$ includes all $J$ categories of $\textbf{y}$. \\
\\
Note that the parameters and initial values for the upper triangular elements of $\Sigma$ are read in as $\Sigma$, though the diagonal is read in as $\log(\Sigma)$, but still denoted as $\Sigma$. Apologies for any confusion this causes, and the diagonal elements could each be renamed manually in \code{parm.names}. The only reason this difference exists is that I am unsure of how to program that in \code{parm.names} for all occasions.
\subsection{Form}
$$\textbf{Z}_{i,1:J} \sim \mathcal{N}_J(\mu_{i,1:J}, \Sigma), \quad i=1,\dots,N$$
\[\textbf{Z}_{i,j} \in \left\{
\begin{array}{l l}
 $[0,10]$ & \quad \mbox{if $\textbf{y}_i = j$}\\
 $[-10,0]$ \\ \end{array} \right. \]
$$\mu_{1:N,j} = \textbf{X} \beta_{j,1:K}$$
$$\Sigma \sim \mathcal{IW}(J, \textbf{R}), \quad \textbf{R} = \textbf{I}_J, \quad \Sigma[1,1] = 1$$
$$\beta_{j,k} \sim \mathcal{N}(0, 1000), \quad j=1,\dots,(J-1), \quad k=1,\dots,K$$
$$\beta_{J,k} = - \sum^{J-1}_{j=1} \beta_{j,k}$$
$$\textbf{Z}_{i,j} \sim \mathcal{N}(0, 1000) \in [-10,10]$$
\subsection{Data}
\code{y <- x1 <- x2 <- c(1:30) \\
y[1:10] <- 1 \\
y[11:20] <- 2 \\
y[21:30] <- 3 \\
x1[1:10] <- rnorm(10, 25, 2.5) \\
x1[11:20] <- rnorm(10, 40, 4.0) \\
x1[21:30] <- rnorm(10, 35, 3.5) \\
x2[1:10] <- rnorm(10, 2.51, 0.25) \\
x2[11:20] <- rnorm(10, 2.01, 0.20) \\
x2[21:30] <- rnorm(10, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of columns to be in design matrix X \\
R <- diag(J) \\
X <- matrix(c(rep(1,N),x1,x2),N,K) \\
mon.names <- "LP" \\
sigma.temp <- parm.names(list(Sigma=diag(J)), uppertri=1) \\
parm.names <- c(sigma.temp[2:length(sigma.temp)], \\
\hspace*{0.27 in} parm.names(list(beta=matrix(0,(J-1),K), Z=matrix(0,N,J)))) \\
MyData <- list(J=J, K=K, N=N, R=R, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,length(R[upper.tri(R, diag=TRUE)])-1), \\
\hspace*{0.27 in} rep(0,(J-1)*K), rep(0,N,J)) \\
}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- matrix(parm[grep("beta", Data$parm.names)], Data$J-1, Data$K) \\
\hspace*{0.27 in} beta <- rbind(beta, colSums(beta)*-1) \#Sum to zero constraint \\
\hspace*{0.27 in} Sigma <- matrix(NA, Data$J, Data$J) \\
\hspace*{0.27 in} Sigma[upper.tri(Sigma, diag=TRUE)] <- c(0, parm[grep("Sigma", \\
\hspace*{0.62 in} Data$parm.names)]) \\
\hspace*{0.27 in} Sigma[lower.tri(Sigma)] <- Sigma[upper.tri(Sigma)] \\
\hspace*{0.27 in}diag(Sigma) <- exp(diag(Sigma)) \\
\hspace*{0.27 in} Z <- matrix(parm[grep("Z", Data$parm.names)], Data$N, Data$J) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Sigma.prior <- dinvwishart(Sigma, Data$J, Data$R, log=TRUE) \\
\hspace*{0.27 in} Z.prior <- sum(dnorm(Z, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} for (j in 1:Data$J) \{mu[,j] <- tcrossprod(beta[j,], Data$X)\} \\
\hspace*{0.27 in} Y <- indmat(Data$y) \\
\hspace*{0.27 in} Z <- ifelse(Z > 10, 10, Z); Z <- ifelse(\{Y == 0\} \& \{Z > 0\}, 0, Z) \\
\hspace*{0.27 in} Z <- ifelse(Z < -10, -10, Z); Z <- ifelse(\{Y == 1\} \& \{Z < 0\}, 0, Z) \\
\hspace*{0.27 in} parm[grep("Z", Data$parm.names)] <- as.vector(Z) \\
\hspace*{0.27 in} LL <- sum(dmvn(Z, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} yrep <- apply(Z, 1, which.max) \\
\hspace*{0.27 in} \#eta <- exp(mu) \\
\hspace*{0.27 in} \#p <- eta / rowSums(eta) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Sigma.prior + Z.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yrep, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Normal, Multilevel} \label{norm.ml}
This is Gelman's school example \citep{gelman04}.  Note that \pkg{LaplacesDemon} is much slower to converge compared to this example that uses the \pkg{R2WinBUGS} package \citep{r:r2winbugs}, an \proglang{R} package on CRAN. However, also note that Laplace's Demon (eventually) provides a better answer (higher ESS, lower DIC, etc.).
\subsection{Form}
$$\textbf{y}_j \sim \mathcal{N}(\theta_j, \tau^{-1}_j), \quad j=1,\dots,J$$
$$\theta_j \sim \mathcal{N}(\theta_{\mu}, \theta_\tau^{-1}), \quad j=1,\dots,J$$
$$\theta_{\mu} \sim \mathcal{N}(0, 1000)$$
$$\theta_{\tau} = \frac{1}{\theta^2_\sigma}$$
$$\sigma \sim \mathcal{U}(1.0E-100, 100)$$
$$\tau_j =\sigma^{-2}_j, \quad j=1,\dots,J$$
\subsection{Data}
\code{J <- 8 \\
y <- c(28.4, 7.9, -2.8, 6.8, -0.6, 0.6, 18.0, 12.2) \\
sd <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6) \\
mon.names <- c("LP","theta.tau") \\
parm.names <- parm.names(list(theta=rep(0,J), theta.mu=0, sigma=0)) \\
MyData <- list(J=J, mon.names=mon.names, parm.names=parm.names, sd=sd, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), 0, 1)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} theta.mu <- parm[Data$J+1] \\
\hspace*{0.27 in} sigma <- interval(parm[grep("sigma", Data$parm.names)], 1.0E-100, 100) \\
\hspace*{0.27 in} parm[grep("sigma", Data$parm.names)] <- sigma \\
\hspace*{0.27 in} theta.tau <- 1 / sigma\textasciicircum 2 \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[1:Data$J]; tau <- 1/(Data$sd*Data$sd) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior and Prior Densities) \\
\hspace*{0.27 in} theta.mu.prior <- dnorm(theta.mu, sqrt(1000), log=TRUE) \\ 
\hspace*{0.27 in} sigma.prior <- dunif(sigma, 1.0E-100, 100, log=TRUE) \\
\hspace*{0.27 in} tau.prior <- sum(dgamma(tau, tau.alpha, tau.beta, log=TRUE)) \\
\hspace*{0.27 in} theta.prior <- sum(dnorm(theta, theta.mu, 1/sqrt(theta.tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, theta, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.mu.prior + sigma.prior + theta.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, theta.tau), \\
\hspace*{0.62 in} yhat=theta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Panel, Autoregressive Poisson} \label{panel.ap}
\subsection{Form}
$$\textbf{Y} \sim \mathcal{P}(\Lambda)$$
$$\Lambda_{1:N,1} = \exp(\alpha + \beta \textbf{x})$$
$$\Lambda_{1:N,t} = \exp(\alpha + \beta \textbf{x} + \rho \log(\textbf{Y}_{1:N,t-1})), \quad t=2,\dots,T$$
$$\alpha_i \sim \mathcal{N}(\alpha_\mu, \alpha^2_\sigma), \quad i=1,\dots,N$$
$$\alpha_\mu \sim \mathcal{N}(0, 1000)$$
$$\alpha_\sigma \sim \mathcal{HC}(25)$$
$$\beta \sim \mathcal{N}(0, 1000)$$
$$\rho \sim \mathcal{N}(0, 1000)$$
\subsection{Data}
\code{N <- 10 \\
T <- 10 \\
alpha <- rnorm(N,2,0.5) \\
rho <- 0.5 \\
beta <- 0.5 \\
x <- runif(N,0,1) \\
Y <- matrix(NA,N,T) \\
Y[,1] <- exp(alpha + beta*x) \\
for (t in 2:T) \{Y[,t] <- exp(alpha + beta*x + rho*log(Y[,t-1]))\} \\
Y <- round(Y) \\
mon.names <- c("LP","alpha.sigma") \\
parm.names <- parm.names(list(alpha=rep(0,N), alpha.mu=0, \\
\hspace*{0.27 in} log.alpha.sigma=0, beta=0, rho=0)) \\
MyData <- list(N=N, T=T, Y=Y, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=x) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,N), 0, log(1), 0, 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- parm[Data$N+1] \\
\hspace*{0.27 in} alpha.sigma <- exp(parm[Data$N+2]) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$N] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} rho <- parm[grep("rho", Data$parm.names)] \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior and Prior Densities) \\
\hspace*{0.27 in} alpha.mu.prior <- dnorm(alpha.mu, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} alpha.sigma.prior <- dhalfcauchy(alpha.sigma, 25, log=TRUE) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} rho.prior <- dnorm(rho, 0, sqrt(1000), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- Data$Y \\
\hspace*{0.27 in} Lambda[,1] <- exp(alpha + beta*x) \\
\hspace*{0.27 in} Lambda[,2:Data$T] <- exp(alpha + beta*Data$x + \\
\hspace*{0.62 in} rho*log(Data$Y[,1:(Data$T-1)])) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$Y, Lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + alpha.mu.prior + alpha.sigma.prior + \\
\hspace*{0.62 in} beta.prior + rho.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,alpha.sigma), \\
\hspace*{0.62 in} yhat=Lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Poisson Regression} \label{poisson.reg}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\lambda)$$
$$\lambda = \exp(\textbf{X}\beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- as.vector(round(exp(tcrossprod(beta, X)))) \\
mon.names <- "LP" \\
parm.names <- parm.names(list(beta=rep(0,J))) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(tcrossprod(beta, Data$X)) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Revision, Normal} \label{revision.normal}
This example provides both an analytic solution and numerical approximation of the revision of a normal distribution. Given a normal prior distribution ($\alpha$) and data distribution ($\beta$), the posterior ($\gamma$) is the revised normal distribution. This is an introductory example of Bayesian inference, and allows the user to experiment numerical approximation, such as with MCMC in \code{LaplacesDemon}. Note that, regardless of the data sample size $N$ in this example, Laplace Approximation is inappropriate due to asymptotics since the data ($\beta$) is perceived by the algorithm as a single datum rather than a collection of data. MCMC, on the other hand, is biased only by the effective number of samples taken of the posterior. \\
\code{\#\#\# Analytic Solution \\
prior.mu <- 0 \\
prior.sigma <- 10 \\
N <- 10 \\
data.mu <- 1 \\
data.sigma <- 2 \\
posterior.mu <- (prior.sigma\textasciicircum -2 * prior.mu + N * data.sigma\textasciicircum -2 * data.mu) / \\
\hspace*{0.27 in} (prior.sigma\textasciicircum -2 + N * data.sigma\textasciicircum -2) \\
posterior.sigma <- sqrt(1/(prior.sigma\textasciicircum -2 + data.sigma\textasciicircum -2)) \\
posterior.mu \\
posterior.sigma \\
}
\subsection{Form}
$$\alpha \sim \mathcal{N}(0,10)$$
$$\beta \sim \mathcal{N}(1,2)$$
$$\gamma = \frac{\alpha^{-2}_\sigma \alpha + N \beta^{-2}_\sigma \beta}{\alpha^{-2}_\sigma + N \beta^{-2}_\sigma}$$
\subsection{Data}
\code{N <- 10 \\
mon.names <- c("LP","gamma") \\
parm.names <- c("alpha","beta") \\
MyData <- list(N=N, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(0,0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} alpha.mu <- 0 \\
\hspace*{0.27 in} alpha.sigma <- 10 \\
\hspace*{0.27 in} beta.mu <- 1 \\
\hspace*{0.27 in} beta.sigma <- 2 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2] \\
\hspace*{0.27 in} \#\#\# Log(Prior Density) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, alpha.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood Density \\
\hspace*{0.27 in} LL <- dnorm(beta, beta.mu, beta.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Posterior \\
\hspace*{0.27 in} gamma <- (alpha.sigma\textasciicircum -2 * alpha + N * beta.sigma\textasciicircum -2 * beta) / \\
\hspace*{0.62 in} (alpha.sigma\textasciicircum -2 + N * beta.sigma\textasciicircum -2) \\
\hspace*{0.27 in} \#\#\# Log(Posterior Density) \\
\hspace*{0.27 in} LP <- LL + alpha.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,gamma), yhat=LL, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Robust Regression} \label{robust.reg}
By replacing the normal distribution with the Student t distribution, linear regression is often called robust regression. As an alternative approach to robust regression, consider Laplace regression (see section \ref{laplace.reg}).
\subsection{Form}
$$\textbf{y} \sim \mathrm{t}(\mu, \sigma^2, \nu)$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
$$\nu \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(tcrossprod(beta, X) + e) \\
mon.names <- c("LP", "sigma", "nu") \\
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0, log.nu=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1), log(2))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} sigma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} nu <- exp(parm[Data$J+2]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} nu.prior <- dhalfcauchy(nu, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dst(Data$y, mu, sigma, nu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + sigma.prior + nu.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma,nu), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Seemingly Unrelated Regression (SUR)} \label{sur}
The following data was used by \citet{zellner62} when introducing the Seemingly Unrelated Regression methodology.
\subsection{Form}
$$\textbf{Y}_{t,k} \sim \mathcal{N}_K(\mu_{t,k}, \Sigma), \quad t=1,\dots,T; \quad k=1,\dots,K$$
$$\mu_{1,t} = \alpha_1 + \alpha_2 \textbf{X}_{t,1} + \alpha_3 \textbf{X}_{t,2}, \quad t=1,\dots,T$$
$$\mu_{2,t} = \beta_1 + \beta_2 \textbf{X}_{t,3} + \beta_3 \textbf{X}_{t,4}, \quad t=1,\dots,T$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim \mathcal{W}(K, \textbf{S}), \quad \textbf{S} = \textbf{I}_K$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
where J=3, K=2, and T=20.
\subsection{Data}
\code{T <- 20 \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
Y <- matrix(c(IG,IW),T,2) \\
S <- diag(NCOL(Y)) \\
mon.names <- c("LP","Sigma[1,1]","Sigma[2,1]","Sigma[1,2]","Sigma[2,2]") \\
parm.names <- parm.names(list(alpha=rep(0,3), beta=rep(0,3), \\
\hspace*{0.27 in} Omega=diag(2)), uppertri=c(0,0,1)) \\
MyData <- list(S=S, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, VW=VW, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,3), rep(0,3), S[upper.tri(S, diag=TRUE)])}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:3] \\
\hspace*{0.27 in} beta <- parm[4:6] \\
\hspace*{0.27 in} Omega <- matrix(parm[c(7,8,8,9)], NROW(Data$S), NROW(Data$S)) \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, NROW(Data$S), Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,Data$T,2) \\
\hspace*{0.27 in} mu[,1] <- alpha[1] + alpha[2]*Data$CG + alpha[3]*Data$VG \\
\hspace*{0.27 in} mu[,2] <- beta[1] + beta[2]*Data$CW + beta[3]*Data$VW \\
\hspace*{0.27 in} LL <- sum(dmvn(Data$Y, mu, Sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, \\
\hspace*{0.62 in} Monitor=c(LP, as.vector(Sigma)), yhat=mu, parm=parm)  \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Simultaneous Equations} \label{simultaneous}
This example of simultaneous equations uses Klein's Model I \citep{kleine50} regarding economic fluctations in the United States in 1920-1941 (\textbf{N}=22). Usually, this example is modeled with 3-stage least sqaures (3SLS), excluding the uncertainty from multiple stages. By constraining each element in the instrumental variables matrix $\nu \in [-10,10]$, this example estimates the model without resorting to stages. The dependent variable is matrix \textbf{Y}, in which $\textbf{Y}_{1,1:N}$ is \textbf{C} or Consumption, $\textbf{Y}_{2,1:N}$ is \textbf{I} or Investment, and $\textbf{Y}_{3,1:N}$ is \textbf{Wp} or Private Wages. Here is a data dictionary: \\
\code{\hspace*{0.27 in} A = Time Trend measured as years from 1931 \\
\hspace*{0.27 in} \textbf{C} = Consumption \\
\hspace*{0.27 in} \textbf{G} = Government Nonwage Spending \\
\hspace*{0.27 in} \textbf{I} = Investment \\
\hspace*{0.27 in} \textbf{K} = Capital Stock \\
\hspace*{0.27 in} \textbf{P} = Private (Corporate) Profits \\
\hspace*{0.27 in} \textbf{T} = Indirect Business Taxes Plus Neg Exports \\
\hspace*{0.27 in} \textbf{Wg} = Government Wage Bill \\
\hspace*{0.27 in} \textbf{Wp} = Private Wages \\
\hspace*{0.27 in} \textbf{X} = Equilibrium Demand (GNP) \\
}
See \citet{kleine50} for more information.
\subsection{Form}
$$\textbf{Y} \sim \mathcal{N}(\mu, \Sigma)$$
$$ \mu_{1,1} = \alpha_1 + \alpha_2 \nu_{1,1} + \alpha_4 \nu_{2,1}$$
$$ \mu_{1,i} = \alpha_1 + \alpha_2 \nu_{1,i} + \alpha_3 \textbf{P}_{i-1} + \alpha_4 \nu_{2,i}, \quad i=2,\dots,N$$
$$ \mu_{2,1} = \beta_1 + \beta_2 \nu_{1,1} + \beta_4 \textbf{K}_1$$
$$ \mu_{2,i} = \beta_1 + \beta_2 \nu_{1,i} + \beta_3 \textbf{P}_{i-1} + \beta_4 \textbf{K}_i, \quad i=2,\dots,N$$
$$\mu_{3,1} = \gamma_1 + \gamma_2 \nu_{3,1} + \gamma_4 \textbf{A}_1$$
$$\mu_{3,i} = \gamma_1 + \gamma_2 \nu_{3,i} + \gamma_3 \textbf{X}_{i-1} + \gamma_4 \textbf{A}_i, \quad i=2,\dots,N$$
$$\textbf{Z}_{j,i} \sim \mathcal{N}(\nu_{j,i}, \sigma^2_j), \quad j=1,\dots,3$$
$$\nu_{j,1} = \pi_{j,1} + \pi_{j,3} \textbf{K}_1 + \pi_{j,5} \textbf{A}_1 + \pi_{j,6} \textbf{T}_1 + \pi_{j,7} \textbf{G}_1, \quad j=1,\dots,3$$
$$\nu_{j,i} = \pi_{j,1} + \pi_{j,2} \textbf{P}_{i-1} + \pi_{j,3} \textbf{K}_i + \pi_{j,4} \textbf{X}_{i-1} + \pi_{j,5} \textbf{A}_i + \pi_{j,6} \textbf{T}_i + \pi \textbf{G}_i, \quad i=1,\dots,N, \quad j=1,\dots,3$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\gamma_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,4$$
$$\pi_{j,i} \sim \mathcal{N}(0, 1000) \in [-10,10], \quad j=1,\dots,3, \quad i=1,\dots,N$$
$$\sigma_j \sim \mathcal{HC}(25), \quad j=1,\dots,3$$
$$\Omega \sim \mathcal{W}(N, \textbf{S}), \quad \textbf{S} = \textbf{I}_3$$
$$\Sigma = \Omega^{-1}$$
\subsection{Data}
\code{N <- 22 \\
A <- c(-11,-10,-9,-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8,9,10) \\
C <- c(39.8,41.9,45,49.2,50.6,52.6,55.1,56.2,57.3,57.8,55,50.9,45.6,46.5, \\
\hspace*{0.27 in} 48.7,51.3,57.7,58.7,57.5,61.6,65,69.7) \\
G <- c(2.4,3.9,3.2,2.8,3.5,3.3,3.3,4,4.2,4.1,5.2,5.9,4.9,3.7,4,4.4,2.9,4.3, \\
\hspace*{0.27 in} 5.3,6.6,7.4,13.8) \\
I <- c(2.7,-0.2,1.9,5.2,3,5.1,5.6,4.2,3,5.1,1,-3.4,-6.2,-5.1,-3,-1.3,2.1,2, \\
\hspace*{0.27 in} -1.9,1.3,3.3,4.9) \\
K <- c(180.1,182.8,182.6,184.5,189.7,192.7,197.8,203.4,207.6,210.6,215.7, \\
\hspace*{0.27 in} 216.7,213.3,207.1,202,199,197.7,199.8,201.8,199.9,201.2,204.5) \\
P <- c(12.7,12.4,16.9,18.4,19.4,20.1,19.6,19.8,21.1,21.7,15.6,11.4,7,11.2, \\
\hspace*{0.27 in} 12.3,14,17.6,17.3,15.3,19,21.1,23.5) \\
T <- c(3.4,7.7,3.9,4.7,3.8,5.5,7,6.7,4.2,4,7.7,7.5,8.3,5.4,6.8,7.2,8.3,6.7, \\
\hspace*{0.27 in} 7.4,8.9,9.6,11.6) \\
Wg <- c(2.2,2.7,2.9,2.9,3.1,3.2,3.3,3.6,3.7,4,4.2,4.8,5.3,5.6,6,6.1,7.4, \\
\hspace*{0.27 in} 6.7,7.7,7.8,8,8.5) \\
Wp <- c(28.8,25.5,29.3,34.1,33.9,35.4,37.4,37.9,39.2,41.3,37.9,34.5,29,28.5, \\
\hspace*{0.27 in} 30.6,33.2,36.8,41,38.2,41.6,45,53.3) \\
X <- c(44.9,45.6,50.1,57.2,57.1,61,64,64.4,64.5,67,61.2,53.4,44.3,45.1, \\
\hspace*{0.27 in} 49.7,54.4,62.7,65,60.9,69.5,75.7,88.4) \\
year <- c(1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932, \\
\hspace*{0.27 in} 1933,1934,1935,1936,1937,1938,1939,1940,1941) \\
Y <- matrix(c(C,I,Wp),3,N, byrow=TRUE) \\
Z <- matrix(c(P, Wp+Wg, X), 3, N, byrow=TRUE) \\
S <- diag(NROW(Y)) \\
mon.names <- "LP" \\
parm.names <- parm.names(list(alpha=rep(0,4), beta=rep(0,4), \\
\hspace*{0.27 in} gamma=rep(0,4), pi=matrix(0,3,7), log.sigma=rep(0,3), \\
\hspace*{0.27 in} Omega=diag(3)), uppertri=c(0,0,0,0,0,1)) \\
MyData <- list(A=A, C=C, G=G, I=I, K=K, N=N, P=P, S=S, T=T, Wg=Wg, Wp=Wp, \\
\hspace*{0.27 in} X=X, Y=Y, Z=Z, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,4), rep(0,4), rep(0,4), rep(0,3*7), rep(0,3),
\hspace*{0.27 in} S[upper.tri(S, diag=TRUE)])}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:4]; beta <- parm[5:8]; gamma <- parm[9:12] \\
\hspace*{0.27 in} pi <- matrix(interval(parm[grep("pi", Data$parm.names)],-10,10), 3, 7) \\
\hspace*{0.27 in} parm[grep("pi", Data$parm.names)] <- as.vector(pi) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} Omega <- matrix(NA, 3, 3) \\
\hspace*{0.27 in} Omega[upper.tri(Omega, diag=TRUE)] <- parm[grep("Omega", \\
\hspace*{0.62 in} Data$parm.names)] \\
\hspace*{0.27 in} Omega[lower.tri(Omega)] <- Omega[upper.tri(Omega)] \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- sum(dnorm(gamma, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} pi.prior <- sum(dnorm(pi, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} Omega.prior <- dwishart(Omega, NROW(Data$S), Data$S, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- nu <- matrix(0,3,Data$N) \\
\hspace*{0.27 in} for (i in 1:3) \{ \\
\hspace*{0.62 in} nu[i,1] <- pi[i,1] + pi[i,3]*Data$K[1] + pi[i,5]*Data$A[1] + \\
\hspace*{0.95 in} pi[i,6]*Data$T[1] + pi[i,7]*Data$G[1] \\
\hspace*{0.62 in} nu[i,-1] <- pi[i,1] + pi[i,2]*Data$P[-Data$N] + \\
\hspace*{0.95 in} pi[i,3]*Data$K[-1] + pi[i,4]*Data$X[-Data$N] + \\
\hspace*{0.95 in} pi[i,5]*Data$A[-1] + pi[i,6]*Data$T[-1] + \\
\hspace*{0.95 in} pi[i,7]*Data$G[-1]\} \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Z, nu, matrix(sigma, 3, Data$N), log=TRUE)) \\
\hspace*{0.27 in} mu[1,1] <- alpha[1] + alpha[2]*nu[1,1] + alpha[4]*nu[2,1] \\
\hspace*{0.27 in} mu[1,-1] <- alpha[1] + alpha[2]*nu[1,-1] + \\
\hspace*{0.62 in} alpha[3]*Data$P[-Data$N] + alpha[4]*nu[2,-1] \\
\hspace*{0.27 in} mu[2,1] <- beta[1] + beta[2]*nu[1,1] + beta[4]*Data$K[1] \\
\hspace*{0.27 in} mu[2,-1] <- beta[1] + beta[2]*nu[1,-1] + \\
\hspace*{0.62 in} beta[3]*Data$P[-Data$N] + beta[4]*Data$K[-1] \\
\hspace*{0.27 in} mu[3,1] <- gamma[1] + gamma[2]*nu[3,1] + gamma[4]*Data$A[1] \\
\hspace*{0.27 in} mu[3,-1] <- gamma[1] + gamma[2]*nu[3,-1] + \\
\hspace*{0.62 in} gamma[3]*Data$X[-Data$N] + gamma[4]*Data$A[-1] \\
\hspace*{0.27 in} LL2 <- sum(dmvn(t(Data$Y), t(mu), Sigma, log=TRUE)) \\
\hspace*{0.27 in} if(!is.nan(LL2)) LL <- LL + LL2 \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + beta.prior + gamma.prior + pi.prior + \\
\hspace*{0.62 in} sigma.prior + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Space-Time, Nonseparable} \label{spacetime.nonsep}
This approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Matrix $\Xi$ contains the space-time effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses a nonseparable, stationary covariance function in which space and time are separable only when $\psi=0$. 
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu = \textbf{X} \beta + \Xi$$
$$\Xi \sim \mathcal{N}_{ST}(\Xi_\mu, \Sigma)$$
$$ \Sigma = \sigma^2_2 \exp \left (-\frac{\textbf{D}_S}{\phi_1}^\kappa - \frac{\textbf{D}_T}{\phi_2}^\lambda - \psi \frac{\textbf{D}_S}{\phi_1}^\kappa \frac{\textbf{D}_T}{\phi_2}^\lambda \right )$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$\sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,2$$
$$\psi \sim \mathcal{HC}(25)$$
$$\Xi_\mu = 0$$
$$\kappa = 1, \quad \lambda = 1$$
\subsection{Data}
\code{S <- 10 \\
T <- 5 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(rep(longitude,T),rep(latitude,T)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
D.T <- as.matrix(dist(cbind(rep(1:T,each=S),rep(1:T,each=S)), diag=TRUE, \\
\hspace*{0.27 in} upper=TRUE)) \\
Sigma <- 10000 * exp(-D.S/3 - D.T/2 - 0.2*(D.S/3)*(D.T/2)) \\
Xi <- as.vector(apply(rmvn(1000, rep(0,S*T), Sigma), 2, mean)) \\
Xi <- matrix(Xi,S,T) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(beta, X)) \\
Y <- mu + Xi \\
mon.names <- c("LP","psi","sigma[1]","sigma[2]") \\
parm.names <- parm.names(list(Xi=matrix(0,S,T), beta=rep(0,2), \\
\hspace*{0.27 in} phi=rep(0,2), log.sigma=rep(0,2), log.psi=0)) \\
MyData <- list(D.S=D.S, D.T=D.T, S=S, T=T, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S*T), mean(Y), 0, rep(1,2), rep(0,2), 0)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} Xi.mu <- rep(0,Data$S*Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} Xi <- parm[grep("Xi", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} psi <- exp(parm[grep("log.psi", Data$parm.names)]) \\
\hspace*{0.27 in} Sigma <- sigma[2]*sigma[2] * exp(-(Data$D.S / phi[1])\textasciicircum kappa - \\
\hspace*{0.62 in} (Data$D.T / phi[2])\textasciicircum lambda - \\
\hspace*{0.62 in} psi*(Data$D.S / phi[1])\textasciicircum kappa * (Data$D.T / phi[2])\textasciicircum lambda) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Xi.prior <- dmvn(Xi, Xi.mu, Sigma, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} psi.prior <- dhalfcauchy(psi, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Xi <- matrix(Xi, Data$S, Data$T) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(beta, Data$X)) + Xi \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + Xi.prior + sigma.prior + phi.prior + psi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,psi,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Space-Time, Separable} \label{spacetime.sep}
This introductory approach to space-time or spatiotemporal modeling applies kriging both to the stationary spatial and temporal components, where space is continuous and time is discrete. Vector $\zeta$ contains the spatial effects and vector $\theta$ contains the temporal effects. Spatial coordinates are given in longitude and latitude for $s=1,\dots,S$ points in space and measurements are taken across time-periods $t=1,\dots,T$ for $\textbf{Y}_{s,t}$. The dependent variable is also a function of design matrix $\textbf{X}$ and regression effects vector $\beta$. For more information on kriging, see section \ref{kriging}. This example uses separable space-time covariances, which is more convenient but usually less appropriate than a nonseparable covariance function.
\subsection{Form}
$$\textbf{Y}_{s,t} \sim \mathcal{N}(\mu_{s,t}, \sigma^2_1), \quad s=1,\dots,S, \quad t=1,\dots,T$$
$$\mu_{s,t} = \textbf{X}_{s,1:J} \beta + \zeta_s + \Theta_{s,t}$$
$$\Theta_{s,1:T} = \theta$$
$$\theta \sim \mathcal{N}_N(\theta_\mu, \Sigma_T)$$
$$\Sigma_T = \sigma^2_3 \exp(-\phi_2 \textbf{D}_T)^\lambda$$
$$ \zeta \sim \mathcal{N}_N(\zeta_\mu, \Sigma_S)$$
$$ \Sigma_S = \sigma^2_2 \exp(-\phi_1 \textbf{D}_S)^\kappa$$
$$ \beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,2$$
$$ \sigma_k \sim \mathcal{HC}(25), \quad k=1,\dots,3$$
$$ \phi_k \sim \mathcal{U}(1, 5), \quad k=1,\dots,2$$
$$ \zeta_\mu = 0$$
$$ \theta_\mu = 0$$
$$ \kappa = 1, \quad \lambda = 1$$

\subsection{Data}
\code{S <- 20 \\
T <- 10 \\
longitude <- runif(S,0,100) \\
latitude <- runif(S,0,100) \\
D.S <- as.matrix(dist(cbind(longitude,latitude), diag=TRUE, upper=TRUE)) \\
Sigma.S <- 10000 * exp(-1.5 * D.S) \\
zeta <- as.vector(apply(rmvn(1000, rep(0,S), Sigma.S), 2, mean)) \\
D.T <- as.matrix(dist(cbind(c(1:T),c(1:T)), diag=TRUE, upper=TRUE)) \\
Sigma.T <- 10000 * exp(-3 * D.T) \\
theta <- as.vector(apply(rmvn(1000, rep(0,T), Sigma.T), 2, mean)) \\
Theta <- matrix(theta,S,T,byrow=TRUE) \\
beta <- c(50,2) \\
X <- matrix(runif(S*2,-2,2),S,2); X[,1] <- 1 \\
mu <- as.vector(tcrossprod(beta, X)) \\
Y <- mu + zeta + Theta + matrix(rnorm(S*T,0,0.1),S,T) \\
mon.names <- c("LP","sigma[1]","sigma[2]","sigma[3]") \\
parm.names <- parm.names(list(zeta=rep(0,S), theta=rep(0,T), \\
\hspace*{0.27 in} beta=rep(0,2), phi=rep(0,2), log.sigma=rep(0,3))) \\
MyData <- list(D.S=D.S, D.T=D.T, S=S, T=T, X=X, Y=Y, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,S), rep(0,T), rep(0,2), rep(1,2), rep(0,3))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} zeta.mu <- rep(0,Data$S) \\
\hspace*{0.27 in} theta.mu <- rep(0,Data$T) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} zeta <- parm[grep("zeta", Data$parm.names)] \\
\hspace*{0.27 in} theta <- parm[grep("theta", Data$parm.names)] \\
\hspace*{0.27 in} kappa <- 1; lambda <- 1 \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} phi <- interval(parm[grep("phi", Data$parm.names)], 1, 5) \\
\hspace*{0.27 in} parm[grep("phi", Data$parm.names)] <- phi \\
\hspace*{0.27 in} Sigma.S <- sigma[2]\textasciicircum 2 * exp(-phi[1] * Data$D.S)\textasciicircum kappa \\
\hspace*{0.27 in} Sigma.T <- sigma[3]\textasciicircum 2 * exp(-phi[2] * Data$D.T)\textasciicircum lambda \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} zeta.prior <- dmvn(zeta, zeta.mu, Sigma.S, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dmvn(theta, theta.mu, Sigma.T, log=TRUE) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(25, log=TRUE)) \\
\hspace*{0.27 in} phi.prior <- sum(dunif(phi, 1, 5, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Theta <- matrix(theta, Data$S, Data$T, byrow=TRUE) \\
\hspace*{0.27 in} mu <- as.vector(tcrossprod(beta, Data$X)) + zeta + Theta \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, sigma[1], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + zeta.prior + theta.prior + sigma.prior + \\
\hspace*{0.62 in} phi.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Survival Analysis} \label{survival}
Although the dependent variable is usually denoted as $\textbf{t}$ in survival analysis, it is denoted here as $\textbf{y}$ so Laplace's Demon recognizes it as a dependent variable for posterior predictive checks.
\subsection{Form}
$$\textbf{y}_i \sim \mathcal{WEIB}(\gamma, \mu_i), \quad i=1,\dots,N$$
$$\mu = \exp(\textbf{X} \beta)$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\gamma \sim \mathcal{G}(1, 0.001)$$
\subsection{Data}
\code{N <- 50 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-1,1) \\
y <- as.vector(round(exp(tcrossprod(beta, X)))) + 1 \\
mon.names <- c("LP","gamma") \\
parm.names <- parm.names(list(beta=rep(0,J), log.gamma=0)) \\
MyData <- list(J=J, N=N, X=X, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} gamma <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} gamma.prior <- dgamma(gamma, 1, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- exp(tcrossprod(beta, Data$X)) + 1 \\
\hspace*{0.27 in} h <- (gamma/lambda)*(Data$y/lambda)\textasciicircum (gamma-1) \\
\hspace*{0.27 in} S <- exp(-mu * Data$y\textasciicircum gamma) \\
\hspace*{0.27 in} LL <- sum(dweibull(Data$y, gamma, mu, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + gamma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, gamma), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Variable Selection} \label{variable.selection}
This example uses a modified form of the random-effects (or global adaptation) Stochastic Search Variable Selection (SSVS) algorithm presented in \citet{ohara09}, which selects variables according to practical significance rather than statistical significance. Here, SSVS is applied to linear regression, though this method is widely applicable. For $J$ variables, each regression effects vector $\beta_j$ is conditional on $\gamma_j$, a binary inclusion variable. Each $\beta_j$ is a discrete mixture distribution with respect to $\gamma_j = 0$ or $\gamma_j = 1$, with precision 100 or $\beta_\sigma = 0.1$, respectively. As with other representations of SSVS, these precisions may require tuning.

With other representations of SSVS, each $\gamma_j$ is Bernoulli-distributed, though this would be problematic in Laplace's Demon, because $\gamma_j$ would be in the list of parameters (rather than monitors), and would not be stationary due to switching behavior. To keep $\gamma$ in the monitors, an uninformative normal density is placed on each prior $\delta_j$, with mean $1/J$ for $J$ variables and variance $1000$. Each $\delta_j$ is transformed with the inverse logit and rounded to $\gamma_j$. Note that $\lfloor x + 0.5 \rfloor$ means to round $x$. The prior for $\delta$ can be manipulated to influence sparseness.

When the goal is to select the best model, each $\textbf{X}_{1:N,j}$ is retained for a future run when the posterior mean of $\gamma_j \ge 0.5$. When the goal is model-averaging, the results of this model may be used directly.

\subsection{Form}
$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X} \beta$$
$$(\beta_j | \gamma_j) \sim (1 - \gamma_j)\mathcal{N}(0, 0.01) + \gamma_j \mathcal{N}(0, \beta^2_\sigma) \quad j=1,\dots,J$$
$$\beta_\sigma \sim \mathcal{HC}(25)$$
$$\gamma_j = \lfloor \frac{1}{1 + \exp(-\delta_j)} + 0.5 \rfloor, \quad j=1,...,J$$
$$\delta_j \sim \mathcal{N}(0, 10) \in [-100,100], \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- NCOL(demonsnacks) \\
y <- log(demonsnacks$Calories) \\
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- c("LP", "min.beta.sigma", "sigma", \\
\hspace*{0.27 in} parm.names(list(gamma=rep(0,J)))) \\
parm.names <- parm.names(list(beta=rep(0,J), delta=rep(0,J), \\
\hspace*{0.27 in} log.beta.sigma=0, log.sigma=0)) \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), rep(0,J), log(1), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperparameters \\
\hspace*{0.27 in} beta.sigma <- exp(parm[grep("log.beta.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} delta <- interval(parm[grep("delta", Data$parm.names)],-100,100) \\
\hspace*{0.27 in} parm[grep("delta", Data$parm.names)] <- delta \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} gamma <- round(invlogit(delta)) \\
\hspace*{0.27 in} beta.sigma <- ifelse(gamma == 0, 0.1, beta.sigma) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Hyperprior and Prior Densities) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, beta.sigma, log=TRUE)) \\
\hspace*{0.27 in} beta.sigma.prior <- sum(dhalfcauchy(beta.sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} delta.prior <- sum(dtrunc(delta, "norm", a=-100, b=100, \\
\hspace*{0.62 in} mean=logit(1/Data$J), sd=sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- tcrossprod(beta, Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(y, mu, sigma, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + beta.prior + beta.sigma.prior + delta.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, min(beta.sigma), \\
\hspace*{0.62 in} sigma, gamma), yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Vector Autoregression, VAR(1)} \label{var1}
\subsection{Form}
$$\textbf{Y}_{t,j} \sim \mathcal{N}(\mu_{t,j}, \sigma^2_j), \quad t=1,\dots,T, \quad j=1,\dots,J$$
$$\mu_{t,j} = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{t-1,j}$$
$$\textbf{y}^{new}_j = \alpha_j + \Phi_{1:J,j} \textbf{Y}_{T,j}$$
$$\alpha_j \sim \mathcal{N}(0, 1000)$$
$$\sigma_j \sim \mathcal{HC}(25)$$
$$\Phi_{i,k} \sim \mathcal{N}(0, 1000), \quad i=1,\dots,J, \quad k=1,\dots,J$$ 
\subsection{Data}
\code{T <- 100 \\
J <- 3 \\
Y <- matrix(0,T,J) \\
for (j in 1:J) \{for (t in 2:T) \{ \\
\hspace*{0.27 in} Y[t,j] <- Y[t-1,j] + rnorm(1,0,0.1)\}\} \\
mon.names <- c("LP", parm.names(list(ynew=rep(0,J)))) \\
parm.names <- parm.names(list(alpha=rep(0,J), Phi=matrix(0,J,J), \\
\hspace*{0.27 in} log.sigma=rep(0,J))) \\
MyData <- list(J=J, T=T, Y=Y, mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(colMeans(Y), rep(0,J*J), rep(log(1),J))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$J] \\
\hspace*{0.27 in} Phi <- matrix(parm[grep("Phi", Data$parm.names)], Data$J, Data$J) \\
\hspace*{0.27 in} sigma <- exp(parm[grep("log.sigma", Data$parm.names)]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} Phi.prior <- sum(dnorm(Phi, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} sigma.prior <- sum(dhalfcauchy(sigma, 25, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(alpha,Data$T,Data$J,byrow=TRUE) \\
\hspace*{0.62 in} mu[-1,] <- mu[-1,] + t(tcrossprod(Phi,Data$Y[-Data$T,])) \\
\hspace*{0.27 in} ynew <- alpha + as.vector(crossprod(Phi, Data$Y[Data$T,])) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$Y, mu, \\
\hspace*{0.62 in} matrix(sigma,Data$T,Data$J,byrow=TRUE), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + Phi.prior + sigma.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,ynew), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Zero-Inflated Poisson (ZIP)} \label{zip}
\subsection{Form}
$$\textbf{y} \sim \mathcal{P}(\Lambda_{1:N,2})$$
$$\textbf{z} \sim \mathcal{BERN}(\Lambda_{1:N,1})$$
\[\textbf{z}_i = \left\{ 
\begin{array}{l l}
  1 & \quad \mbox{if $\textbf{y}_i = 0$}\\
  0 \\ \end{array} \right. \]
\[\Lambda_{i,2} = \left\{ 
\begin{array}{l l}
  0 & \quad \mbox{if $\Lambda_{i,1} \ge 0.5$}\\
  \Lambda_{i,2} \\ \end{array} \right. \]
$$\Lambda_{1:N,1} = \frac{1}{1 + \exp(-\textbf{X}_1 \alpha)}$$
$$\Lambda_{1:N,2} = \exp(\textbf{X}_2 \beta)$$
$$\alpha_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_1$$
$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J_2$$
\subsection{Data}
\code{N <- 1000 \\
J1 <- 4 \\
J2 <- 3 \\
X1 <- matrix(runif(N*J1,-2,2),N,J1); X1[,1] <- 1 \\
X2 <- matrix(runif(N*J2,-2,2),N,J2); X2[,1] <- 1 \\
alpha <- runif(J1,-1,1) \\
beta <- runif(J2,-1,1) \\
p <- as.vector(invlogit(tcrossprod(alpha, X1) + rnorm(N,0,0.1))) \\
mu <- as.vector(round(exp(tcrossprod(beta, X2) + rnorm(N,0,0.1)))) \\
y <- ifelse(p > 0.5, 0, mu) \\
z <- ifelse(y == 0, 1, 0) \\
mon.names <- "LP" \\
parm.names <- parm.names(list(alpha=rep(0,J1), beta=rep(0,J2))) \\
MyData <- list(J1=J1, J2=J2, N=N, X1=X1, X2=X2, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y, z=z)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J1+J2)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$J1] \\
\hspace*{0.27 in} beta <- parm[grep("beta", Data$parm.names)] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- sum(dnorm(alpha, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} beta.prior <- sum(dnorm(beta, 0, sqrt(1000), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- matrix(NA, Data$N, 2) \\
\hspace*{0.27 in} Lambda[,1] <- invlogit(tcrossprod(alpha, Data$X1)) \\
\hspace*{0.27 in} Lambda[,2] <- exp(tcrossprod(beta, Data$X2)) \\
\hspace*{0.27 in} Lambda[,2] <- ifelse(Lambda[,1] >= 0.5, 0, Lambda[,2]) \\
\hspace*{0.27 in} LL1 <- sum(dbern(Data$z, Lambda[,1], log=TRUE)) \\
\hspace*{0.27 in} LL2 <- sum(dpois(Data$y, Lambda[,2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL1 + LL2 + alpha.prior + beta.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL2, Monitor=LP, \\
\hspace*{0.62 in} yhat=Lambda[,2], parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\bibliography{References.bib}

\end{document}
