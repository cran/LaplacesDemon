\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Examples}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Byron Hall\\STATISTICAT, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon} Examples}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Byron Hall} %% comma-separated
\Plaintitle{LaplacesDemon Examples} %% without formatting
\Shorttitle{Examples} %% a short title (if necessary)

\Abstract{The \pkg{LaplacesDemon} package in \proglang{R} enables Bayesian inference with any Bayesian model, provided the user specifies the likelihood.  This vignette is a compendium of examples of how to specify different model forms.
}
\Keywords{Bayesian, Bayesian Inference, Laplace's Demon, LaplacesDemon, R, 
STATISTICAT}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
  Byron Hall\\
  STATISTICAT, LLC\\
  Farmington, CT\\
  E-mail: \email{statisticat@gmail.com}\\
  URL: \url{http://www.statisticat.com/laplacesdemon.html}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
\pkg{LaplacesDemon} \citep{hall11}, usually referred to as Laplace's Demon, is an \proglang{R} package that is available on CRAN \citep{rdct:r}. A formal introduction to Laplace's Demon is provided in an accompanying vignette entitled ``\pkg{LaplacesDemon} Tutorial'', and an introduction to Bayesian inference is provided in the ``Bayesian Inference'' vignette.

The purpose of this document is to provide users of the \pkg{LaplacesDemon} package with examples of a variety of Bayesian methods. To conserve space, the examples are not worked out in detail, and only the minimum of necessary materials is provided for using the various methodologies. Necessary materials include the form expressed in notation, data (which is often simulated), initial values, and the \code{Model} function. This vignette will grow over time as examples of more methods become included. Contributed examples are welcome. Please send contributed examples in a similar format in an email to \email{statisticat@gmail.com} for review and testing. All accepted contributions are, of course, credited.

\begin{center} \textbf{Contents} \end{center}
\begin{itemize}
\item Autoregression, AR(1) \ref{ar1}
\item Binary Logit \ref{binary.logit}
\item Binomial Probit \ref{binomial.probit}
\item Dynamic Linear Model (DLM) \ref{dlm}
\item Laplace Regression \ref{laplace.reg}
\item Linear Regression \ref{linear.reg}
\item Multinomial Logit \ref{mnl}
\item Normal, Multilevel \ref{norm.ml}
\item Poisson Regression \ref{poisson.reg}
\item Seemingly Unrelated Regression (SUR) \ref{sur}
\item Zero-Inflated Poisson (ZIP) \ref{zip}
\end{itemize}

\section{Autoregression, AR(1)} \label{ar1}
\subsection{Form}
$$y_t \sim N(\mu_{t-1}, \tau^{-1}), \quad t=2,\dots,(T-1)$$
$$y^{new}_T \sim N(\mu_T, \tau^{-1})$$
$$\mu_t = \alpha + \phi y_t, \quad t=1,\dots,T$$
$$\alpha \sim N(0, 1000)$$
$$\phi \sim N(0, 1000)$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{T <- 100 \\
y <- rep(0,T) \\
y[1] <- 0 \\
for (t in 2:T) \{y[t] <- y[t-1] + rnorm(1,0,0.1)\} \\
mon.names <- c("LP", "tau", paste("mu[",T,"]", sep="")) \\
parm.names <- c("alpha","phi","log.tau") \\
MyData <- list(T=T, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,2), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} alpha.mu <- 0; alpha.tau <- 1.0E-3 \\
\hspace*{0.27 in} phi.mu <- 0; phi.tau <- 1.0E-3 \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3; tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1]; phi <- parm[2]; tau <- exp(parm[3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} phi.prior <- dnorm(phi, phi.mu, 1/sqrt(phi.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + phi*Data$y \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[2:(Data$T-1)], mu[1:(Data$T-2)], \\ 
\hspace*{0.62 in} 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + phi.prior + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,tau,mu[Data$T]), \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binary Logit} \label{binary.logit}
\subsection{Form}
$$y \sim Bern(\eta)$$
$$\eta = \log[1 + \exp(\mu)]$$
$$\mu = \textbf{X} \beta$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{data(demonsnacks) \\
N <- NROW(demonsnacks) \\
J <- 3 \\
y <- ifelse(demonsnacks$Calories <= 137, 0, 1) \\
X <- cbind(1, as.matrix(demonsnacks[,c(7,8)])) \\
for (j in 2:J) \{X[,j] <- CenterScale(X[,j])\} \\
mon.names <- "LP" \\
parm.names <- rep(NA,J) \\
for (j in 1:J) \{parm.names[j] <- paste("beta[",j,"]",sep="")\} \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} eta <- invlogit(mu) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dbern(Data$y, eta, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=eta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Binomial Probit} \label{binomial.probit}
\subsection{Form}
$$y \sim Bin(p, n)$$
$$p = \phi(\mu)$$
$$\mu = \beta_1 + \beta_2 x$$
$$\beta_j \sim N(0,1000), \quad j=1,\dots,J$$
where $\phi$ is the inverse CDF, and $J$=2.
\subsection{Data}
\code{\#10 Trials \\
exposed <- c(100,100,100,100,100,100,100,100,100,100) \\
deaths <- c(10,20,30,40,50,60,70,80,90,100) \\
dose <- c(1,2,3,4,5,6,7,8,9,10) \\
J <- 2 \#Number of parameters \\
mon.names <- "LP" \\
parm.names <- c("beta[1]","beta[2]") \\
MyData <- list(J=J, n=exposed, mon.names=mon.names, parm.names=parm.names, \\
\hspace*{0.27 in} x=dose, y=deaths)
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log of Prior Densities \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta[1] + beta[2]*Data$x \\
\hspace*{0.27 in} mu <- ifelse(mu < -10, -10, mu); mu <- ifelse(mu > 10, 10, mu) \\
\hspace*{0.27 in} p <- pnorm(mu) \\
\hspace*{0.27 in} LL <- sum(dbinom(Data$y, Data$n, p, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=p, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Dynamic Linear Model (DLM)} \label{dlm}
The data is presented so that the time-series is subdivided into three 
sections: modeled ($t=1,\dots,T_m$), one-step ahead forecast ($t=T_m+1$), 
and future forecast [$t=(T_m+2),\dots,T$].
\subsection{Form}
$$y_t \sim N(\mu_t, \tau^{-1}_V), \quad t=1,\dots,T_m$$
$$y^{new}_t \sim N(\mu_t, \tau^{-1}_V), \quad t=(T_m+1),\dots,T$$
$$\mu_t = \alpha + x_t \beta_t, \quad t=1,\dots,T$$
$$\alpha \sim N(0, 1000)$$
$$\beta_1 \sim N(0, 1000)$$
$$\beta_t \sim N(\beta_{t-1}, \tau^{-1}_W), \quad t=2,\dots,T$$
$$\tau_V \sim \Gamma(0.001, 0.001)$$
$$\tau_W \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{T <- 20 \\
T.m <- 14 \\
beta.orig <- x <- rep(0,T) \\
for (t in 2:T) \{ \\
     beta.orig[t] <- beta.orig[t-1] + rnorm(1,0,0.1) \\
     x[t] <- x[t-1] + rnorm(1,0,0.1)\} \\
y <- 10 + beta.orig*x + rnorm(T,0,0.1) \\
y[(T.m+2):T] <- NA \\
mon.names <- rep(NA, (T-T.m)) \\
for (i in 1:(T-T.m)) {mon.names[i] <- paste("mu[",(T.m+i),"]", sep="")} \\
parm.names <- rep(NA, T+3) \\
parm.names[1] <- "alpha" \\
for (i in 1:T) \{parm.names[i+1] <- paste("beta[", i, "]", sep="")\} \\
parm.names[(T+2):(T+3)] <- c("log.beta.w.tau","log.v.tau") \\
MyData <- list(T=T, T.m=T.m, mon.names=mon.names, parm.names=parm.names, \\ 
\hspace*{0.27 in} x=x, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,T+3)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1] \\
\hspace*{0.27 in} beta <- parm[2:(Data$T+1)] \\
\hspace*{0.27 in} beta.w.tau <- exp(parm[Data$T+2]) \\
\hspace*{0.27 in} v.tau <- exp(parm[Data$T+3]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, 0, 1/sqrt(1.0E-3), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- rep(0,Data$T) \\
\hspace*{0.27 in} beta.prior[1] <- dnorm(beta[1], 0, 1/sqrt(1.0E-3), log=TRUE) \\
\hspace*{0.27 in} beta.prior[2:Data$T] <- dnorm(beta[2:Data$T], beta[1:(Data$T-1)], \\
\hspace*{0.62 in} 1/sqrt(beta.w.tau), log=TRUE) \\
\hspace*{0.27 in} beta.w.tau.prior <- dgamma(beta.w.tau, 0.001, 0.001, log=TRUE) \\
\hspace*{0.27 in} v.tau.prior <- dgamma(v.tau, 1.0E-3, 1.0E-3, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- alpha + beta*Data$x \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y[1:Data$T.m], mu[1:Data$T.m], 1/sqrt(v.tau), \\
\hspace*{0.62 in} log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + alpha.prior + sum(beta.prior) + beta.w.tau.prior + \\
\hspace*{0.62 in} v.tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=mu[(Data$T.m+1):Data$T], \\
\hspace*{0.62 in} yhat=mu, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Laplace Regression} \label{laplace.reg}
This linear regression specifies that $y$ is Laplace-distributed, where it is usually Gaussian or normally-distributed.  It has been claimed that it should be surprising that the normal distribution became the standard, when the Laplace distribution usually fits better and has wider tails \citep{kotz01}. Another popular alternative is to use the t-distribution, though it is more computationally expensive to estimate, because it has three parameters.  The Laplace distribution has only two parameters, location and scale like the normal distribution, and is computationally easier to fit.  This example could be taken one step further, and the parameter vector $\beta$ could be Laplace-distributed.  Laplace's Demon recommends that users experiment with replacing the normal distribution with the Laplace distribution.
\subsection{Form}
$$y \sim L(\mu, \tau^{-1})$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rlaplace(N,0,0.1) \\
y <- as.vector(beta \%*\% t(X) + e) \\
mon.names <- c("LP", "tau") \\
parm.names <- rep(NA, J+1) \\
for (j in 1:J) \{parm.names[j] <- paste("beta[",j,"]",sep="")\} \\
parm.names[J+1] <- "log.tau" \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} LL <- sum(dlaplace(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, tau), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Linear Regression} \label{linear.reg}
\subsection{Form}
$$y \sim N(\mu, \tau^{-1})$$
$$\mu = \textbf{X}\beta$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J$$
$$\tau \sim \Gamma(0.001, 0.001)$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(1,N,J) \\
for (j in 2:J) \{X[,j] <- rnorm(N,runif(1,-3,3),runif(1,0.1,1))\} \\
beta <- runif(J,-3,3) \\
e <- rnorm(N,0,0.1) \\
y <- as.vector(beta \%*\% t(X) + e) \\
mon.names <- c("LP", "tau") \\
parm.names <- rep(NA, J+1) \\
for (j in 1:J) \{parm.names[j] <- paste("beta[",j,"]",sep="")\} \\
parm.names[J+1] <- "log.tau" \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,J), log(1))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm[1:Data$J] \\
\hspace*{0.27 in} tau <- exp(parm[Data$J+1]) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- beta \%*\% t(Data$X) \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, mu, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) + tau.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, tau), yhat=mu, \\
\hspace*{0.62 in} parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Multinomial Logit} \label{mnl}
\subsection{Form}
$$y_i \sim Cat(p_{i,1:J})$$
$$p_{i,j} = \frac{\phi_{i,j}}{\sum^J_{j=1} \phi_{i,j}}, \quad \sum^J_{j=1} p_{i,j} = 1$$
$$\phi = \exp(\mu)$$
$$\mu_{i,J} = 0$$
$$\mu_{i,j} = \textbf{X}_{i,1:K} \beta_{j,1:K}, \quad j=1,\dots,(J-1)$$
$$\beta_{j,k} \sim N(0, 1000) \quad j=1,\dots,(J-1)$$
\subsection{Data}
\code{y <- x01 <- x02 <- c(1:300) \\
y[1:100] <- 1 \\
y[101:200] <- 2 \\
y[201:300] <- 3 \\
x01[1:100] <- rnorm(100, 25, 2.5) \\
x01[101:200] <- rnorm(100, 40, 4.0) \\
x01[201:300] <- rnorm(100, 35, 3.5) \\
x02[1:100] <- rnorm(100, 2.51, 0.25) \\
x02[101:200] <- rnorm(100, 2.01, 0.20) \\
x02[201:300] <- rnorm(100, 2.70, 0.27) \\
N <- length(y) \\
J <- 3 \#Number of categories in y \\
K <- 3 \#Number of predictors (including the intercept) \\
X <- matrix(c(rep(1,N),x01,x02),N,K) \\
mon.names <- "LP" \\
parm.names <- c("beta[1,1]","beta[1,2]","beta[1,3]","beta[2,1]", \\
\hspace*{0.27 in} "beta[2,2]","beta[2,3]") \#\#\# Parameter Names [J,K] \\
MyData <- list(J=J, K=K, N=N, X=X, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,(J-1)*K))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,(Data$J-1)*Data$K) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,(Data$J-1)*Data$K) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} mu <- matrix(0,Data$N,(Data$J-1)) \\
\hspace*{0.27 in} mu[,1] <- beta[1] + beta[2]*Data$X[,2] + beta[3]*Data$X[,3] \\
\hspace*{0.27 in} mu[,2] <- beta[4] + beta[5]*Data$X[,2] + beta[6]*Data$X[,3] \\
\hspace*{0.27 in} mu <- ifelse(mu > 700, 700, mu) \\
\hspace*{0.27 in} mu <- ifelse(mu < -700, -700, mu) \\
\hspace*{0.27 in} p <- phi <- matrix(c(exp(mu[,1]),exp(mu[,2]),rep(1,Data$N)), \\
\hspace*{0.62 in} Data$N, Data$J) \\
\hspace*{0.27 in} for(j in 1:Data$J) \{p[,j] <- phi[,j] / apply(phi,1,sum)\} \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Y <- matrix(0,Data$N,Data$J) \\
\hspace*{0.27 in} for (j in 1:Data$J) \{Y[,j] <- ifelse(Data$y == j, 1, 0)\} \\
\hspace*{0.27 in} LL <- sum(Y * log(p)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=as.vector(phi), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Normal, Multilevel} \label{norm.ml}
This is Gelman's school example \citep{gelman04}.  Note that \pkg{LaplacesDemon} is much slower to converge compared to this example that uses the \pkg{R2WinBUGS} package \citep{gelman09}, an \proglang{R} package on CRAN.  However, also note that Laplace's Demon (eventually) provides a better answer (higher ESS, lower DIC, etc.).

\subsection{Form}
$$y_j \sim N(\theta_j, \tau^{-1}_j)$$
$$\theta_j \sim N(\theta_{\mu}, \theta_\tau^{-1})$$
$$\theta_{\mu} \sim N(0, 1000)$$
$$\theta_{\tau} \sim \Gamma(0.001, 0.001)$$
$$\tau_j =sd^{-2}$$
\subsection{Data}
\code{J <- 8 \\
y <- c(28.4, 7.9, -2.8, 6.8, -0.6, 0.6, 18.0, 12.2) \\
sd <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6) \\
mon.names <- c("LP","theta.sigma") \\
parm.names <- 2*J+2 \\
for (j in 1:J) \{parm.names[j] <- paste("theta[",j,"]",sep="")\} \\
parm.names[J+1] <- paste("theta.mu[",j,"]",sep="") \\
parm.names[J+2] <- paste("log.theta.sigma[",j,"]",sep="") \\
MyData <- list(J=J, mon.names=mon.names, parm.names=parm.names, sd=sd, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J+2)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Hyperprior Parameters \\
\hspace*{0.27 in} theta.mu.mu <- 0 \\
\hspace*{0.27 in} theta.mu.tau <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} theta.mu <- parm[Data$J+1] \\
\hspace*{0.27 in} theta.sigma <- exp(parm[Data$J+2]) \\
\hspace*{0.27 in} tau.alpha <- 1.0E-3 \\
\hspace*{0.27 in} tau.beta <- 1.0E-3 \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} theta <- parm[1:Data$J]; tau <- 1/(sd*sd) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} theta.mu.prior <- dnorm(theta.mu, theta.mu.mu, \\ 
\hspace*{0.62 in} 1/sqrt(theta.mu.tau), log=TRUE) \\ 
\hspace*{0.27 in} tau.prior <- dgamma(tau, tau.alpha, tau.beta, log=TRUE) \\
\hspace*{0.27 in} theta.prior <- dnorm(theta, theta.mu, theta.sigma, log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} LL <- sum(dnorm(Data$y, theta, 1/sqrt(tau), log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + theta.mu.prior + sum(theta.prior) + sum(tau.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP, theta.sigma), \\
\hspace*{0.62 in} yhat=theta, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \} \\
}

\section{Poisson Regression} \label{poisson.reg}
\subsection{Form}
$$y \sim Pois(\lambda)$$
$$\lambda = \exp(\textbf{X}\beta)$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J$$
\subsection{Data}
\code{N <- 10000 \\
J <- 5 \\
X <- matrix(runif(N*J,-2,2),N,J); X[,1] <- 1 \\
beta <- runif(J,-2,2) \\
y <- as.vector(round(exp(beta \%*\% t(X)))) \\
mon.names <- "LP" \\
parm.names <- rep(NA,J) \\
for (j in 1:J) \{parm.names[j] <- paste("beta[",j,"]",sep="")\} \\
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y) \\
}
\subsection{Initial Values}
\code{Initial.Values <- rep(0,J)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} beta.mu <- rep(0,Data$J) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,Data$J) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} beta <- parm \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} lambda <- exp(beta \%*\% t(Data$X)) \\
\hspace*{0.27 in} LL <- sum(dpois(Data$y, lambda, log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, \\
\hspace*{0.62 in} yhat=lambda, parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Seemingly Unrelated Regression (SUR)} \label{sur}
The following data was used by \citet{zellner62} when introducing the Seemingly Unrelated Regression methodology. In this particular example, the elements of the precision matrix $\Omega$ are constrained. Hopefully I will soon discover, or someone can show me, a better way of maintaining positive-definiteness. Nonetheless, this form seems to work well.
\subsection{Form}
$$Y_{t,k} \sim N_K(\mu, \Sigma), \quad t=1,\dots,T; \quad k=1,\dots,K$$
$$\mu_{1,t} = \alpha_1 + \alpha_2 X_{t,1} + \alpha_3 X_{t,2}, \quad t=1,\dots,T$$
$$\mu_{2,t} = \beta_1 + \beta_2 X_{t,3} + \beta_3 X_{t,4}, \quad t=1,\dots,T$$
$$\Sigma = \Omega^{-1}$$
$$\Omega \sim Wishart(\nu, S), \quad 1.0E-7 \le \Omega_{i,l}$$
$$ S = R^{-1}$$
$$\alpha_j \sim N(0, 1000), \quad j=1,\dots,J$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J$$
where each element $i$ and $l$ of precision matrix $\Omega$ is constrained to the interval [1.0E-7,$\infty$), J=3, K=2, and T=20.
\subsection{Data}
\code{T <- 20 \\
year <- c(1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946, \\
\hspace*{0.27 in} 1947,1948,1949,1950,1951,1952,1953,1954) \\
IG <- c(33.1,45.0,77.2,44.6,48.1,74.4,113.0,91.9,61.3,56.8,93.6,159.9, \\
\hspace*{0.27 in} 147.2,146.3,98.3,93.5,135.2,157.3,179.5,189.6) \\
VG <- c(1170.6,2015.8,2803.3,2039.7,2256.2,2132.2,1834.1,1588.0,1749.4, \\
\hspace*{0.27 in} 1687.2,2007.7,2208.3,1656.7,1604.4,1431.8,1610.5,1819.4,2079.7, \\
\hspace*{0.27 in} 2371.6,2759.9) \\
CG <- c(97.8,104.4,118.0,156.2,172.6,186.6,220.9,287.8,319.9,321.3,319.6, \\
\hspace*{0.27 in} 346.0,456.4,543.4,618.3,647.4,671.3,726.1,800.3,888.9) \\
IW <- c(12.93,25.90,35.05,22.89,18.84,28.57,48.51,43.34,37.02,37.81, \\
\hspace*{0.27 in} 39.27,53.46,55.56,49.56,32.04,32.24,54.38,71.78,90.08,68.60) \\
VW <- c(191.5,516.0,729.0,560.4,519.9,628.5,537.1,561.2,617.2,626.7, \\
\hspace*{0.27 in} 737.2,760.5,581.4,662.3,583.8,635.2,723.8,864.1,1193.5,1188.9) \\
CW <- c(1.8,0.8,7.4,18.1,23.5,26.5,36.2,60.8,84.4,91.2,92.4,86.0,111.1, \\
\hspace*{0.27 in} 130.6,141.8,136.7,129.7,145.5,174.8,213.5) \\
Y <- matrix(c(IG,IW),T,2) \\
R <- matrix(0.001,2,2); diag(R) <- 1 \\
mon.names <- c("LP","Sigma[1,1]","Sigma[2,1]","Sigma[1,2]","Sigma[2,2]") \\
parm.names <- c("alpha[1]","alpha[2]","alpha[3]","beta[1]","beta[2]", \\
\hspace*{0.27 in} "beta[3]","log.Omega[1,1]","log.Omega[2,1]","log.Omega[2,2]") \\
MyData <- list(R=R, T=T, Y=Y, CG=CG, CW=CW, IG=IG, IW=IW, VG=VG, VW=VW, \\
\hspace*{0.27 in} mon.names=mon.names, parm.names=parm.names) \\
}
\subsection{Initial Values}
\code{Initial.Values <- c(rep(0,6), log(as.vector(R)[c(1,2,4)]))}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Constraints \\
\hspace*{0.27 in} Omega <- matrix(exp(parm[c(7,8,8,9)]),NROW(Data$R),NROW(Data$R)) \\
\hspace*{0.27 in} Omega <- ifelse(Omega < 1.0E-7, 1.0E-7, Omega) \\
\hspace*{0.27 in} parm[7:9] <- log(as.vector(Omega)[c(1,2,4)]) \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} alpha.mu <- rep(0,3) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3,3) \\
\hspace*{0.27 in} beta.mu <- rep(0,3) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3,3) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:3] \\
\hspace*{0.27 in} beta <- parm[4:6] \\
\hspace*{0.27 in} R <- Data$R \\
\hspace*{0.27 in} S <- solve(R) \\
\hspace*{0.27 in} Sigma <- solve(Omega) \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} Omega.prior <- log(dwishart(Omega,NROW(R),S)) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} mu <- matrix(0,T,2) \\
\hspace*{0.27 in} mu[,1] <- alpha[1] + alpha[2]*Data$CG + alpha[3]*Data$VG \\
\hspace*{0.27 in} mu[,2] <- beta[1] + beta[2]*Data$CW + beta[3]*Data$VW \\
\hspace*{0.27 in} LL <- rep(0, NROW(Data$Y)) \\
\hspace*{0.27 in} for (i in 1:length(LL)) \{ \\
\hspace*{0.62 in} LL[i] <- sum(dmvn(Data$Y[i,], mu[i,], Sigma, log=TRUE))\} \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- sum(LL) + sum(alpha.prior) + sum(beta.prior) + Omega.prior \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*sum(LL), \\
\hspace*{0.62 in} Monitor=c(LP, as.vector(Sigma)), yhat=as.vector(mu), parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\section{Zero-Inflated Poisson (ZIP)} \label{zip}
\subsection{Form}
$$y \sim Pois(\Lambda_{1:N,2})$$
$$z \sim Bern(\Lambda_{1:N,1})$$
$$z_i = 1 \quad when \quad y_i = 0, \quad else \quad z_i = 0, \quad i=1,\dots,N$$
$$\Lambda_{i,2} = 0 \quad if \quad \Lambda_{i,1} \ge 0.5, \quad i=1,\dots,N$$
$$\Lambda_{1:N,1} = \log[1 + \exp(\textbf{X}_1 \alpha)]$$
$$\Lambda_{1:N,2} = \exp(\textbf{X}_2 \beta)$$
$$\alpha_j \sim N(0, 1000), \quad j=1,\dots,J_1$$
$$\beta_j \sim N(0, 1000), \quad j=1,\dots,J_2$$
\subsection{Data}
\code{N <- 1000 \\
J1 <- 4 \\
J2 <- 3 \\
X1 <- matrix(runif(N*J1,-2,2),N,J1); X1[,1] <- 1 \\
X2 <- matrix(runif(N*J2,-2,2),N,J2); X2[,1] <- 1 \\
alpha <- runif(J1,-1,1) \\
beta <- runif(J2,-1,1) \\
p <- as.vector(invlogit(alpha \%*\% t(X1) + rnorm(N,0,0.1))) \\
mu <- as.vector(round(exp(beta \%*\% t(X2) + rnorm(N,0,0.1)))) \\
y <- ifelse(p > 0.5, 0, mu) \\
z <- ifelse(y == 0, 1, 0) \\
mon.names <- "LP" \\
parm.names <- rep(NA, J1+J2) \\
for (j in 1:J1) \{parm.names[j] <- paste("alpha[", j, "]", sep="")\} \\
for (j in 1:J2) \{parm.names[J1+j] <- paste("beta[", j, "]", sep="")\} \\
MyData <- list(J1=J1, J2=J2, N=N, X1=X1, X2=X2, mon.names=mon.names, \\
\hspace*{0.27 in} parm.names=parm.names, y=y, z=z)
}
\subsection{Initial Values}
\code{Initial.Values <- c(alpha,beta)}
\subsection{Model}
\code{Model <- function(parm, Data) \\
\hspace*{0.27 in} \{ \\
\hspace*{0.27 in} \#\#\# Prior Parameters \\
\hspace*{0.27 in} alpha.mu <- rep(0, Data$J1) \\
\hspace*{0.27 in} alpha.tau <- rep(1.0E-3, Data$J1) \\
\hspace*{0.27 in} beta.mu <- rep(0, Data$J2) \\
\hspace*{0.27 in} beta.tau <- rep(1.0E-3, Data$J2) \\
\hspace*{0.27 in} \#\#\# Parameters \\
\hspace*{0.27 in} alpha <- parm[1:Data$J1] \\
\hspace*{0.27 in} beta <- parm[(Data$J1+1):(Data$J1 + Data$J2)] \\
\hspace*{0.27 in} \#\#\# Log(Prior Densities) \\
\hspace*{0.27 in} alpha.prior <- dnorm(alpha, alpha.mu, 1/sqrt(alpha.tau), log=TRUE) \\
\hspace*{0.27 in} beta.prior <- dnorm(beta, beta.mu, 1/sqrt(beta.tau), log=TRUE) \\
\hspace*{0.27 in} \#\#\# Log-Likelihood \\
\hspace*{0.27 in} Lambda <- matrix(NA, Data$N, 2) \\
\hspace*{0.27 in} Lambda[,1] <- invlogit(alpha \%*\% t(Data$X1)) \\
\hspace*{0.27 in} Lambda[,2] <- exp(beta \%*\% t(Data$X2)) \\
\hspace*{0.27 in} Lambda[,2] <- ifelse(Lambda[,1] >= 0.5, 0, Lambda[,2]) \\
\hspace*{0.27 in} LL1 <- sum(dbern(Data$z, Lambda[,1], log=TRUE)) \\
\hspace*{0.27 in} LL2 <- sum(dpois(Data$y, Lambda[,2], log=TRUE)) \\
\hspace*{0.27 in} \#\#\# Log-Posterior \\
\hspace*{0.27 in} LP <- LL1 + LL2 + sum(alpha.prior) + sum(beta.prior) \\
\hspace*{0.27 in} Modelout <- list(LP=LP, Dev=-2*LL2, Monitor=LP, \\
\hspace*{0.62 in} yhat=Lambda[,2], parm=parm) \\
\hspace*{0.27 in} return(Modelout) \\
\hspace*{0.27 in} \}
}

\bibliography{References.bib}

\end{document}
