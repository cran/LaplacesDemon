\documentclass[nojss]{jss}
%% need no \usepackage{Sweave.sty}
\usepackage{amsmath}

%\VignetteIndexEntry{LaplacesDemon Tutorial}
%\VignettePackage{LaplacesDemon}
%\VignetteDepends{LaplacesDemon}

\author{Byron Hall\\STATISTICAT, LLC}
\title{\includegraphics[height=1in,keepaspectratio]{LDlogo} \\ \pkg{LaplacesDemon}: An \proglang{R} Package for Bayesian Inference}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Byron Hall} %% comma-separated
\Plaintitle{LaplacesDemon: An R Package for Bayesian Inference} %% without formatting
\Shorttitle{LaplacesDemon} %% a short title (if necessary)

\Abstract{
\pkg{LaplacesDemon}, usually referred to as Laplace's Demon, is a contributed \proglang{R} package for Bayesian inference, and is freely available on the Comprehensive \proglang{R} Archive Network (CRAN). Laplace's Demon allows Laplace Approximation and the choice of four MCMC algorithms to update a Bayesian model according to a user-specified model function. The user-specified model function enables Bayesian inference for any model form, provided the user specifies, or approximates, the likelihood. Laplace's Demon also attempts to assist the user by creating and offering \proglang{R} code, based on a previous model update, that can be copy/pasted and executed. Posterior predictive checks and many other features are included as well. Laplace's Demon seeks to be generalizable and user-friendly to Bayesians, especially Laplacians.
}
\Keywords{Adaptive, AM, Bayesian, Conjugate Gradient, Delayed Rejection, 
DR, DRAM, DRM, Gradient Ascent, Laplace Approximation, LaplacesDemon, 
Laplace's Demon, Markov chain Monte Carlo, MCMC, Metropolis, Optimization, 
\proglang{R}, Random Walk, Random-Walk, STATISTICAT}
\Plainkeywords{adaptive, am, bayesian, conjugate gradient, delayed 
rejection, dr, dram, drm, gradient ascent, laplace approximation, 
laplacesdemon, laplace's demon, markov chain monte carlo, mcmc, 
metropolis, optimization, r, random walk, random-walk, statisticat}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2011}
%% \Submitdate{2011-01-18}
%% \Acceptdate{2011-01-18}

\Address{
 Byron Hall\\
 STATISTICAT, LLC\\
 Farmington, CT\\
 E-mail: \email{laplacesdemon@statisticat.com}\\
 URL: \url{http://www.statisticat.com/laplacesdemon.html}
}

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\begin{document}
Bayesian inference is named after Reverend Thomas Bayes (1702-1761) for developing Bayes' theorem, which was published posthumously after his death \citep{bayes63}. This was the first instance of what would be called inverse probability\footnote{`Inverse probability' refers to assigning a probability distribution to an unobserved variable, and is in essence, probability in the opposite direction of the usual sense. Bayes' theorem has been referred to as ``the principle of inverse probability''. Terminology has changed, and the term `Bayesian probability' has displaced `inverse probability'. The adjective ``Bayesian'' was introduced by R. A. Fisher as a derogatory term.}.

Unaware of Bayes, Pierre-Simon Laplace (1749-1827) independently developed Bayes' theorem and first published his version in 1774, eleven years after Bayes, in one of Laplace's first major works \citep[p. 366--367]{laplace74}. In 1812, Laplace introduced a host of new ideas and mathematical techniques in his book, \emph{Theorie Analytique des Probabilites}, \citep{laplace12}. Before Laplace, probability theory was solely concerned with developing a mathematical analysis of games of chance. Laplace applied probabilistic ideas to many scientific and practical problems. Although Laplace is not the father of probability, Laplace may be considered the father of the field of probability.

In 1814, Laplace published his ``Essai Philosophique sur les Probabilites'', which introduced a mathematical system of inductive reasoning based on probability \citep{laplace14}. In it, the Bayesian interpretation of probability was developed independently by Laplace, much more thoroughly than Bayes, so some ``Bayesians'' refer to Bayesian inference as Laplacian inference. This is a translation of a quote in the introduction to this work:

\begin{quote}
``We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes'' \citep{laplace14}.
\end{quote}

The `intellect' has been referred to by future biographers as Laplace's Demon. In this quote, Laplace expresses his philosophical belief in hard determinism and his wish for a computational machine that is capable of estimating the universe.

This article is an introduction to an \proglang{R} \citep{rdct:r} package called \pkg{LaplacesDemon} \citep{r:laplacesdemon}, which was designed without consideration for hard determinism, but instead with a lofty goal toward facilitating high-dimensional Bayesian (or Laplacian) inference\footnote{Even though the \pkg{LaplacesDemon} package is dedicated to Bayesian inference, frequentist inference may be used instead with the same functions by omitting the prior distributions and maximizing the likelihood.}, posing as its own intellect that is capable of impressive analysis. The \pkg{LaplacesDemon} \proglang{R} package is often referred to as Laplace's Demon. This article guides the user through installation, data, specifying a model, initial values, updating Laplace's Demon, summarizing and plotting output, posterior predictive checks, general suggestions, discusses independence and observability, covers details of the algorithm, software comparisons, discusses large data sets and speed, and explains future goals.

Herein, it is assumed that the reader has basic familiarity with Bayesian inference, numerical approximation, and \proglang{R}. If any part of this assumption is violated, then suggested sources include the vignette entitled ``Bayesian Inference'' that comes with the \pkg{LaplacesDemon} package, \citet{gelman04}, and \citet{crawley07}.

\section{Installation} \label{installation}
To obtain Laplace's Demon, simply open \proglang{R} and install the \pkg{LaplacesDemon} package from a CRAN mirror:

%%\SweaveOpts{echo=TRUE,results=verbatim,fig=FALSE}
\begin{Scode}{eval=FALSE}
install.packages("LaplacesDemon")
\end{Scode}

A goal in developing Laplace's Demon was to minimize reliance on other packages or software. Therefore, the usual \code{dep=TRUE} argument does not need to be used, because \pkg{LaplacesDemon} does not depend on anything other than base \proglang{R}. Once installed, simply use the \code{library} or \code{require} function in \proglang{R} to activate the \pkg{LaplacesDemon} package and load its functions into memory:

\begin{Scode}
library(LaplacesDemon)
\end{Scode}

\section{Data} \label{data}
Laplace's Demon requires data that is specified in a list. As an example, there is a data set called \code{demonsnacks} that is provided with the \pkg{LaplacesDemon} package. For no good reason, other than to provide an example, the log of \code{Calories} will be fit as an additive, linear function of the remaining variables. Since an intercept will be included, a vector of 1's is inserted into design matrix \textbf{X}.

\begin{Scode}
data(demonsnacks)
N <- NROW(demonsnacks)
J <- NCOL(demonsnacks)
y <- log(demonsnacks$Calories)
X <- cbind(1, as.matrix(demonsnacks[,c(1,3:10)]))
for (j in 2:J) {X[,j] <- CenterScale(X[,j])}
mon.names <- c("LP","sigma")
parm.names <- parm.names(list(beta=rep(0,J), log.sigma=0))
MyData <- list(J=J, X=X, mon.names=mon.names, parm.names=parm.names, y=y)
\end{Scode}

There are J=\Sexpr{J} independent variables (including the intercept), one for each column in design matrix \textbf{X}. However, there are \Sexpr{J+1} parameters, since the residual variance, $\sigma^2$, must be included as well. The reason why it is called \code{log.sigma} will be explained later. Each parameter must have a name specified in the vector \code{parm.names}, and parameter names must be included with the data. This is using a function called \code{parm.names}. Also, note that each predictor has been centered and scaled, as per \citet{gelman08}. Laplace's Demon provides a \code{CenterScale} function to center and scale predictors\footnote{Centering and scaling a predictor is \code{x.cs <- (x - mean(x)) / (2*sd(x))}.}.

Laplace's Demon will consider using Laplace Approximation, and part of this consideration includes determining the sample size. The user must specify the number of observations in the data as either a scalar \code{n} or \code{N}. If these are not found by the \code{LaplaceApproximation} or \code{LaplacesDemon} functions, then it will attempt to determine sample size as the number of rows in \code{y} or \code{Y}.

\section{Specifying a Model} \label{specification}
Laplace's Demon is capable with any Bayesian model for which the likelihood is specified\footnote{Examples of more than 50 Bayesian models may be found in the ``Examples'' vignette that comes with the \pkg{LaplacesDemon} package. Likelihood-free estimation is also possible by approximating the likelihood, such as in Approximate Bayesian Computation (ABC).}. To use Laplace's Demon, the user must specify a model. Let's consider a linear regression model, which is often denoted as:

$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$
$$\mu = \textbf{X}\beta$$

The dependent variable, $\textbf{y}$, is normally distributed according to expectation vector $\mu$ and scalar variance $\sigma^2$, and expectation vector $\mu$ is equal to the inner product of design matrix \textbf{X} and parameter vector $\beta$.

For a Bayesian model, the notation for the residual variance, $\sigma^2$, has often been replaced with the inverse of the residual precision, $\tau^{-1}$. Here, $\sigma^2$ will be used. Prior probabilities are specified for $\beta$ and $\sigma$ (the standard deviation, rather than the variance):

$$\beta_j \sim \mathcal{N}(0, 1000), \quad j=1,\dots,J$$
$$\sigma \sim \mathcal{HC}(25)$$

Each of the $J$ $\beta$ parameters is assigned an uninformative\footnote{`Non-informative' may be more widely used than 'uninformative', but here that is considered poor English, such as saying something is `non-correct' when there's a word for that \dots `incorrect'.} prior probability distribution that is normally-distributed according to $\mu=0$ and $\sigma^2=1000$. The large variance or small precision indicates a lot of uncertainty about each $\beta$, and is hence an uninformative distribution. The residual standard deviation $\sigma$ is half-Cauchy-distributed according to its hyperparameter, scale=25.

To specify a model, the user must create a function called \code{Model}. Here is an example for a linear regression model:

\begin{Scode}
Model <- function(parm, Data)
     {
     ### Parameters
     beta <- parm[1:Data$J]
     sigma <- exp(parm[Data$J+1])
     ### Log(Prior Densities)
     beta.prior <- dnorm(beta, 0, sqrt(1000), log=TRUE)
     sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE)
     ### Log-Likelihood
     mu <- tcrossprod(beta, Data$X)
     LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE))
     ### Log-Posterior
     LP <- LL + sum(beta.prior) + sigma.prior
     Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma), yhat=mu,
          parm=parm)
     return(Modelout)
     }
\end{Scode}

Laplace's Demon iteratively maximizes the logarithm of the unnormalized joint posterior density as specified in this \code{Model} function. In Bayesian inference, the logarithm of the unnormalized joint posterior density is proportional to the sum of the log-likelihood and logarithm of the prior densities:

$$\log[p(\Theta|\textbf{y})] \propto \log[p(\textbf{y}|\Theta)] + \log[p(\Theta)]$$

where $\Theta$ is a set of parameters, $\textbf{y}$ is the data, $\propto$ means `proportional to'\footnote{For those unfamiliar with $\propto$, this symbol simply means that two quantities are proportional if they vary in such a way that one is a constant multiplier of the other. This is due to an unspecified constant of proportionality in the equation. Here, this can be treated as `equal to'.}, $p(\Theta|\textbf{y})$ is the joint posterior density, $p(\textbf{y}|\Theta)$ is the likelihood, and $p(\Theta)$ is the set of prior densities.

During each iteration in which Laplace's Demon is maximizing the logarithm of the unnormalized joint posterior density, Laplace's Demon passes two arguments to \code{Model}: \code{parm} and \code{Data}, where \code{parm} is short for the set of parameters, and \code{Data} is a list of data. These arguments are specified in the beginning of the function:

\code{Model <- function(parm, Data)} 

Then, the \code{Model} function is evaluated and the logarithm of the unnormalized joint posterior density is calculated as \code{LP}, and returned to Laplace's Demon in a list called \code{Modelout}, along with the deviance (\code{Dev}), a vector (\code{Monitor}) of any variables desired to be monitored in addition to the parameters, $\textbf{y}^{rep}$ (\code{yhat}) or replicates of $\textbf{y}$, and the parameter vector \code{parm}. All arguments must be returned. Even if there is no desire to observe the deviance and any monitored variable, a scalar must be placed in the second position of the \code{Modelout} list, and at least one element of a vector for a monitored variable. This can be seen in the end of the function:

\code{LP <- LL + sum(beta.prior) + sigma.prior} \\
\code{Modelout <- list(LP=LP, Dev=-2*LL, Monitor=c(LP,sigma),} \\
\hspace*{0.27 in} \code{yhat=mu, parm=parm)} \\
\code{return(Modelout)}

The rest of the function specifies the parameters, log of the prior densities, and calculates the log-likelihood. Since design matrix \textbf{X} has J=\Sexpr{J} column vectors (including the intercept), there are \Sexpr{J} \code{beta} parameters and a \code{sigma} parameter for the residual standard deviation.

Since Laplace's Demon passes a vector of parameters called \code{parm} to \code{Model}, the function needs to know which parameter is associated with which element of \code{parm}. For this, the vector \code{beta} is declared, and then each element of \code{beta} is populated with the value associated in the corresponding element of \code{parm}. The reason why \code{sigma} is exponentiated will, again, be explained later.

\code{beta <- parm[1:Data$J]} \\
\code{sigma <- exp(parm[Data$J+1])}

To work with the log of the prior densities and according to the assigned names of the parameters and hyperparameters, they are specified as follows:

\code{beta.prior <- dnorm(beta, 0, sqrt(1000), log=TRUE)} \\
\code{sigma.prior <- dhalfcauchy(sigma, 25, log=TRUE)}

It is important to reparameterize all parameters to be real-valued. For example, a positive-only parameter such as variance should be allowed to range from $-\infty$ to $\infty$, and be transformed in the \code{Model} function with the \code{exp} function, which will force it to positive values. A parameter $\theta$ that needs to be bounded in the model, such as in the interval [1,5], can be transformed to that range with a logistic function, such as $1 + 4[\exp(\theta) / (\exp(\theta) + 1)]$. Alternatively, each parameter may be constrained in the \code{Model} function, such as with the \code{interval} function. Laplace's Demon will attempt to increase or decrease the value of each parameter to maximize \code{LP}, without consideration for the distributional form of the parameter. In the above example, the residual standard deviation \code{sigma} receives a half-Cauchy distributed prior of the form:

$$\sigma \sim \mathcal{HC}(25)$$

In this specification, \code{sigma} cannot be negative. By reparameterizing \code{sigma} as

\code{sigma <- exp(parm[Data$J+1])}

Laplace's Demon will increase or decrease \code{parm[Data$J+1]}, which is effectively \code{log(sigma)}. Now it is possible for Laplace's Demon to decrease \code{log(sigma)} below zero without causing an error or violating its half-Cauchy distributed specification.

Finally, everything is put together to calculate \code{LP}, the logarithm of the unnormalized joint posterior density. The expectation vector \code{mu} is the inner product of the vector \code{beta} and the transpose of the design matrix, \code{Data$X}. Expectation vector \code{mu}, vector \code{Data$y}, and scalar \code{sigma} are used to estimate the sum of the log-likelihoods, where:

$$\textbf{y} \sim \mathcal{N}(\mu, \sigma^2)$$

and as noted before, the logarithm of the unnormalized joint posterior density is:

$$\log[p(\Theta|\textbf{y})] \propto \log[p(\textbf{y}|\Theta)] + \log[p(\Theta)]$$

\code{mu <- tcrossprod(beta, Data$X)} \\
\code{LL <- sum(dnorm(Data$y, mu, sigma, log=TRUE)} \\
\code{LP <- LL + sum(beta.prior) + sigma.prior}

Specifying the model in the \code{Model} function is the most involved aspect for the user of Laplace's Demon. But it has been designed so it is also incredibly flexible, allowing a wide variety of Bayesian models to be specified. Missing values are also easy to estimate (see the ``Examples'' vignette).

\section{Initial Values} \label{initialvalues}
Laplace's Demon requires a vector of initial values for the parameters. Each initial value is a starting point for the estimation of a parameter. When all initial values are set to zero, Laplace's Demon will optimize initial values using a conjugate gradient algorithm in the \code{LaplaceApproximation} function. Laplace Approximation is asymptotic with respect to sample size, so it is inappropriate in this example with a sample size of \Sexpr{N} and \Sexpr{J+1} parameters. Laplace's Demon will not use Laplace Approximation when the sample size is not at least five times the number of parameters. Otherwise, the user may prefer to optimize initial values in the \code{LaplaceApproximation} function before using the \code{LaplacesDemon} function. When Laplace's Demon receives initial values that are not all set to zero, it will begin to update each parameter.

In this example, there are \Sexpr{J+1} parameters. With no prior knowledge, it is a good idea either to randomize each initial value within an interval, say -3 to 3, or set all of them equal to zero and let the \code{LaplaceApproximation} function optimize the initial values, provided there is sufficient sample size. Here, the \code{LaplaceApproximation} function will be introduced in the \code{LaplacesDemon} function, so the first \Sexpr{J} parameters, the \code{beta} parameters, have been set equal to zero, and the remaining parameter, \code{log.sigma}, has been set equal to \code{log(1)}, which is equal to zero. This visually reminds me that I am working with the log of \code{sigma}, rather than \code{sigma}, and is merely a personal preference. The order of the elements of the vector of initial values must match the order of the parameters associated with each element of \code{parm} passed to the \code{Model} function.

\begin{Scode}
Initial.Values <- c(rep(0,J), log(1))
\end{Scode} 

\section{Laplace's Demon} \label{laplacesdemon}
Compared to specifying the model in the \code{Model} function, the actual use of Laplace's Demon is very easy. Since Laplace's Demon is stochastic, or involves pseudo-random numbers, it's a good idea to set a `seed' for pseudo-random number generation, so results can be reproduced. Pick any number you like, but there's only one number appropriate for a demon\footnote{Demonic references are used only to add flavor to the software and its use, and in no way endorse beliefs in demons. This specific pseudo-random seed is often referred to, jokingly, as the `demon seed'.}:

\begin{Scode}
set.seed(666)
\end{Scode}

As with any \proglang{R} package, the user can learn about a function by using the \code{help} function and including the name of the desired function. To learn the details of the \pkg{LaplacesDemon} function, enter:

\begin{Scode}{eval=false}
help(LaplacesDemon) 
\end{Scode}

Here is one of many possible ways to begin:

\begin{Scode}{eval=false}
Fit <- LaplacesDemon(Model, Data=MyData, Adaptive=900, 
     Covar=NULL, DR=1, Initial.Values, Iterations=10000,
     Periodicity=10, Status=1000, Thinning=10)
\end{Scode}

In this example, an output object called \code{Fit} will be created as a result of using the \pkg{LaplacesDemon} function. \code{Fit} is an object of class \code{demonoid}, which means that since it has been assigned a customized class, other functions have been custom-designed to work with it. Laplace's Demon offers Laplace Approximation and four MCMC algorithms (which are explained in section \ref{details}). The above example did not use Laplace Approximation due to small sample size, and instead used the Delayed Rejection Adaptive Metropolis (DRAM) algorithm for updating.

This example tells the \pkg{LaplacesDemon} function to maximize the first component in the list output from the user-specified \code{Model} function, given a data set called \code{Data}, and according to several settings.
\begin{itemize}
\item The \code{Adaptive=900} argument indicates that a non-adaptive MCMC algorithm will begin, and that it will become adaptive at the 900th iteration. Beginning with the 900th iteration, the MCMC algorithm will estimate the proposal variance or covariance based on the history of the chains.
\item The \code{Covar=NULL} argument indicates that a user-specified variance vector or covariance matrix has not been supplied, so the algorithm will begin with its own estimate.
\item The \code{DR=1} argument indicates that delayed rejection will occur, such that when a proposal is rejected, an additional proposal will be attempted, thus potentially delaying rejection of proposals.
\item The \code{Initial.Values} argument requires a vector of initial values for the parameters.
\item The \code{Iterations=10000} argument indicates that the \code{LaplacesDemon} function will update 10,000 times before completion.
\item The \code{Periodicity=10} argument indicates that once adaptation begins, the algorithm will adapt every 10 iterations.
\item The \code{Status=1000} argument indicates that a status message will be printed to the \proglang{R} console every 1,000 iterations.
\item Finally, the \code{Thinning=10} argument indicates that only every \code{n}th iteration will be retained in the output, and in this case, every 10th iteration will be retained.
\end{itemize}

By running the \code{LaplacesDemon} function, the following output was obtained:

\begin{Scode}
Fit <- LaplacesDemon(Model, Data=MyData, Adaptive=900, 
     Covar=NULL, DR=1, Initial.Values, Iterations=10000,
     Periodicity=10, Status=1000, Thinning=10)
\end{Scode}

Laplace's Demon finished quickly, though it had a small data set (N=\Sexpr{NROW(X)}), few parameters (K=\Sexpr{J+1}), and the model was very simple. At each status of \Sexpr{Fit$Status} iterations, the proposal was multivariate, so it did not have to resort to single-component proposals. The output object, \code{Fit}, was created as a list. As with any \proglang{R} object, use \code{str()} to examine its structure:

\begin{Scode}{eval=false}
str(Fit)
\end{Scode}

To access any of these values in the output object \code{Fit}, simply append a dollar sign and the name of the component. For example, here is how to access the observed acceptance rate:

\begin{Scode}
Fit$Acceptance.Rate
\end{Scode}

\section{Summarizing Output} \label{summarizingoutput}
The output object, \code{Fit}, has many components. The (copious) contents of \code{Fit} can be printed to the screen with the usual \proglang{R} functions:

\begin{Scode}{eval=false}
Fit
print(Fit)
\end{Scode}

Both return the same output, which is:

\begin{Scode}
Fit
\end{Scode}

Several components are labeled as \code{NOT SHOWN HERE}, due to their size, such as the covariance matrix \code{Covar} or the stationary posterior samples \code{Posterior2}. As usual, these can be printed to the screen by appending a dollar sign, followed by the desired component, such as:

\begin{Scode}{eval=false}
Fit$Posterior2
\end{Scode}

Although a lot can be learned from the above output, notice that it completed \Sexpr{Fit$Iterations} iterations of \Sexpr{J+1} variables in \Sexpr{round(Fit$Minutes,2)} minutes. Of course this was fast, since there were only \Sexpr{NROW(X)} records, and the form of the specified model was simple. As discussed later, Laplace's Demon does better than most other MCMC software with large numbers of records, such as 100,000 (see section \ref{largedata}).

In \proglang{R}, there is usually a \code{summary} function associated with each class of output object. The \code{summary} function usually summarizes the output. For example, with frequentist models, the \code{summary} function usually creates a table of parameter estimates, complete with p-values.

Since this is not a frequentist package, p-values are not part of any table with the \code{LaplacesDemon} function, and the marginal posterior distributions of the parameters and other variables have already been summarized in \code{Fit}, there is no point to have an associated \code{summary} function. Going one more step toward useability, \code{LaplacesDemon} has a \code{Consort} function, where the user consorts with Laplace's Demon about the output object.

Consorting with Laplace's Demon produces two kinds of output. The first section is identical to \code{print(Fit)}, but by consorting with Laplace's Demon, it also produces a second section called \code{Demonic Suggestion}.

\begin{Scode}
Consort(Fit)
\end{Scode}

The \code{Demonic Suggestion} is a very helpful section of output. When Laplace's Demon was developed initially in late 2010, there were not to my knowledge any tools of Bayesian inference that make suggestions to the user.

Before making its \code{Demonic Suggestion}, Laplace's Demon considers and presents five conditions: the algorithm, acceptance rate, Monte Carlo standard error (MCSE), effective sample size (ESS), and stationarity. There are 48 combinations of these five conditions, though many combinations lead to the same conclusions. In addition to these conditions, there are other suggested values, such as a recommended number of iterations or values for the \code{Periodicity} and \code{Status} arguments. The suggested value for \code{Status} is seeking to print a status message every minute when the expected time is longer than a minute, and is based on the time in minutes it took, the number of iterations, and the recommended number of iterations. This estimate is fairly accurate for non-adaptive algorithms, and is hard to estimate for adaptive algorithms. But, back to the really helpful part\dots

If these five conditions are unsatisfactory, then Laplace's Demon is not appeased, and suggests it should continue updating, and that the user should copy/paste and execute its suggested \proglang{R} code. Here are the criteria it measures against. The final algorithm must be non-adaptive, so that the Markov property holds (this is covered in section \ref{details}). The acceptance rate is considered satisfactory if it is within the interval [15\%,50\%]\footnote{While \citet{spiegelhalter03} recommend updating until the acceptance rate is within the interval [20\%,40\%], and \citet{roberts01} suggest [10\%,40\%], the interval recommended here is [15\%,50\%].}. MCSE is considered satisfactory for each target distribution if it is less than 6.27\% of the standard deviation of the target distribution. This allows the true mean to be within 5\% of the area under a Gaussian distribution around the estimated mean. ESS is considered satisfactory for each target distribution if it is at least 100, which is usually enough to describe 95\% probability intervals. And finally, each variable must be estimated as stationary.

\begin{Scode}{echo=false}
Rec.Iterations <- trunc(Fit$Rec.Thinning / Fit$Thinning *
     Fit$Iterations)
Status.temp <- round(Rec.Iterations / (Fit$Minutes *
     Rec.Iterations / Fit$Iterations),0)
if(Status.temp < Rec.Iterations) Rec.Status <- Status.temp
#else Rec.Status <- sqrt(Rec.Iterations)
\end{Scode}

Notice that since stationarity has been estimated beginning with the \Sexpr{Fit$Rec.BurnIn.Thinned}st iteration, the suggested \proglang{R} code changes from \code{Adaptive=900} to \code{Adaptive=0}. The suggestion is to abandon the adaptive MCMC algorithm in favor of a non-adaptive algorithm, specifically a Random-Walk Metropolis (RWM). It is also replacing the initial values with the latest values of the parameter chains, and is suggesting to begin with the latest covariance matrix. Some of the arguments in the suggested \proglang{R} code seem excessive, such as \code{Iterations=\Sexpr{Rec.Iterations}} and \code{Thinning=\Sexpr{Rec.Iterations/1000}}. For the sake of the example and saving the reader from a few pages of output, the suggested \proglang{R} code will not be run and the following will be run instead:

\begin{Scode}
Initial.Values <- Fit$Posterior1[Fit$Thinned.Samples,]
Fit <- LaplacesDemon(Model, Data=MyData, Adaptive=0,
     Covar=Fit$Covar, DR=0, Initial.Values, Iterations=290000,
     Periodicity=0, Status=10000, Thinning=290)
\end{Scode}

Next, the user consorts with Laplace's Demon:

\begin{Scode}
Consort(Fit)
\end{Scode}

In \Sexpr{round(Fit$Minutes,2)} minutes, Laplace's Demon updated \Sexpr{Fit$Iterations} iterations, retaining every \Sexpr{Fit$Thinning}th iteration due to thinning, and reported an acceptance rate of \Sexpr{round(Fit$Acceptance.Rate,3)}. Notice that all criteria have been met: MCSE's are sufficiently small, ESS's are sufficiently large, and stationarity was estimated beginning with the first iteration. Since the algorithm was RWM, the Markov property holds, so let's look at some plots.

\section{Plotting Output} \label{plottingoutput}
Laplace's Demon has a \code{plot.demonoid} function to enable its own customized plots with \code{demonoid} objects. The variable \code{BurnIn} (below) may be left as it is so it will show only the stationary samples (samples that are no longer trending), or set equal to one so that all samples can be plotted. In this case, it will already be one, so I will leave it alone. The function also enables the user to specify whether or not the plots should be saved as a .pdf file, and allows the user to limit the number of parameters plotted, in case the number is very large and only a quick glance is desired.

\begin{Scode}
BurnIn <- Fit$Rec.BurnIn.Thinned
\end{Scode}
\begin{Scode}{eval=false}
plot(Fit, BurnIn, MyData, PDF=FALSE, Parms=Fit$Parameters)
\end{Scode}

%% Control graphic size, default width=0.8
\setkeys{Gin}{width=0.5\textwidth}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig1,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(3,3))
for (j in 1:3){
     plot(BurnIn:Fit$Thinned.Samples,
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j],
          type="l", xlab="Iterations", ylab="Value",
          main=MyData$parm.names[j])
     panel.smooth(BurnIn:Fit$Thinned.Samples, 
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], pch="")
     plot(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          xlab="Value", main=MyData$parm.names[j])
     polygon(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          col="black", border="black")
     abline(v=0, col="red", lty=2)
     z <- acf(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], plot=FALSE)
     se <- 1/sqrt(length(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]))
     plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
          main=MyData$parm.names[j], xlab="Lag", ylab="Correlation")
     abline(h=(2*se), col="red", lty=2)
     abline(h=(-2*se), col="red", lty=2)
     }
\end{Scode}
\end{center}
\caption{Plots of Marginal Posterior Samples}
\end{figure}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig2,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(3,3))
for (j in 4:6){
     plot(BurnIn:Fit$Thinned.Samples,
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j],
          type="l", xlab="Iterations", ylab="Value",
          main=MyData$parm.names[j])
     panel.smooth(BurnIn:Fit$Thinned.Samples, 
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], pch="")
     plot(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          xlab="Value", main=MyData$parm.names[j])
     polygon(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          col="black", border="black")
     abline(v=0, col="red", lty=2)
     z <- acf(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], plot=FALSE)
     se <- 1/sqrt(length(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]))
     plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
          main=MyData$parm.names[j], xlab="Lag", ylab="Correlation")
     abline(h=(2*se), col="red", lty=2)
     abline(h=(-2*se), col="red", lty=2)
     }
\end{Scode}
\end{center}
\caption{Plots of Marginal Posterior Samples}
\end{figure}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig3,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(3,3))
for (j in 7:9){
     plot(BurnIn:Fit$Thinned.Samples,
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j],
          type="l", xlab="Iterations", ylab="Value",
          main=MyData$parm.names[j])
     panel.smooth(BurnIn:Fit$Thinned.Samples, 
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], pch="")
     plot(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          xlab="Value", main=MyData$parm.names[j])
     polygon(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          col="black", border="black")
     abline(v=0, col="red", lty=2)
     z <- acf(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], plot=FALSE)
     se <- 1/sqrt(length(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]))
     plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
          main=MyData$parm.names[j], xlab="Lag", ylab="Correlation")
     abline(h=(2*se), col="red", lty=2)
     abline(h=(-2*se), col="red", lty=2)
     }
\end{Scode}
\end{center}
\caption{Plots of Marginal Posterior Samples}
\end{figure}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig4,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(3,3))
for (j in 10:11){
     plot(BurnIn:Fit$Thinned.Samples,
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j],
          type="l", xlab="Iterations", ylab="Value",
          main=MyData$parm.names[j])
     panel.smooth(BurnIn:Fit$Thinned.Samples, 
          Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], pch="")
     plot(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          xlab="Value", main=MyData$parm.names[j])
     polygon(density(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]),
          col="black", border="black")
     abline(v=0, col="red", lty=2)
     z <- acf(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j], plot=FALSE)
     se <- 1/sqrt(length(Fit$Posterior1[BurnIn:Fit$Thinned.Samples,j]))
     plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
          main=MyData$parm.names[j], xlab="Lag", ylab="Correlation")
     abline(h=(2*se), col="red", lty=2)
     abline(h=(-2*se), col="red", lty=2)
     }
plot(BurnIn:length(Fit$Deviance),
     Fit$Deviance[BurnIn:length(Fit$Deviance)],
     type="l", xlab="Iterations", ylab="Value", main="Deviance")
panel.smooth(BurnIn:length(Fit$Deviance), 
     Fit$Deviance[BurnIn:length(Fit$Deviance)], pch="")
plot(density(Fit$Deviance[BurnIn:length(Fit$Deviance)]),
     xlab="Value", main="Deviance")
polygon(density(Fit$Deviance[BurnIn:length(Fit$Deviance)]),
     col="black", border="black")
abline(v=0, col="red", lty=2)
z <- acf(Fit$Deviance[BurnIn:length(Fit$Deviance)], plot=FALSE)
se <- 1/sqrt(length(Fit$Deviance[BurnIn:length(Fit$Deviance)]))
plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
     main="Deviance", xlab="Lag", ylab="Correlation")
abline(h=(2*se), col="red", lty=2)
abline(h=(-2*se), col="red", lty=2)
\end{Scode}
\end{center}
\caption{Plots of Marginal Posterior Samples}
\end{figure}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig5,fig=TRUE,echo=FALSE,width=6,height=4}
par(mfrow=c(2,3))
JJ <- NCOL(Fit$Monitor); nn <- NROW(Fit$Monitor)
for (j in 1:JJ){
     plot(BurnIn:nn, 
          Fit$Monitor[BurnIn:nn,j],
          type="l", xlab="Iterations", ylab="Value", 
          main=MyData$mon.names[j])
     panel.smooth(BurnIn:nn, 
          Fit$Monitor[BurnIn:nn,j], pch="")
     plot(density(Fit$Monitor[BurnIn:nn,j]), xlab="Value", 
          main=MyData$mon.names[j])
     polygon(density(Fit$Monitor[BurnIn:nn,j]),
          col="black", border="black")
     abline(v=0, col="red", lty=2)
     z <- acf(Fit$Monitor[BurnIn:nn,j], plot=FALSE)
     se <- 1/sqrt(length(Fit$Monitor[BurnIn:nn,j]))
     plot(z$lag, z$acf, ylim=c(min(z$acf,-2*se),1), type="h",
          main=MyData$mon.names[j], xlab="Lag", ylab="Correlation")
     abline(h=(2*se), col="red", lty=2)
     abline(h=(-2*se), col="red", lty=2)
     }
\end{Scode}
\end{center}
\caption{Plots of Marginal Posterior Samples}
\end{figure}

There are three plots for each parameter, the deviance, and each monitored variable (which in this example are \code{sigma} and \code{mu[1]}). The leftmost plot is a trace-plot, showing the history of the value of the parameter according to the iteration. The middlemost plot is a kernel density plot. The rightmost plot is an ACF or autocorrelation function plot, showing the autocorrelation at different lags. The chains look stationary (do not exhibit a trend), the kernel densities look Gaussian, and the ACF's show low autocorrelation.

Another useful plot is called the caterpillar plot, which plots a horizontal representation of three quantiles (2.5\%, 50\%, and 97.5\%) of each selected parameter from the posterior samples summary. The caterpillar plot will attempt to plot the stationary samples first (\code{Fit$Summary2}), but if stationary samples do not exist, then it will plot all samples (\code{Fit$Summary1}). Here, only the first ten parameters are selected for a caterpillar plot:

\begin{Scode}{eval=false}
caterpillar.plot(Fit, Parms=1:10)
\end{Scode}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig6,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(1,1))
caterpillar.plot(Fit, Parms=1:10)
\end{Scode}
\end{center}
\caption{Caterpillar Plot}
\end{figure}

When predicting the logarithm of \code{y} (Calories) with the \code{demonsnacks} data, the caterpillar plot shows that the best fitting variables are \code{beta[6]} (Sodium), \code{beta[7]} (Total.Carbohydrate), and \code{beta[10]} (Protein). Overall, Laplace's Demon seems to have done well, eating \code{demonsnacks} for breakfast.

If all is well, then the Markov chains should be studied with MCMC diagnostics, and finally, further assessments of model fit should be estimated with posterior predictive checks, showing how well (or poorly) the model fits the data. When the user is satisfied, the \code{BayesFactor} function may be useful in selecting the best model, and the marginal posterior samples may be used for inference.

\section{Posterior Predictive Checks} \label{ppc}
A posterior predictive check is a method to assess discrepancies between the model and the data \citep{gelman96a}. To perform posterior predictive checks with Laplace's Demon, simply use the \code{predict} function:

\begin{Scode}
Pred <- predict(Fit, Model, MyData)
\end{Scode}

This creates \code{Pred}, which is an object of class \code{demonoid.ppc} (where ppc is short for posterior predictive check) that is a list which contains \code{y} and \code{yhat}. If the data set that was used to estimate the model is supplied in \code{predict}, then replicates of \code{y} (also called $\textbf{y}^{rep}$) are estimated. If a new data set is supplied in \code{predict}, then new, unobserved instances of \code{y} (called $\textbf{y}^{new}$) are estimated. Note that with new data, a \code{y} vector must still be supplied, and if unknown, can be set to something sensible such as the mean of the \code{y} vector in the model.

The \code{predict} function calls the \code{Model} function once for each set of stationary samples in \code{Fit\$Posterior2}. Each set of samples is used to calculate \code{mu}, which is the expectation of \code{y}, and \code{mu} is reported here as \code{yhat}. When there are few discrepancies between \code{y} and $\textbf{y}^{rep}$, the model is considered to fit well to the data.

Since \code{Pred\$yhat} is a large (39 x 1000) matrix, let's look at the summary of the posterior predictive distribution:

\begin{Scode}
summary(Pred, Discrep="Chi-Square")
\end{Scode}

The \code{summary.demonoid.ppc} function returns a list with 4 components when \code{y} is continuous (different output occurs for categorical dependent variables when given the argument \code{Categorical=TRUE}):
\begin{itemize}
\item \code{Concordance} is the predictive concordance of \citet{gelfand96}, that indicates the percentage of times that \code{y} that was within the 95\% probability interval of \code{yhat}. A goal is to have 95\% predictive concordance. For more information, see the accompanying vignette entitled ``Bayesian Inference''. In this case, roughly \Sexpr{round(summary(Pred)$Concordance,0)}\% of the time, \code{y} is within the 95\% probability interval of \code{yhat}. These results suggest that the model should be attempted again under different conditions, such as using different predictors, or specifying a different form to the model.
\item \code{Discrepancy.Statistic} is a summary of a specified discrepancy measure. There are many options for discrepancy measures that may be specified in the \code{Discrep} argument. In this example, the specified discrepancy measure was the $\chi^2$ test in \citet[p. 175]{gelman04}, and higher values indicate a worse fit.
\item \code{L-criterion} is a posterior predictive check for model and variable selection that measures the distance between $\textbf{y}$ and $\textbf{y}^{rep}$, providing a criterion to be minimized \citep{laud95}.
\item The last part of the summarized output reports \code{y}, information about the distribution of \code{yhat}, and the predictive quantile (\code{PQ}). The mean prediction of \code{y[1]}, or $\textbf{y}^{rep}_1$, given the model and data, is \Sexpr{round(summary(Pred)$Summary[1,2],3)}. Most importantly, \code{PQ[1]} is \Sexpr{round(summary(Pred)$Summary[1,7],3)}, indicating that \Sexpr{round(summary(Pred)$Summary[1,7]*100,1)}\% of the time, \code{yhat[1,]} was greater than \code{y[1]}, or that \code{y[1]} is close to the mean of \code{yhat[1,]}. Contrast this with the 6th record, where \code{y[6]}=\Sexpr{round(summary(Pred)$Summary[6,1],3)} and \code{PQ[6]}=\Sexpr{round(summary(Pred)$Summary[6,7],3)}. Therefore, \code{yhat[6,]} was not a good replication of \code{y[6]}, because the distribution of \code{yhat[6,]} is always greater than \code{y[6]}. While \code{y[1]} is within the 95\% probability interval of \code{yhat[1,]}, the 95\% probability interval of \code{yhat[6,]} is above \code{y[6]} 100\% of the time, indicating a strong discrepancy between the model and data, in this case.
\end{itemize}

There are also a variety of plots for posterior predictive checks, and the type of plot is controlled with the \code{Style} argument. Many styles exist, such as producing plots of covariates and residuals. The last component of this summary may be viewed graphically as posterior densities. Rather than observing plots for each of \Sexpr{NROW(Pred$yhat)} records or rows, only the first 9 densities will be shown here:

\begin{Scode}{eval=false}
plot(Pred, Style="Density", Rows=1:9)
\end{Scode}

\begin{figure}
\begin{center}
\begin{Scode}{label=fig7,fig=TRUE,echo=FALSE,width=6,height=6}
par(mfrow=c(3,3))
for (j in 1:9){
     plot(density(Pred$yhat[j,]), xlab="Value",
          main=paste("Post. Pred. Plot of yhat[", j,
               ",]", sep=""), sub="Black=Density, Red=y")
     polygon(density(Pred$yhat[j,]), col="black", border="black")
     abline(v=Pred$y[j], col="red")
     }
\end{Scode}
\end{center}
\caption{Posterior Predictive Plots}
\end{figure}

These posterior predictive checks indicate that there is plenty of room to improve this model.

\section{General Suggestions} \label{generalsuggestions}
Following are general suggestions on how best to use Laplace's Demon:

\begin{itemize}
\item As suggested by \citet{gelman08}, continuous predictors should be centered and scaled. Here is an explicit example in \proglang{R} of how to center and scale a single predictor called \code{x}: \code{x.cs <- (x - mean(x)) / (2*sd(x))}. However, it is instead easier to use the \code{CenterScale} function provided in \pkg{LaplacesDemon}.
\item Do not forget to reparameterize any bounded parameters in the \code{Model} function to be real-valued in the \code{parm} vector.
\item MCMC is a stochastic method of numerical approximation, and as such, results may differ with each run due to the use of pseudo-random number generation. It is good practice to set a seed so that each update of the model may be reproduced. Here is an example in \proglang{R}: \code{set.seed(666)}.
\item Once a model has been specified in the \code{Model} function, it may be tempting to specify a large number of iterations and thinning in the \code{LaplacesDemon} function, and simply let the model update a long time, hoping for convergence. Instead, it is wise to begin with few iterations such as \code{Iterations=20}, set \code{Adaptive=0} (preventing adaptation), and set \code{Thinning=1}. User-error in specifying the \code{Model} function will be frustrating otherwise.
\item As model complexity increases, the number of parameters increases, and as initial values are further from high-probability regions, the initial acceptance rate may be very low. If the previous general suggestion was successful, but the aceptance rate was zero, then update the model again, but for more iterations. The goal here is to verify that proposals are accepted without problems before attempting an ``actual'' model update.
\item After studying updates with few iterations, the first ``actual'' update should be long enough that proposals are accepted (the acceptance rate is not zero), adaptation begins to occur, and that enough iterations occur after the first adaptation to allow the user to study the adaptation. In the supplied example, adaptation was allowed to begin at the 900th iteration (\code{Adaptive=900}), but also occurred with \code{Periodicity=10}, so every 10th iteration, adaptation occurred. It is also wise to use delayed rejection to assist with the acceptance rate when the algorithm may begin far from its solution, so set \code{DR=1}.
\item If adaptation does not seem to improve estimation or the initial movement in the chains is worse than expected, then consider optimizing the initial values with the \code{LaplaceApproximation} function, changing the initial values, or setting all initial values equal to zero so the \code{LaplacesDemon} function will use the \code{LaplaceApproximation} function. In MCMC, initial values are most effective when the starting points are close to the target distributions (though, if the target distributions were known \textit{a priori}, then there would be little point in much of this). When initial values are far enough away from the target distributions to be in low-probability regions, the algorithms (both Laplace Approximation and MCMC) may take longer than usual. The MCMC algorithms herein will struggle more as the proposal covariance matrix approaches near-singularity. In extreme examples, it is possible for the proposal covariance matrix to become singular, which will stop Laplace's Demon. If there is no information available to make a better selection, then randomize the initial values and use \code{LaplaceApproximation}. Centered and scaled predictors also help by essentially standardizing the possible range of the target distributions.
\item If Laplace's Demon exhibits an unreasonably low acceptance rate (say, arbitrarily, lower than 15\%, but greater than 0\%) and is having a hard time exploring (but is still able to explore) after significant iterations, then investigate the latest proposal covariance matrix by entering \code{Fit\$Covar}. Chances are that the elements of the diagonal, the variances, are large. In this case, it may be best to set \code{Covar=NULL} for the next time it continues to update, which will begin by default with a scaled identity matrix that should get more movement in the chains. As is usual practice, the latest sampled values should also replace the initial values, so it begins from the last update, but with larger proposal variances. The chains will mix better the closer they get to their target distributions. The user can confirm that Laplace's Demon is making progress and moving overall in the right direction by observing the trace-plots of the deviance, or better yet, the logarithm of the unnormalized joint posterior density. If the deviance is decreasing and the joint posterior is increasing run after run, then the model is continuously fitting better and better, and one possible sign of convergence will be when the deviance and the joint posterior seem to become stationary or no longer show a trend.
\item When speed is a concern, such as with complex models, there may be things in the \code{Model} function that can be commented out, such as sometimes calculating \code{yhat}. The model can be updated without some features, that can be un-commented and used for posterior predictive checks. By commenting out things that are strictly unnecessary to updating, the model will update more quickly.
\item If Laplace's Demon is exploring areas of the state space that the user knows \textit{a priori} should not be explored, then the parameters may be constrained in the \code{Model} function before being passed back to the \code{LaplacesDemon} function. Simply change the parameter of interest as appropriate and place the constrained value back in the \code{parm} vector.
\item \code{Demonic Suggestion} is intended as an aid, not an infallible replacement for critical thinking. As with anything else, its suggestions are based on assumptions, and it is the responsibility of the user to check those assumptions. For example, the \code{Geweke.Diagnostic} may indicate stationarity (lack of a trend) when it does not exist, and this most likely occurs when too few thinned samples remain. Or, the \code{Demonic Suggestion} may indicate that the next update may need to run for a million iterations in a complex model, requiring weeks to complete. Is this really best for the user?
\item Use a two-phase approach with Laplace's Demon, where the first phase consists of using the AM or DRAM algorithm to achieve stationary samples that seem to have converged to the target distributions (convergence can never be determined with MCMC, but some instances of non-convergence can be observed). Once it is believed that convergence has occurred, continue Laplace's Demon with \code{Adaptive=0} so that adaptation will not occur. The final samples should again be checked for signs of non-convergence and, if satisfactory, used for inference.
\item The desirable number of final, thinned samples for inference depends on the required precision of the inferential goal. A good, general goal is to end up with 1,000 thinned samples \citep[p. 295]{gelman04}, where the ESS is at least 100 (and more is desirable).
\item Disagreement exists in MCMC literature as to whether to update one, long chain \citep{geyer92}, or multiple, long chains with different, randomized initial values \citep*{gelman92}. Laplace's Demon is not designed to simultaneously update multiple chains. Nonetheless, if multiple chains are desired, then Laplace's Demon can be updated a series of times, each beginning with different initial values, until multiple output objects of class \code{demonoid} exist with stationary samples, if time allows.
\end{itemize}

\section{Independence and Observability} \label{independence}
For the user, one set of advantages of Laplace's Demon compared to many other available methods is that it was designed with independence and observability in mind. By independence, it is meant that a goal was to minimize dependence on other software. Laplace's Demon is performed completely within base \proglang{R} (though of course the \pkg{LaplacesDemon} package is required). A goal is to provide a complete, Bayesian environment. From personal experience, I've used multiple packages to achieve goals before, and have been trapped when one of those packages failed to keep pace with other changes.

Common Bayesian probability distributions (such as Dirichlet, multivariate normal, Wishart, and others, as well as truncated forms of distributions) have been included in \pkg{LaplacesDemon} so the user does not have to load numerous \proglang{R} packages. All functions in Laplace's Demon are written entirely in \proglang{R}, so the user can easily observe or manipulate the algorithm or functions. For example, to print the code for \pkg{LaplacesDemon} to the \proglang{R} console, simply enter:

\begin{Scode}{eval=false}
LaplacesDemon
\end{Scode}

\section{Details} \label{details}
The \pkg{LaplacesDemon} package uses two broad types of numerical approximation algorithms: Laplace Approximation and Markov chain Monte Carlo (MCMC), and Approximate Bayesian Computation (ABC) may be estimated within each. Each are described below, but MCMC is emphasized.

\subsection{Approximate Bayesian Computation}
Approximate Bayesian Computation (ABC), also called likelihood-free estimation, is a family of numerical approximation techniques in Bayesian inference. ABC is especially useful when evaluation of the likelihood, $p(\textbf{y} | \Theta)$ is computationally prohibitive, or when suitable likelihoods are unavailable. As such, ABC algorithms estimate likelihood-free approximations. ABC is usually faster than a similar likelihood-based numerical approximation technique, because the likelihood is not evaluated directly, but replaced with an approximation that is usually easier to calculate. The approximation of a likelihood is usually estimated with a measure of distance between the observed sample, $\textbf{y}$, and its replicate given the model, $\textbf{y}^{rep}$, or with summary statistics of the observed and replicated samples. See the accompanying vignette entitled ``Examples'' for an example.

\subsection{Laplace Approximation}
The Laplace Approximation or Laplace Method is a family of asymptotic techniques used to approximate integrals. Laplace's method seems to accurately approximate uni-modal posterior moments and marginal posterior distributions in many cases. Since it is not applicable in all cases, it is recommended here that Laplace Approximation is used cautiously in its own right, or preferably, it is used before MCMC.

After introducing the Laplace Approximation \citep[p. 366--367]{laplace74}, a proof was published later \citep{laplace14} as part of a mathematical system of inductive reasoning based on probability. Laplace used this method to approximate posterior moments.

Since its introduction, the Laplace Approximation has been applied successfully in many disciplines. In the 1980s, the Laplace Approximation experienced renewed interest, especially in statistics, and some improvements in its implementation were introduced \citep{tierney86, tierney89}. Only since the 1980s has the Laplace Approximation been seriously considered by statisticians in practical applications.

There are many variations of Laplace Approximation, with an effort toward replacing Markov chain Monte Carlo (MCMC) algorithms as the dominant form of numerical approximation in Bayesian inference. The run-time of Laplace Approximation is a little longer than Maximum Likelihood Estimation (MLE), and much shorter than MCMC \citep{azevedo94}. In the \pkg{LaplacesDemon} package, Laplace Approximation may iterate faster or slower than MCMC, so this is not the fastest possible implementation of Laplace Approximation. Laplace Approximation extends MLE, but shares similar limitations, such as its asymptotic nature with respect to sample size. \citet{bernardo00} note that Laplace Approximation is an attractive numerical approximation algorithm, and will continue to develop, though it currently works best with few parameters.

The \code{LaplaceApproximation} function may be called by the user before using \code{LaplacesDemon}, or \code{LaplacesDemon} may call this function if all initial values are zero. Chasing convergence with \code{LaplaceApproximation} may be time-consuming and unimportant. The goal, instead, is to improve the logarithm of the unnormalized joint posterior density so that it is easier for the \code{LaplacesDemon} function to begin updating the parameters in search of the target distributions. This can be difficult when the initial values are in low-probability regions, and can cause unreasonably low acceptance rates.

\code{LaplaceApproximation} seeks a global maximum of the logarithm of the unnormalized joint posterior density by taking steps proportional to an adaptive scale of the approximate gradient. This portion of the \code{LaplaceApproximation} function uses a conjugate gradient or gradient ascent algorithm, where gradient ascent is called a gradient descent or steepest descent algorithm elsewhere for minimization problems. Laplace's Demon uses the \code{LaplaceApproximation} algorithm to optimize initial values, estimate covariance, and save time for the user, though it is used only when sample size is at least five times the number of parameters or initial values.

This algorithm assumes that the logarithm of the unnormalized joint posterior density is defined and differentiable. An approximate gradient is taken for each initial value as the difference in the logarithm of the unnormalized joint posterior density due to a slight increase versus decrease in the parameter.

With adaptive gradient ascent, at 10 evenly-space times, \code{LaplaceApproximation} attempts several step sizes, which are also called rate parameters in other literature, and selects the best step size from a set of 10 fixed options. Thereafter, each iteration in which an improvement does not occur, the step size shrinks, being multiplied by 0.999.

Gradient ascent is criticized for sometimes being relatively slow when close to the maximum, and its asymptotic rate of convergence is inferior to other methods. However, compared to other popular optimization algorithms such as Newton-Rhapson, an advantage of the gradient ascent is that it works in infinite dimensions, requiring only sufficient computer memory. Although Newton-Rhapson converges in fewer iterations, calculating the inverse of the negative Hessian matrix of second-derivatives is more computationally expensive and subject to singularities. Therefore, gradient ascent takes longer to converge, but is more generalizable. Conjugate gradient should give superior results, except in very large dimensions.

After \code{LaplaceApproximation} finishes, due either to early convergence or completing the number of specified iterations, it approximates the Hessian matrix of second derivatives, and attempts to calculate the covariance matrix by taking the inverse of the negative of this matrix. If successful, then this covariance matrix may be passed to \code{LaplacesDemon}, and the diagonal of this matrix is the variance of the parameters. If unsuccessful, then a scaled identity matrix is returned, and each parameter's variance will be 1.

\subsection{Markov Chain Monte Carlo}

Although the \code{LaplacesDemon} function may be assisted by Laplace Approximation, Laplace's Demon mainly accomplishes numerical approximation with Markov chain Monte Carlo (MCMC) algorithms. There are a large number of MCMC algorithms, too many to review here. Popular families (which are often non-distinct) include Gibbs sampling, Metropolis-Hastings, Random-Walk Metropolis (RWM), slice sampling, and many others, including hybrid algorithms. RWM was developed first \citep{metropolis53}, and Metropolis-Hastings was a generalization of RWM \citep{hastings70}. All MCMC algorithms are known as special cases of the Metropolis-Hastings algorithm. Regardless of the algorithm, the goal in Bayesian inference is to maximize the unnormalized joint posterior distribution and collect samples of the target distributions, which are marginal posterior distributions, later to be used for inference.

While designing Laplace's Demon, the primary goal in numerical approximation was generalization. The most generalizable MCMC algorithm is the Metropolis-Hastings (MH) generalization of the RWM algorithm. The MH algorithm extended RWM to include asymmetric proposal distributions. Having no need of asymmetric proposals, Laplace's Demon uses variations of the original RWM algorithm, which use symmetric proposal distributions, specifically Gaussian proposals. For years, the main disadvantage of the RWM and MH algorithms was that the proposal variance (see below) had to be tuned manually, and therefore other MCMC algorithms have become popular because they do not need to be tuned.

Gibbs sampling became popular for Bayesian inference, though it requires conditional sampling of conjugate distributions, so it is precluded from non-conjugate sampling in its purest form. Gibbs sampling also suffers under high correlations \citep{gilks96}. Due to these limitations, Gibbs sampling is less generalizable than RWM. Slice sampling samples a distribution by sampling uniformly from the region under the plot of its density function, and is more appropriate with bounded distributions that cannot approach infinity.

There are valid ways to tune the RWM algorithm as it updates. This is known by many names, including adaptive Metropolis and adaptive MCMC, among others. A brief discussion follows of RWM and its adaptive variants.

\subsubsection{Block Updating}
Usually, there is more than one target distribution, in which case it must be determined whether it is best to sample from target distributions individually, in groups, or all at once. Block updating refers to splitting a multivariate vector into groups called blocks, so each block may be treated differently. A block may contain one or more variables. Advantages of block updating are that a different MCMC algorithm may be used for each block (or variable, for that matter), creating a more specialized approach, and the acceptance of a newly proposed state is likely to be higher than sampling from all target distributions at once in high dimensions. Disadvantages of block updating are that correlations probably exist between variables between blocks, and each block is updated while holding the other blocks constant, ignoring these correlations of variables between blocks. Without simultaneously taking everything into account, the algorithm may converge slowly or never arrive at the proper solution. Also, as the number of blocks increases, more computation is required, which slows the algorithm. In general, block updating allows a more specialized approach at the expense of accuracy, generalization, and speed. Laplace's Demon avoids block updating, though this increases the importance that the initial values are not in low-probability regions, and may cause Laplace's Demon to have chains that are slow to begin moving.

\subsubsection{Random-Walk Metropolis}
In MCMC algorithms, each iterative estimate of a parameter is part of a changing state. The succession of states or iterations constitutes a Markov chain when the current state is influenced only by the previous state. In random-walk Metropolis (RWM), a proposed future estimate, called a proposal\footnote{Laplace's Demon allows the user to constrain proposals in the \code{Model} function. Laplace's Demon generates a proposal vector, which is passed to the \code{Model} function in the \code{parm} vector. In the \code{Model} function, the user may constrain the proposal to prevent the sampler from exploring certain areas of the state space by altering the proposed values and placing them back into the \code{parm} vector, which will be passed back to Laplace's Demon.} or candidate, of the joint posterior density is calculated, and a ratio of the proposed to the current joint posterior density, called $\alpha$, is compared to a random number drawn uniformly from the interval (0,1). In practice, the logarithm of the unnormalized joint posterior density is used, so $\log(\alpha)$ is the proposal density minus the current density. The proposed state is accepted, replacing the current state with probability 1 when the proposed state is an improvement over the current state, and may still be accepted if the logarithm of a random draw from a uniform distribution is less than $\log(\alpha)$. Otherwise, the proposed state is rejected, and the current state is repeated so that another proposal may be estimated at the next iteration. By comparing $\log(\alpha)$ to the log of a random number when $\log(\alpha)$ is not an improvement, random-walk behavior is included in the algorithm, and it is possible for the algorithm to backtrack while it explores.

Random-walk behavior is desirable because it allows the algorithm to explore, and hopefully avoid getting trapped in undesirable regions. On the other hand, random-walk behavior is undesirable because it takes longer to converge to the target distribution while the algorithm explores. The algorithm generally progresses in the right direction, but may periodically wander away. Such exploration may uncover multi-modal target distributions, which other algorithms may fail to recognize, and then converge incorrectly. With enough iterations, RWM is guaranteed theoretically to converge to the correct target distribution, regardless of the starting point of each parameter, provided the proposal variance for each proposal of a target distribution is sensible.

Multiple parameters usually exist, and therefore correlations may occur between the parameters. All MCMC algorithms in Laplace's Demon are modified to attempt to estimate multivariate proposals, thereby taking correlations into account through a covariance matrix. If a failure is experienced in attempting to estimate multivariate proposals, or if the acceptance rate is less than 5\%, then Laplace's Demon temporarily resorts to single-component proposals by updating one randomly-selected parameter, and will continue to attempt to return to multivariate proposals at each iteration.

Throughout the RWM algorithm, the proposal covariance or variance remains fixed. The user may enter a vector of proposal variances or a proposal covariance matrix, and if neither is supplied, then Laplace's Demon estimates both before it begins, based on the number of variables.

The acceptance or rejection of each proposal should be observed at the completion of the RWM algorithm as the acceptance rate, which is the number of acceptances divided by the total number of iterations. If the acceptance rate is too high, then the proposal variance or covariance is too small. In this case, the algorithm will take longer than necessary to find the target distribution and the samples will be highly autocorrelated. If the acceptance rate is too low, then the proposal variance or covariance is too large, and the algorithm is ineffective at exploration. In the worst case scenario, no proposals are accepted and the algorithm fails to move. Under theoretical conditions, the optimal acceptance rate for a sole, independent and identically distributed (IID), Gaussian, marginal posterior distribution is 0.44 or 44\%. The optimal acceptance rate for an infinite number of distributions that are IID and Gaussian is 0.234 or 23.4\%.

\subsubsection{Delayed Rejection Metropolis}
The Delayed Rejection Metropolis (DRM or DR) algorithm is a RWM with one, small twist. Whenever a proposal is rejected, the DRM algorithm will try one or more alternate proposals, and correct for the probability of this conditional acceptance. By delaying rejection, autocorrelation in the chains may be decreased, and the algorithm is encouraged to move. Currently, Laplace's Demon will attempt one alternate proposal when using the DRAM (see below) or DRM algorithm. The additional calculations may slow each iteration of the algorithm in which the first set of proposals is rejected, but it may also converge faster. For more information on DRM, see \citet{mira01}.

DRM may be considered to be an adaptive MCMC algorithm, because it adapts the proposal based on a rejection. However, DRM does not violate the Markov property (see below), because the proposal is based on the current state. For the purposes of Laplace's Demon, DRM is not considered to be an adaptive MCMC algorithm, because it is not adapting to the target distribution by considering previous states in the Markov chain, but merely makes more attempts from the current state. DRM is rarely suggested by Laplace's Demon, though the combination of DRM and AM, called DRAM (see below), is suggested frequently.

Laplace's Demon also temporarily shrinks the proposal covariance arbitrarily by 50\% for delayed rejection.  A smaller proposal covariance is more likely to be accepted, and the goal of delayed rejection is to increase acceptance. In the long-term, a proposal covariance that is too small is undesirable, and so it is only used in this case to assist acceptance.

\subsubsection{Adaptive Metropolis}
In traditional, non-adaptive RWM, the Markov property is satisfied, creating valid Markov chains, but it is difficult to manually optimize the proposal variance or covariance, and it is crucial that it is optimized for good mixing of the Markov chains. Adaptive MCMC may be used to automatically optimize the proposal variance or covariance based on the history of the chains, though this violates the Markov property, which declares the proposed state is influenced only by the current state\footnote{\citet{haario01} assert that the chains remain ergodic in the limit as the amount of change in the adaptations should decrease to zero as the chains approach the target distributions.}. To retain the Markov property, and therefore valid Markov chains, a two-phase approach may be used, in which adaptive MCMC is used in the first phase to arrive at the target distributions while violating the Markov property, and non-adaptive DRM or RWM is used in the second phase to sample from the target distributions for inference, while possessing the Markov property.

There are too many adaptive MCMC algorithms to review here. All of them adapt the proposal variance to improve mixing. Some adapt the proposal variance to also optimize the acceptance rate (which becomes difficult as dimensionality increases), minimize autocorrelation, or optimize a scale factor. Laplace's Demon uses a variation of the Adaptive Metropolis (AM) algorithm of \citet{haario01}.

Given the number of dimensions (\textit{d}) or parameters, the optimal scale of the proposal variance, also called the jumping kernel, has been reported as $2.4^2/d$\footnote{The optimal proposal standard deviation in this case is approximately $2.4/\sqrt{d}$.} based on the asymptotic limit of infinite-dimensional Gaussian target distributions that are independent and identically-distributed \citep{gelman96b}. In applied settings, each problem is different, so the amount of correlation varies between variables, target distributions may be non-Gaussian, the target distributions may be non-IID, and the scale should be optimized. Laplace's Demon uses a scale that is accurate to more decimals: $2.381204^2/d$. There are algorithms in statistical literature that attempt to optimize this scale, and it is hoped that these algorithms will be included in Laplace's Demon in the future.

\citet{haario01} tested their algorithm with up to 200 dimensions or parameters. It has been tested in Laplace's Demon with 2,600 parameters, so it is capable of large-scale Bayesian inference. The version in Laplace's Demon should be capable of more dimensions than the AM algorithm as it was presented, because when Laplace's Demon experiences an error in multivariate AM, or when the acceptance rate is less than 5\%, it defaults to single-component adaptive proposals \citep{haario05}. Although single-component adaptive proposals should take more iterations to converge, the algorithm is limited in dimension only by the RAM of the computer.

For multivariate adaptive tuning, the formula across \code{K} parameters and \code{t} iterations is:

$$\Sigma^* = [\phi_K \mathrm{cov}(\Theta_{1:t,1:K})] + (\phi_KC\textbf{I}_K)$$

where $\phi_K$ is the scale according to \code{K} parameters, $C$ is a small (1.0E-5) constant to ensure the proposal covariance matrix is positive definite (does not have zero or negative variance on the diagonal), and \textbf{I}$_K$ is a \code{K} x \code{K} identity matrix. The initial proposal covariance matrix, when none is provided, defaults to the scaling component multiplied by its identity matrix: $\phi_K$\textbf{I}$_K$.

For single-component adaptive tuning, the formula across \code{K} parameters and \code{t} iterations is:

$$\sigma^{*2}_k = \phi_k \mathrm{var}(\Theta_{1:t_,k}) + \phi_kC$$

Each element in the initial vector of proposal variances is set equal to the asymptotic scale according to its dimensions: $\phi_k$.

In both the multivariate and single-component cases, the AM algorithm begins with a fixed proposal variance or covariance that is either estimated internally or supplied by the user. Next, the algorithm begins, and it does not adapt until the iteration is reached that is specified by the user in the \code{Adaptive} argument of the \code{LaplacesDemon} function. Then, the algorithm will adapt with every \code{n} iterations according to the \code{Periodicity} argument. Therefore, the user has control over when the AM algorithm begins to adapt, and how often it adapts. The value of the \code{Adaptive} argument in Laplace's Demon is chosen subjectively by the user according to their confidence in the accuracy of the initial proposal covariance or variance. The value of the \code{Periodicity} argument is chosen by the user according to their patience: when the value is 1, the algorithm will adapt continuously, which will be slower to calculate. The AM algorithm adapts the proposal covariance or variance according to the observed covariance or variance in the entire history of all parameter chains, as well as the scale factor.

As recommended by \citet{haario01}, there are two tricks that may be used to assist the AM algorithm in the beginning. Although Laplace's Demon does not use the suggested ``greedy start'' method (and will instead use Laplace Approximation whensample size permits), it uses the second suggested trick of shrinking the proposal as long as the acceptance rate is less than 5\%, and there have been at least five acceptances. \citet{haario01} suggest loosely that if ``it has not moved enough during some number of iterations, the proposal could be shrunk by a constant factor''. For each iteration that the acceptance rate is less than 5\% and that the AM algorithm is used but the current iteration is prior to adaptation, Laplace's Demon multiplies the proposal covariance or variance by (1 - 1/Iterations). Over pre-adaptive time, this encourages a smaller proposal covariance or variance to increase the acceptance rate so that when adaptation begins, the observed covariance or variance of the chains will not be constant, and then shrinkage will cease and adaptation will take it from there.

\subsubsection{Delayed Rejection Adaptive Metropolis}
The Delayed Rejection Adaptive Metropolis (DRAM) algorithm is merely the combination of both DRM (or DR) and AM \citep{haario06}. DRAM has been demonstrated as robust in extreme situations where DRM or AM fail separately. \citet{haario06} present an example involving ordinary differential equations in which least squares could not find a stable solution, and DRAM did well.

\subsection{Afterward}
Once the model is updated with the \code{LaplacesDemon} function, the \code{Geweke.Diagnostic} function of \citet{geweke92} is iteratively applied to successively smaller tail-sections of the thinned samples to assess stationarity (or lack of trend). When all parameters are estimated as stationary beyond a given iteration, the previous iterations are suggested to be considered as burn-in and discarded. The number of thinned samples is divided into cumulative 10\% groups, and the \code{Geweke.Diagnostic} function is applied by beginning with each cumulative group.

The importance of Monte Carlo Standard Error (MCSE) is debated. Here, it is considered important enough to be one of five main criteria to appease Laplace's Demon. It is often recommended that one of several competing batch methods should be used to estimate MCSE, arguing that the simple method \code{(MCSE = }$\sigma/\sqrt{m}$) is biased and reports less error (where $m$ is the ESS). I have calculated both the simple method and non-overlapping batch MCSE's on a wide range of applied models, and noted just as many cases of the simple method producing higher MCSE's as lower MCSE's. As far as Laplace's Demon is concerned, the simple method is used to estimate MCSE, but it is open to debate.

\section{Software Comparisons} \label{software}
There is now a wide variety of software to perform MCMC for Bayesian inference. Perhaps the most common is BUGS, which is an acronym for Bayesian Using Gibbs Sampling \citep{lunn09}. BUGS has several versions. A popular variant is JAGS, which is an acronym for Just Another Gibbs Sampler \citep{plummer03}. The only other comparisons made here are with some \proglang{R} packages (\pkg{AMCMC}, \pkg{mcmc}, \pkg{MCMCpack}, and \pkg{UMACS}) and SAS. Many other \proglang{R} packages use MCMC, but are not intended as general-purpose MCMC software. Hopefully I have not overlooked any general-purpose MCMC packages in \proglang{R}.

WinBUGS has been the most common version of BUGS, though it is no longer developed. BUGS is an intelligent MCMC engine that is capable of numerous MCMC algorithms, but prefers Gibbs sampling. According to its user manual \citep{spiegelhalter03}, WinBUGS 1.4 uses Gibbs sampling with full conditionals that are continuous, conjugate, and standard. For full conditionals that are log-concave and non-standard, derivative-free Adaptive Rejection Sampling (ARS) is used. Slice sampling is selected for non-log-concave densities on a restricted range, and tunes itself adaptively for 500 iterations. Seemingly as a last resort, an adaptive MCMC algorithm is used for non-conjugate, continuous, full conditionals with an unrestricted range. The standard deviation of the Gaussian proposal distribution is tuned over the first 4,000 iterations to obtain an acceptance rate between 20\% and 40\%. Samples from the tuning phases of both Slice sampling and adaptive MCMC are ignored in the calculation of all summary statistics, although they appear in trace-plots.

The current version of BUGS, OpenBUGS, allows the user to specify an MCMC algorithm from a long list for each parameter \citep{lunn09}. This is a step forward, overcoming what is perceived here as an over-reliance on Gibbs sampling. However, if the user does not customize the selection of the MCMC sampler, then Gibbs sampling will be selected for full conditionals that are continuous, conjugate, and standard, just as with WinBUGS.

Based on years of almost daily experience with WinBUGS and JAGS, which are excellent software packages for Bayesian inference, Gibbs sampling is selected too often in these automatic, MCMC engines. An advantage of Gibbs sampling is that the proposals are accepted with probability 1, so convergence may be faster, whereas the RWM algorithm backtracks due to its random-walk behavior. Unfortunately, Gibbs sampling is not as generalizable, because it can function only when certain conjugate distributional forms are known \textit{a priori} \citep{gilks96}. Moreover, Gibbs sampling was avoided for Laplace's Demon because it doesn't perform well with correlated variables or parameters, which usually exist, and I have been bitten by that \textit{bug} many times.

The BUGS and JAGS families of MCMC software are excellent. BUGS is capable of several things that Laplace's Demon is not. BUGS allows the user to specify the model graphically as a directed acyclic graph (DAG) in Doodle BUGS. Laplace's Demon limits the user to one chain per parameter per update, where BUGS can update multiple chains per parameter simultaneously. Lastly, many textbooks in several fields have been written that are full of WinBUGS examples.

The four MCMC algorithms in Laplace's Demon are generalizable, and generally robust to correlation between variables or parameters. The disadvantages are that convergence is slower and RWM may get stuck in regions of low probability. The advantages, however, are faster convergence when correlations are high, and more confidence in the results.

At the time this article was written, the \pkg{AMCMC} package in \proglang{R} is unavailable on CRAN, but may be downloaded from the author's website\footnote{\pkg{AMCMC} is available from J. S. Rosenthal's website at \url{http://www.probability.ca/amcmc/}}. This download is best suited for a Linux, Mac, or UNIX operating system, because it requires the \code{gcc} \proglang{C} compiler, which is unavailable in Windows. It performs adaptive Metropolis-within-Gibbs \citep{roberts07}, and uses \proglang{C} language for significantly faster sampling. Metropolis-within-Gibbs is not as generalizable as adaptive MCMC. Otherwise, if the user wishes to see the code of the \pkg{AMCMC} sampler, then the user must also be familiar with \proglang{C} language.

Also in \proglang{R}, the \pkg{mcmc} package \citep{r:mcmc} offers RWM with multivariate Gaussian proposals and allows batching, as well as a simulated tempering algorithm, but it does not have any adaptive algorithms.

The \pkg{MCMCpack} package \citep{r:mcmcpack} in \proglang{R} takes a canned-function approach to RWM, which is convenient if the user needs the specific form provided, but is otherwise not generalizable. General-purpose RWM is included, but adaptive algorithms are not. It also offers the option of Laplace Approximation to optimize initial values, though the algorithm is evaluated in \code{optim}, which has not performed well in my testing of Laplace Approximations.

At the time this article was written, the \pkg{UMACS} package \citep{r:umacs} has been removed from CRAN. It became outdated due to lack of interest, but did include an adaptive MCMC algorithm as well as Gibbs sampling.

In SAS 9.2 \citep{sas08}, an experimental procedure called \code{PROC MCMC} has been introduced. It is undeniably a rip-off of BUGS (including its syntax), though OpenBUGS is much more powerful, tested, and generalizable. Since SAS is proprietary, the user cannot see or manipulate the source code, and should expect much more from it than OpenBUGS or any open-source software, given the absurd price.

\section{Large Data Sets and Speed} \label{largedata}
An advantage of Laplace's Demon compared to other MCMC software is that the model is specified in a way that takes advantage of \proglang{R}'s vectorization. BUGS and JAGS, for example, require models to be specified so that each record of data is processed one by one inside a `\code{for} loop', which significantly slows updating with larger data sets. In contrast, Laplace's Demon avoids `\code{for} loops' and \code{apply} functions wherever possible\footnote{However, when `for loops' or \code{apply} functions must be used, Laplace's Demon is typically slower than BUGS.}. For example, a data set of 100,000 rows and 16 columns (the dependent variable, a column vector of 1's for the intercept, and 14 predictors) was updated 1,000 times with \code{Adaptive=2}, \code{DR=0}, \code{Periodicity=10}, and the initial value for each $\beta$ set to 0.1 to bypass Laplace Approximation. This took 0.98 minutes with Laplace's Demon, according to a simple, linear regression\footnote{These updates were performed on a 2010 System76 Pangolin Performance laptop with 64-bit Debian Linux and 8GB RAM.}. It was nowhere near convergence, but updating the same model with the same data for 1,000 iterations took 45.55 minutes in JAGS.

However, the speed with which an iteration is estimated is not a good, overall criterion of performance. For example, a Gibbs sampling algorithm with uncorrelated target distributions should converge in fewer iterations than a random-walk algorithm, such as those used in Laplace's Demon. Depending on circumstances, Laplace's Demon may handle larger data sets better, and it may estimate each iteration faster, but it may also take more iterations to converge\footnote{To continue this example, JAGS may be \textit{guessed} to take 20,000 iterations or 15.18 hours, and \code{LaplacesDemon} may take 400,000 iterations or 6.53 hours, and also have less autocorrelation in the chains due to more thinning.}.

However, with small data sets, other MCMC software (\pkg{AMCMC} is a good example) can be faster than Laplace's Demon, if it is programmed in a faster language such as \proglang{Component Pascal}, \proglang{C}, \proglang{C++}, or \proglang{FORTRAN}. I have not studied all MCMC algorithms in \proglang{R}, but most are probably programmed in \proglang{C} and called from \proglang{R}. And Laplace's Demon could be much faster if programmed in \proglang{C} as well.

When the non-adaptive algorithm updates in Laplace's Demon, the expected speed of an iteration should not differ depending on how many iterations it has previously updated. However, the adaptive algorithm will slow as iterations are updated, because each time it adapts, it is adapting to the covariance of the entire history of the chains. As the history increases, the calculations take longer to complete, and the expected speed of an adaptive iteration decreases, compared to earlier adaptive iterations. If time is of the essence and the algorithm needs to be adaptive, then it may be best to make multiple, shorter updates in place of one, longer update.

\section{Conclusion} \label{conclusion}

The \pkg{LaplacesDemon} package is a significant contribution toward Bayesian inference in \proglang{R}. In turn, contributions toward the development of Laplace's Demon are welcome. Please send an email to \email{laplacesdemon@statisticat.com} with constructive criticism, reports of software bugs, or offers to contribute to Laplace's Demon.

\bibliography{References.bib}

\end{document}
